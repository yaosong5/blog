{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"themes/icarus/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/icarus/source/js/insight.js","path":"js/insight.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/aALEHuN.png","path":"css/images/aALEHuN.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/avatar.png","path":"css/images/avatar.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/avatar2.png","path":"css/images/avatar2.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/bad-boy-logo.png","path":"css/images/bad-boy-logo.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/bad-boy.jpg","path":"css/images/bad-boy.jpg","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/badboy.png","path":"css/images/badboy.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/badboyCon.png","path":"css/images/badboyCon.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/badboyCon的副本.png","path":"css/images/badboyCon的副本.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/donate.gif","path":"css/images/donate.gif","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/eye.png","path":"css/images/eye.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/images.png","path":"css/images/images.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/logo.png","path":"css/images/logo.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/logo1.png","path":"css/images/logo1.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/logo2.png","path":"css/images/logo2.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/thumb-default-small.png","path":"css/images/thumb-default-small.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/logo_badboy.png","path":"css/images/logo_badboy.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/venum.gif","path":"css/images/venum.gif","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/justified-gallery/jquery.justifiedGallery.min.js","path":"libs/justified-gallery/jquery.justifiedGallery.min.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/justified-gallery/justifiedGallery.min.css","path":"libs/justified-gallery/justifiedGallery.min.css","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/styles.css","path":"libs/open-sans/styles.css","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/source-code-pro/styles.css","path":"libs/source-code-pro/styles.css","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/alipay.jpg","path":"css/images/alipay.jpg","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/avatar2323.png","path":"css/images/avatar2323.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/wechatpay.jpg","path":"css/images/wechatpay.jpg","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/font-awesome/css/font-awesome.css","path":"libs/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/font-awesome/css/font-awesome.min.css","path":"libs/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/css/lg-fb-comment-box.css","path":"libs/lightgallery/css/lg-fb-comment-box.css","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/css/lg-fb-comment-box.css.map","path":"libs/lightgallery/css/lg-fb-comment-box.css.map","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/css/lg-fb-comment-box.min.css","path":"libs/lightgallery/css/lg-fb-comment-box.min.css","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/css/lg-transitions.css.map","path":"libs/lightgallery/css/lg-transitions.css.map","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/css/lg-transitions.css","path":"libs/lightgallery/css/lg-transitions.css","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/css/lg-transitions.min.css","path":"libs/lightgallery/css/lg-transitions.min.css","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/css/lightgallery.css","path":"libs/lightgallery/css/lightgallery.css","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/css/lightgallery.css.map","path":"libs/lightgallery/css/lightgallery.css.map","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/css/lightgallery.min.css","path":"libs/lightgallery/css/lightgallery.min.css","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/fonts/lg.eot","path":"libs/lightgallery/fonts/lg.eot","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/fonts/lg.svg","path":"libs/lightgallery/fonts/lg.svg","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/fonts/lg.ttf","path":"libs/lightgallery/fonts/lg.ttf","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/fonts/lg.woff","path":"libs/lightgallery/fonts/lg.woff","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/img/loading.gif","path":"libs/lightgallery/img/loading.gif","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/img/video-play.png","path":"libs/lightgallery/img/video-play.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/img/vimeo-play.png","path":"libs/lightgallery/img/vimeo-play.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/img/youtube-play.png","path":"libs/lightgallery/img/youtube-play.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-autoplay.js","path":"libs/lightgallery/js/lg-autoplay.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-autoplay.min.js","path":"libs/lightgallery/js/lg-autoplay.min.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-fullscreen.js","path":"libs/lightgallery/js/lg-fullscreen.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-fullscreen.min.js","path":"libs/lightgallery/js/lg-fullscreen.min.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-hash.js","path":"libs/lightgallery/js/lg-hash.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-hash.min.js","path":"libs/lightgallery/js/lg-hash.min.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-pager.js","path":"libs/lightgallery/js/lg-pager.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-pager.min.js","path":"libs/lightgallery/js/lg-pager.min.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-share.js","path":"libs/lightgallery/js/lg-share.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-share.min.js","path":"libs/lightgallery/js/lg-share.min.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-thumbnail.js","path":"libs/lightgallery/js/lg-thumbnail.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-thumbnail.min.js","path":"libs/lightgallery/js/lg-thumbnail.min.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-video.js","path":"libs/lightgallery/js/lg-video.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-video.min.js","path":"libs/lightgallery/js/lg-video.min.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-zoom.js","path":"libs/lightgallery/js/lg-zoom.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-zoom.min.js","path":"libs/lightgallery/js/lg-zoom.min.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lightgallery.min.js","path":"libs/lightgallery/js/lightgallery.min.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/lightgallery/js/lightgallery.js","path":"libs/lightgallery/js/lightgallery.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/59ZRklaO5bWGqF5A9baEERJtnKITppOI_IvcXXDNrsc.woff2","path":"libs/open-sans/fonts/59ZRklaO5bWGqF5A9baEERJtnKITppOI_IvcXXDNrsc.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/K88pR3goAWT7BTt32Z01mxJtnKITppOI_IvcXXDNrsc.woff2","path":"libs/open-sans/fonts/K88pR3goAWT7BTt32Z01mxJtnKITppOI_IvcXXDNrsc.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/LWCjsQkB6EMdfHrEVqA1KRJtnKITppOI_IvcXXDNrsc.woff2","path":"libs/open-sans/fonts/LWCjsQkB6EMdfHrEVqA1KRJtnKITppOI_IvcXXDNrsc.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNShWV49_lSm1NYrwo-zkhivY.woff2","path":"libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNShWV49_lSm1NYrwo-zkhivY.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSj0LW-43aMEzIO6XUTLjad8.woff2","path":"libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSj0LW-43aMEzIO6XUTLjad8.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSpX5f-9o1vgP2EXwfjgl7AY.woff2","path":"libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSpX5f-9o1vgP2EXwfjgl7AY.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSq-j2U0lmluP9RWlSytm3ho.woff2","path":"libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSq-j2U0lmluP9RWlSytm3ho.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSqaRobkAwv3vxw3jMhVENGA.woff2","path":"libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSqaRobkAwv3vxw3jMhVENGA.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSugdm0LZdjqr5-oayXSOefg.woff2","path":"libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSugdm0LZdjqr5-oayXSOefg.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSv8zf_FOSsgRmwsS7Aa9k2w.woff2","path":"libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSv8zf_FOSsgRmwsS7Aa9k2w.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/RjgO7rYTmqiVp7vzi-Q5URJtnKITppOI_IvcXXDNrsc.woff2","path":"libs/open-sans/fonts/RjgO7rYTmqiVp7vzi-Q5URJtnKITppOI_IvcXXDNrsc.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/cJZKeOuBrn4kERxqtaUH3VtXRa8TVwTICgirnJhmVJw.woff2","path":"libs/open-sans/fonts/cJZKeOuBrn4kERxqtaUH3VtXRa8TVwTICgirnJhmVJw.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/u-WUoqrET9fUeobQW7jkRRJtnKITppOI_IvcXXDNrsc.woff2","path":"libs/open-sans/fonts/u-WUoqrET9fUeobQW7jkRRJtnKITppOI_IvcXXDNrsc.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBiYE0-AqJ3nfInTTiDXDjU4.woff2","path":"libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBiYE0-AqJ3nfInTTiDXDjU4.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBjTOQ_MqJVwkKsUn0wKzc2I.woff2","path":"libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBjTOQ_MqJVwkKsUn0wKzc2I.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBjUj_cnvWIuuBMVgbX098Mw.woff2","path":"libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBjUj_cnvWIuuBMVgbX098Mw.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBkbcKLIaa1LC45dFaAfauRA.woff2","path":"libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBkbcKLIaa1LC45dFaAfauRA.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBmo_sUJ8uO4YLWRInS22T3Y.woff2","path":"libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBmo_sUJ8uO4YLWRInS22T3Y.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBr6up8jxqWt8HVA3mDhkV_0.woff2","path":"libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBr6up8jxqWt8HVA3mDhkV_0.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBo4P5ICox8Kq3LLUNMylGO4.woff2","path":"libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBo4P5ICox8Kq3LLUNMylGO4.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/open-sans/fonts/xozscpT2726on7jbcb_pAhJtnKITppOI_IvcXXDNrsc.woff2","path":"libs/open-sans/fonts/xozscpT2726on7jbcb_pAhJtnKITppOI_IvcXXDNrsc.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/source-code-pro/fonts/mrl8jkM18OlOQN8JLgasD9V_2ngZ8dMf8fLgjYEouxg.woff2","path":"libs/source-code-pro/fonts/mrl8jkM18OlOQN8JLgasD9V_2ngZ8dMf8fLgjYEouxg.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/source-code-pro/fonts/mrl8jkM18OlOQN8JLgasDy2Q8seG17bfDXYR_jUsrzg.woff2","path":"libs/source-code-pro/fonts/mrl8jkM18OlOQN8JLgasDy2Q8seG17bfDXYR_jUsrzg.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/Basketball-icon.png","path":"css/images/Basketball-icon.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/avatar3.png","path":"css/images/avatar3.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/i.ico","path":"css/images/i.ico","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/font-awesome/fonts/FontAwesome.otf","path":"libs/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/font-awesome/fonts/fontawesome-webfont.eot","path":"libs/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/font-awesome/fonts/fontawesome-webfont.woff","path":"libs/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/jquery/2.1.3/jquery.min.js","path":"libs/jquery/2.1.3/jquery.min.js","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/font-awesome/fonts/fontawesome-webfont.woff2","path":"libs/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/avatar4.png","path":"css/images/avatar4.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/c02a6475bff33996304bbb5e104883f4.jpg","path":"css/images/c02a6475bff33996304bbb5e104883f4.jpg","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/logo4.png","path":"css/images/logo4.png","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/font-awesome/fonts/fontawesome-webfont.ttf","path":"libs/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/icarus/source/libs/font-awesome/fonts/fontawesome-webfont.svg","path":"libs/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/icarus/source/css/images/badboygq.jpg","path":"css/images/badboygq.jpg","modified":1,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"9e205238c7526e10c04946e95cd1cca048fe608c","modified":1533579411527},{"_id":"source/CNAME","hash":"644e70095e994a1949b603b7667f3b3a3efd7ea1","modified":1526797840397},{"_id":"source/baidu_verify_WHXmBFaAkY.html","hash":"36e24c745efca4967a17ac5c5a1138579ff4d299","modified":1526521981988},{"_id":"source/google00655d7c846aab3a.html","hash":"ef9d527a608cd37ec86911e11d73c4fd83f6e5b3","modified":1526523340515},{"_id":"themes/icarus/.DS_Store","hash":"b98c2787ead2f76747f4d910d9274258d48e1558","modified":1526786097441},{"_id":"themes/icarus/.gitignore","hash":"542aaea07afe90211c6a45c90b7d6879a4503043","modified":1525046792000},{"_id":"themes/icarus/LICENSE","hash":"df00918fa95de563927fd92b26f14c7affdc3052","modified":1525046792000},{"_id":"themes/icarus/README.md","hash":"acd2d5d12820b065345d68f88bfc3a739f8d8ae2","modified":1525046792000},{"_id":"themes/icarus/_config.yml","hash":"9bc615e06c5bb3be0130bfaf1fe568520fc4f7f2","modified":1536115700147},{"_id":"themes/icarus/_config.yml.example","hash":"b389a45f97c36b24c267b75447ee26dcfc6f2fe3","modified":1525046792000},{"_id":"themes/icarus/baidu_verify_WHXmBFaAkY.html","hash":"36e24c745efca4967a17ac5c5a1138579ff4d299","modified":1526710953431},{"_id":"themes/icarus/package-lock.json","hash":"c8247f0015a946887f9de4448545202ebe83a80e","modified":1526003752034},{"_id":"themes/icarus/package.json","hash":"b024ebe21bd44bd1a4159a7f0cd78470567180f0","modified":1526003751994},{"_id":"source/404/index.md","hash":"c8c926f4b57bda1eb7519e726b3c260e81229d66","modified":1525773651508},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1536049511601},{"_id":"source/_posts/Ambari搭建.md","hash":"2362fceffaf3802fc4fc1bd932d804b97f779af2","modified":1533926917181},{"_id":"source/_posts/Centos7上搭建Jenkins.md","hash":"2e6a2dc9ad8e897345543db2a72a0e1f4be2464b","modified":1533952814159},{"_id":"source/_posts/CoudearManager搭建.md","hash":"b851be218be9961c1551360b1c1f4af6441cb4dd","modified":1533927949436},{"_id":"source/_posts/DataStream简介.md","hash":"e4f395ecb297e483eba8be2b17484f9ed9e4c2d5","modified":1536116500805},{"_id":"source/_posts/Docker-machine的搭建(与宿主机在同一ip段下).md","hash":"ee04c1d1455f9a2fd588ba1227dc4dc975759c99","modified":1535076075797},{"_id":"source/_posts/Docker命令汇集.md","hash":"ed9fe0c4f4d5bef2be2ba153ae2903661df433b1","modified":1535076782239},{"_id":"source/_posts/Docker安装Hadoop集群【引用】.md","hash":"7e786341bf54bc43fecd8ba28f90dc0a31055769","modified":1533583654504},{"_id":"source/_posts/Docker构建免密登录.md","hash":"62c05f65b98fc103995fa4f387d1a43a77b1114c","modified":1533929084124},{"_id":"source/_posts/HBase性能分析.md","hash":"7b55a7e9df1aa7591728739391d6cb844ac430ae","modified":1534344377237},{"_id":"source/_posts/HBase拷贝生产环境数据到本地运行调试.md","hash":"d72d1502dc891baf9d83b9204e677a94f39cf1f6","modified":1534343392538},{"_id":"source/_posts/HDFS元数据备份流程.md","hash":"b62776c58876a04b092a5c590f6fcce7924dc27f","modified":1534348602934},{"_id":"source/_posts/Hadoop-构成及HA-.md","hash":"269b5dc6fcbd381e2c74ef7e8b809a85c32b7927","modified":1534416769096},{"_id":"source/_posts/Hadoop零碎知识点.md","hash":"ceba7471504aeecd0621f62463b7ff61897e0f7d","modified":1536116308337},{"_id":"source/_posts/Hbase-shell操作.md","hash":"6bd4230d05bbc7f92479ee0b5bea692bc81f85d7","modified":1533924025636},{"_id":"source/_posts/Hdfs结构性能分析及读写流程.md","hash":"87b819c9fae997e3ffe60cf9fea19e9a7a1b7ac6","modified":1534434524374},{"_id":"source/_posts/Hive sql相关.md","hash":"43cc1bb52554e7dd51b914d1f05beccc135c4fb3","modified":1535601608449},{"_id":"source/_posts/Hive分桶表相关.md","hash":"8b852b550bfb517ef513a2bc87982b107044b6cb","modified":1534150852971},{"_id":"source/_posts/Hive搭建及启动.md","hash":"2a306829594cffa7ec3a2025225095df61be58d6","modified":1534356883446},{"_id":"source/_posts/Hive累计报表.md","hash":"f5d63b907d0492f2ab2e141a2d495b883db7f81b","modified":1534236495072},{"_id":"source/_posts/Hive自定义函数流程.md","hash":"e4afa1e6a7d8a7aa9ff2b511426c94ae1c898f4b","modified":1534130384441},{"_id":"source/_posts/Json与Scala类型的一些互相转换处理.md","hash":"c7999375ea024cdd36cf28c9f17acae01b2dc2ff","modified":1533929946343},{"_id":"source/_posts/Kafka小知识点.md","hash":"34a38f5828390b9e655acde29066ee2b1813b5f6","modified":1534305297767},{"_id":"source/_posts/Kafka深入解析.md","hash":"5f14bfd7170121b8cb987d3998e2493056148279","modified":1534235428045},{"_id":"source/_posts/Kafka读取数据性能.md","hash":"33f15ae798794e759897f1208d8cede97292e9fc","modified":1534317804298},{"_id":"source/_posts/Kafka集群配置及配置文件.md","hash":"e69acbc60842e54a3e6a7f5732e1f31f38e0f2f5","modified":1534144894526},{"_id":"source/_posts/Linux命令积累.md","hash":"3ee1a34b2d90b9110a1563c629e249a68cf76df0","modified":1535076862229},{"_id":"source/_posts/Linux安装mysql.md","hash":"b7a51abab9baf2ddb575d24b9557ce5332bd45da","modified":1533923576914},{"_id":"source/_posts/MapReduce中Shuffle中的机制.md","hash":"baf12a61288315909787f15f2e8d21ccd80398ac","modified":1534846000273},{"_id":"source/_posts/MyBatis注解.md","hash":"8727b19807e6bfa1fe4cc85bafa56affdb5fd3fa","modified":1533923573516},{"_id":"source/_posts/Scala基本使用.md","hash":"541ce6b2827f9e924e1321a5a54fa1c8c3fefe4e","modified":1535102653619},{"_id":"source/_posts/Spark-On-yarn.md","hash":"0e45afaa4d08a6d5aaf58a143749d846e9830cf7","modified":1534472367066},{"_id":"source/_posts/Spark-on-Yarn源码解析(一)Yarn任务解析.md","hash":"a8c65915f09b7b4187949605fe3e5c5bbfab5597","modified":1536056036834},{"_id":"source/_posts/Spark-on-Yarn源码解析(三)client做的事情.md","hash":"60a34eeb5f38968db39d86ab6b8434bda457a80f","modified":1536056043710},{"_id":"source/_posts/Spark-on-Yarn源码解析(二)Spark-Submit解析.md","hash":"473a37d6b659a4bc65c257903d3a56867641e011","modified":1536055994301},{"_id":"source/_posts/Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分.md","hash":"9927d1b3db323ec3ef7f9929cdf8e2acb3c7e7b9","modified":1536056175370},{"_id":"source/_posts/SparkRDD介绍.md","hash":"7df3b0bc2909f3a81d3c2f1cb50be0dc23d248c1","modified":1535074450728},{"_id":"source/_posts/SparkSQL介绍.md","hash":"79adce13224b71e06ec4c5b6d92212084e70247a","modified":1534729093848},{"_id":"source/_posts/SparkStreaming介绍.md","hash":"2be6a31b51311d78605bb377b1fb46280050879a","modified":1534356580373},{"_id":"source/_posts/SparkStreaming消费Kafka数据.md","hash":"cb0c59bb44ac652e46492a99a26a46c650c762c3","modified":1534729158490},{"_id":"source/_posts/Spark启动流程及一些小总结.md","hash":"9ec83cd37e9f2d3c605339b9ce9decb9d41d880f","modified":1535043045753},{"_id":"source/_posts/Spark本地调试远程集群程序.md","hash":"7bb1024832a440e323b6529de22190ea4af4910e","modified":1533925783521},{"_id":"source/_posts/Spark算子.md","hash":"e6ad729d25ce2fa4bf403f89f3f1f973952b3345","modified":1534729482481},{"_id":"source/_posts/Spark读取HBase解析json创建临时表录入到Hive表.md","hash":"d1bdc3e041243c9ed92ce6c0f2408c2a65d322eb","modified":1534140998124},{"_id":"source/_posts/Spark读取Hbase.md","hash":"f8910b0b395f9ace9e5955c9e1a56604be1e0000","modified":1533923807563},{"_id":"source/_posts/elk容器的搭建.md","hash":"e7a000659e41efe3b5a0c8bccb21d738a1543f10","modified":1533923432279},{"_id":"source/_posts/es测试命令.md","hash":"a13d639a83eea01e4ed2d97565c52f667b1ef1df","modified":1533924133602},{"_id":"source/_posts/flink容器搭建.md","hash":"0019bb4339447066b460ac18473cbfe58611eda1","modified":1533610378322},{"_id":"source/_posts/git命令总结.md","hash":"e1a6abe33932133099f5c3c78fb797c63cb4dc00","modified":1533924076275},{"_id":"source/_posts/hadoop-spark集群搭建.md","hash":"60d4c71031649f3ae593b0c646190f27445249e1","modified":1533917267069},{"_id":"source/_posts/hbasezk容器的搭建.md","hash":"d0ebe2765ae2caa776e85a83e63b9109868aa61c","modified":1534343644875},{"_id":"source/_posts/hue搭建.md","hash":"c1687d78a6b4ee7c4b740245d9868d7a0adaa2fb","modified":1533923059068},{"_id":"source/_posts/json-tool使用.md","hash":"74838d075d6711a171723fd79e8d621e3dc2f302","modified":1533923216174},{"_id":"source/_posts/kafka启动脚本及命令.md","hash":"77c8610aff7578465a2ae7645e7bb2f18ed83a01","modified":1534246576094},{"_id":"source/_posts/mainlib.db","hash":"c941949a00625e5b7bc35a0fdc5741d8632c6a5f","modified":1535598845599},{"_id":"source/_posts/spring集成权限校验.md","hash":"231399cd65875286327274bd3968048cc02e86bc","modified":1526559682941},{"_id":"source/_posts/zookeeper配置.md","hash":"121b8b2bd13f0acf3f843a78ff23678a9f980062","modified":1533923895529},{"_id":"source/_posts/博客修改备份.md","hash":"66481baff4799003e116004f24a9a4232e4ef27c","modified":1526573926428},{"_id":"source/_posts/博客源文件提交到仓库.md","hash":"a57d89b78eea7cdbdc2f401bfb963ec140bedbe3","modified":1526924643947},{"_id":"source/_posts/各种快捷键.md","hash":"5856520f1cb01f9e789fae03c061f1cf5cbd1bb8","modified":1533926441685},{"_id":"source/_posts/大数据命令积累.md","hash":"411d8401f4ba2959027898ac226b4a4b78026a24","modified":1533579609751},{"_id":"source/_posts/转移Github博客到云服务器.md","hash":"7198dafdd90d456abbbd2fca141f773b61a5a2f2","modified":1533925934487},{"_id":"source/about/index.md","hash":"eb6e7d592f6298be88301cb3adbc5698ed80d266","modified":1533626870398},{"_id":"source/categories/index.md","hash":"13f3d932ad7c8e68ce9c5fe489de1e2398d031f9","modified":1533626884841},{"_id":"source/tags/index.md","hash":"b5cea9db4620416c871ea1b14b7eddf23b330779","modified":1533626849135},{"_id":"themes/icarus/.github/ISSUE_TEMPLATE.md","hash":"9393fd3dbc943f1544facb66af7fd8b7a5b9ddbb","modified":1525046792000},{"_id":"themes/icarus/_source/.DS_Store","hash":"658236c4f1e31d31ece1f508d28f9e361113fec8","modified":1526566930307},{"_id":"themes/icarus/languages/en.yml","hash":"ade241498b85503a8953a1deca963222f47067a7","modified":1525046792000},{"_id":"themes/icarus/languages/es.yml","hash":"d7432219be5bee4cb569331378ade61b749688e0","modified":1525046792000},{"_id":"themes/icarus/languages/fr.yml","hash":"cb3e597cbec7e8f458858c457bafd1f3a225083d","modified":1525046792000},{"_id":"themes/icarus/languages/id.yml","hash":"e4961da507f66c8f7c37f85653a9437cb2056bf6","modified":1525046792000},{"_id":"themes/icarus/languages/ja.yml","hash":"ff972961e5f468a695d80d21b62c3e9032cdf561","modified":1525046792000},{"_id":"themes/icarus/languages/ko.yml","hash":"7c4ad4577dc0577ad2ca1c0410507f5e5fadf530","modified":1525046792000},{"_id":"themes/icarus/languages/pt-BR.yml","hash":"3c5d5293575593705b9a2dfa9d97b017eb4bc8c3","modified":1525046792000},{"_id":"themes/icarus/languages/ru.yml","hash":"d1aab2b0c939d0c6020f881d664b660a01ee7327","modified":1525046792000},{"_id":"themes/icarus/languages/tr.yml","hash":"8b7eb6aec264db50dbabea89f680acca256f4cd1","modified":1525046792000},{"_id":"themes/icarus/languages/zh-CN.yml","hash":"3dc8ec524805afd090438be717908750da439204","modified":1525046792000},{"_id":"themes/icarus/languages/zh-TW.yml","hash":"d8d96a0a17c20af11919ce036e87379a6b163db9","modified":1525046792000},{"_id":"themes/icarus/layout/.DS_Store","hash":"bac1aa5ca885fbe18abeff0d8ab7943faefb192e","modified":1533619095280},{"_id":"themes/icarus/layout/categories.ejs","hash":"aa95629b770cff8cca9d663aeb6b17928f070de5","modified":1525046792000},{"_id":"themes/icarus/layout/archive.ejs","hash":"c1ecf667f40f34d61ab33eed46bab143eb1af36d","modified":1525046792000},{"_id":"themes/icarus/layout/category.ejs","hash":"1d407f9176db84e83062c52ad4755aaea9e74401","modified":1525046792000},{"_id":"themes/icarus/layout/cnzz.ejs","hash":"fbf1e89ac35c61ba4d269eb2f252bcfadac77b45","modified":1526708105585},{"_id":"themes/icarus/layout/index.ejs","hash":"43e971ebc35657b18e08a049559790348a16666f","modified":1525046792000},{"_id":"themes/icarus/layout/layout.ejs","hash":"5f641b03687021a6c21b15b599e5e3e9c5a1ae49","modified":1533623916056},{"_id":"themes/icarus/layout/page.ejs","hash":"50170783bac99946ae8af483920568de9b2d9801","modified":1525046792000},{"_id":"themes/icarus/layout/post.ejs","hash":"50170783bac99946ae8af483920568de9b2d9801","modified":1525046792000},{"_id":"themes/icarus/layout/tag.ejs","hash":"f6c220d4e5c231028bc71ddc11aec97d7b5a9943","modified":1525046792000},{"_id":"themes/icarus/layout/tags.ejs","hash":"b0fcea68d7c11e5899bf0375d80997685111653f","modified":1525046792000},{"_id":"themes/icarus/node_modules/.DS_Store","hash":"311cbe86a19c3daf63b9c408945b38b3696b1607","modified":1526575009450},{"_id":"themes/icarus/scripts/meta.js","hash":"1993754a2f3dffa283fa0538eb8f056385b69ad4","modified":1525046792000},{"_id":"themes/icarus/scripts/thumbnail.js","hash":"e667a611f9baac270281b765832020d50bf8fb7f","modified":1525046792000},{"_id":"themes/icarus/source/.DS_Store","hash":"9ead42a141966d6cb9390e6f9007d9d4196ccc2e","modified":1526631193066},{"_id":"themes/icarus/node_modules/underscore.string/libpeerconnection.log","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1373532129000},{"_id":"source/_posts/.MWebMetaData/0713430F6F73E8D6849789E2EE6FEE7E.data","hash":"96b0f6229fd61e8ddd82cc20c4572cbd0a187be9","modified":1536111585626},{"_id":"source/_posts/.MWebMetaData/1615C43C4C762BC00BE3D0337BB8112E.data","hash":"308ddcdeb6e06b21827e92196a486646c866c580","modified":1536111567158},{"_id":"source/_posts/.MWebMetaData/558CCDAEE9F3FA7F9199CDC1CC1159F0.data","hash":"d8f9caf8f7a7e4adc9f035bf1aa78639c4afe606","modified":1536111548184},{"_id":"source/_posts/.MWebMetaData/75F0A7DCF2D367D3E05BF96373D16CEE.data","hash":"3fab6a0d6d21ada2bda1f3b92d08a85b6131e1bd","modified":1536111472159},{"_id":"source/_posts/attachments/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1534346407289},{"_id":"source/_posts/images/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1526627006441},{"_id":"source/_posts/metadata/134A9FDBB9C2E6477B875B34D30A8D9B.data","hash":"3ab5ab0051828cbe29e21eafb50a87874c3bcfdf","modified":1535598682768},{"_id":"source/_posts/metadata/ED08FB4969FF179E2535A693EE43079D.data","hash":"228bf2245398b82113d2e2da3fd00cee65f85b1b","modified":1535596466462},{"_id":"source/_posts/metadata/publish-sh-15355953057944.data","hash":"ba870d31b166135b759023f10db72f41f5ba4357","modified":1535598831108},{"_id":"themes/icarus/_source/about/index.md","hash":"19e20df31036b52d254cb64c973a423a55f3fa22","modified":1533626600211},{"_id":"themes/icarus/_source/categories/index.md","hash":"13f3d932ad7c8e68ce9c5fe489de1e2398d031f9","modified":1533626590288},{"_id":"themes/icarus/_source/tags/index.md","hash":"b5cea9db4620416c871ea1b14b7eddf23b330779","modified":1533626614937},{"_id":"themes/icarus/layout/comment/changyan.ejs","hash":"b4bd2e641af59e06becaa8f2dc97867f023b8dcf","modified":1525046792000},{"_id":"themes/icarus/layout/comment/counter.ejs","hash":"57c194d0fa268ce2f3e9c755b3154b8a0709f613","modified":1525046792000},{"_id":"themes/icarus/layout/comment/disqus.ejs","hash":"1b32a90f400dc580f4b8298de75b94429ca6de68","modified":1525046792000},{"_id":"themes/icarus/layout/comment/duoshuo.ejs","hash":"ce46d7410a99b57704da32e9d09071cef6c9fa93","modified":1525046792000},{"_id":"themes/icarus/layout/comment/facebook.ejs","hash":"5ee16430a4435c2fead0275ff83fc98092d73d4c","modified":1525046792000},{"_id":"themes/icarus/layout/comment/gitment.ejs","hash":"c7998209b9a766eeba6976dd4bdffb1f87469358","modified":1525046792000},{"_id":"themes/icarus/layout/comment/index.ejs","hash":"0bce9ca0767b49d2a9fc026a6004a8526a20624b","modified":1525046792000},{"_id":"themes/icarus/layout/comment/isso.ejs","hash":"4f8b81ff5bb418ec11ce080d515f247bfe436014","modified":1525046792000},{"_id":"themes/icarus/layout/comment/livere.ejs","hash":"792e4f0e93b2bdc5abd85d447d804a5c608a9c5c","modified":1525046792000},{"_id":"themes/icarus/layout/comment/scripts.ejs","hash":"d45f652268671de96c86d8ca50b9deaf011274f0","modified":1525046792000},{"_id":"themes/icarus/layout/comment/valine.ejs","hash":"0ea90a606f0d386a5ae0da83c0501cbb25319fde","modified":1525046792000},{"_id":"themes/icarus/layout/comment/youyan.ejs","hash":"6fe807992832939caf6c3e7651d052df9520d88e","modified":1525046792000},{"_id":"themes/icarus/layout/common/.DS_Store","hash":"0af4458db0eb004a5a78c033931219532df46e65","modified":1526575885106},{"_id":"themes/icarus/layout/common/article.ejs","hash":"68828ead3781cac0dd67efd8efc42e4743122094","modified":1526730074964},{"_id":"themes/icarus/layout/common/cnzz.ejs","hash":"1470dfcb83377e314c364124b2bc8bf6b61b56d6","modified":1526710715776},{"_id":"themes/icarus/layout/common/donate.ejs","hash":"3a7e0d4d1fd9f99a858b6cef2ecc769e7f550238","modified":1526722850010},{"_id":"themes/icarus/layout/common/footer.ejs","hash":"92cbffd312f993f76ff2105aeae554884ee6b39c","modified":1526798425843},{"_id":"themes/icarus/layout/common/head.ejs","hash":"34dff5f2c6eaf8c95d821045f3302927dbcf3bd4","modified":1526710816721},{"_id":"themes/icarus/layout/common/header.ejs","hash":"70eba183ed59a48bb1893c67099184e33ef77f89","modified":1526574105222},{"_id":"themes/icarus/layout/common/profile.ejs","hash":"bfd23e65436315fd5127f97cfc30ec36c8a2b992","modified":1533929355260},{"_id":"themes/icarus/layout/common/scripts.ejs","hash":"c0a1a9e53f89440c42c325d5bd8c7234652c8937","modified":1525046792000},{"_id":"themes/icarus/layout/common/sidebar.ejs","hash":"1ee2384f98c8ccaa7a5b00f14ad2a9a67022c143","modified":1525046792000},{"_id":"themes/icarus/layout/common/thumbnail.ejs","hash":"1b70f8a98cd8650b159bda858dbee38dbdb7f0c5","modified":1525046792000},{"_id":"themes/icarus/layout/common/timeline.ejs","hash":"6420e34e0332c9b6670011519f341340db989343","modified":1525046792000},{"_id":"themes/icarus/layout/plugin/baidu-analytics.ejs","hash":"6a7bee18e666e627e62541a5e30906f87ba1bfe8","modified":1525046792000},{"_id":"themes/icarus/layout/plugin/google-analytics.ejs","hash":"349f08b6521a16e79046b1f94f04317ac74f556e","modified":1525046792000},{"_id":"themes/icarus/layout/plugin/scripts.ejs","hash":"e22f99652a220d926103801a3cad55ea9c450e05","modified":1525046792000},{"_id":"themes/icarus/layout/search/baidu.ejs","hash":"3e603a702d20c53fd3bcbeb570a16a86d54781ce","modified":1525046792000},{"_id":"themes/icarus/layout/search/index-mobile.ejs","hash":"50a727ac1dfe3073eb6fa6699ba01e66f4ac41c0","modified":1525046792000},{"_id":"themes/icarus/layout/search/index.ejs","hash":"40fede3dd906b2bb0707a0a15df09a6cdaa01d97","modified":1526567999520},{"_id":"themes/icarus/layout/search/insight.ejs","hash":"130fe3d33ac71da0b50f7fee6a87979f30938a1b","modified":1525046792000},{"_id":"themes/icarus/layout/search/swiftype.ejs","hash":"379e66d2c13526e72e4120c443f95fccf4edef71","modified":1525046792000},{"_id":"themes/icarus/layout/share/addtoany.ejs","hash":"ac180c4c84b73a04d61b17e7dc18c257e20bf59f","modified":1525046792000},{"_id":"themes/icarus/layout/share/bdshare.ejs","hash":"a1e772c5a6f174d585b0c1e574058f75dc8e2898","modified":1525046792000},{"_id":"themes/icarus/layout/share/default.ejs","hash":"ebfb919dc525b3ed61a6a5ee05ee71410eedc541","modified":1525046792000},{"_id":"themes/icarus/layout/share/index.ejs","hash":"2a2c0095b95b11e5692bd8ad6a2337aa644189a2","modified":1525046792000},{"_id":"themes/icarus/layout/share/jiathis.ejs","hash":"21ebaa51e828cba2cefbeeaccb01514643565755","modified":1525046792000},{"_id":"themes/icarus/layout/widget/archive.ejs","hash":"5abb2ffc73e09ed2851d8b266469254f1211983a","modified":1526798331842},{"_id":"themes/icarus/layout/widget/category.ejs","hash":"faec0d20972d148f071a6e4a1833773e1defc79a","modified":1526798324685},{"_id":"themes/icarus/layout/widget/links.ejs","hash":"d05dcf4b710b631f18cbd2ba8e5deac2f1a590da","modified":1526798334036},{"_id":"themes/icarus/layout/widget/recent_posts.ejs","hash":"670655917158497cdfd321b909956dc757c2ef0c","modified":1526659613614},{"_id":"themes/icarus/layout/widget/tag.ejs","hash":"95be419ba4f5d7f3b863f0c47684487044daba1a","modified":1526659605211},{"_id":"themes/icarus/layout/widget/tagcloud.ejs","hash":"fbec1b1c64f83da68dbd86a3087e185a50eef566","modified":1526659596158},{"_id":"themes/icarus/node_modules/.bin/which","hash":"5b6b3e1838316fb3f1b3b4194cdf49db0674eb17","modified":1526003751967},{"_id":"themes/icarus/node_modules/bluebird/.DS_Store","hash":"5a2eb10bf0f429edd53d534258c8f887360a7b75","modified":1526575009452},{"_id":"themes/icarus/node_modules/bluebird/LICENSE","hash":"59002342e7a5468c5b5b9ae6fb4eb41bbc7f33ae","modified":1488577289000},{"_id":"themes/icarus/node_modules/bluebird/README.md","hash":"4e1269bd88d06bc10978da978e28e4b2276a2450","modified":1493061424000},{"_id":"themes/icarus/node_modules/bluebird/changelog.md","hash":"bc6df7caea3b83fa361fa0a96c9b34c09465a93a","modified":1472620968000},{"_id":"themes/icarus/node_modules/bluebird/package.json","hash":"08ab4c8f2fe841911b2a7229f3e8d2382049736b","modified":1526003751949},{"_id":"themes/icarus/node_modules/camel-case/LICENSE","hash":"3a7f886b632d2197676a40020d354e84b7860604","modified":1465681882000},{"_id":"themes/icarus/node_modules/camel-case/camel-case.d.ts","hash":"30e10eb66736cbb97c906af8b2ceb7b4755e6259","modified":1465681882000},{"_id":"themes/icarus/node_modules/camel-case/camel-case.js","hash":"6828b41a59ec548d513d5a7f1c91f7e683f926ce","modified":1465692324000},{"_id":"themes/icarus/node_modules/camel-case/package.json","hash":"b4a176a25465a6a6d1a77d77ccdd1713e3453b50","modified":1526003751945},{"_id":"themes/icarus/node_modules/cross-spawn/LICENSE","hash":"45c1476739d0c028c845b2c90c401c3a4435de04","modified":1474803762000},{"_id":"themes/icarus/node_modules/cross-spawn/README.md","hash":"179da75e34197ceca8bc1b79ce95c9ab7f13c43c","modified":1474803762000},{"_id":"themes/icarus/node_modules/cross-spawn/index.js","hash":"db81cb7b0c65f5b0dfe1b81f2217a6277bc4822a","modified":1474803762000},{"_id":"themes/icarus/node_modules/cross-spawn/package.json","hash":"2113206706b49cf315e2ce73e8ca41c7f6247835","modified":1526003751950},{"_id":"themes/icarus/node_modules/hexo-generator-json-content/.npmignore","hash":"fa3b2eb70de258c0efb3efac3dceb5abd7c92666","modified":1479839219000},{"_id":"themes/icarus/node_modules/hexo-generator-json-content/LICENSE","hash":"d9ae358aefebd8c2242c55044bc7fb9edbae3b0a","modified":1479839219000},{"_id":"themes/icarus/node_modules/hexo-generator-json-content/README.md","hash":"c884c52fdda83064e889dc7e116cb63a223e5558","modified":1479839219000},{"_id":"themes/icarus/node_modules/hexo-generator-json-content/index.js","hash":"37e7ce344478ab903c825cc9b2173d76ce47e298","modified":1479840141000},{"_id":"themes/icarus/node_modules/hexo-generator-json-content/package.json","hash":"00cebd4f4991fbfed19150363434175d894b6009","modified":1526003751950},{"_id":"themes/icarus/node_modules/hexo-util/.eslintignore","hash":"0abf1c392f32bdf193fe824ead96926f550a85ae","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/.eslintrc","hash":"2c4a55a386408308d637f658a4b2f5e82f4e58db","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/.jscsrc","hash":"5f410f9c9caedfdd1eabb1571de1f8f9014eefb6","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/.npmignore","hash":"8e028ac8e9925655e0f8f3df33ed78df98334074","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/CHANGELOG.md","hash":"aa50cb035f402b4e58e4053ef5c541601153fc3f","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/LICENSE","hash":"fecd013bdadc9ead2732027f06bf8fc19761efcb","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/README.md","hash":"65707c36c6bd911b9e4a7b07cb5603954faf5afd","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/highlight_alias.json","hash":"1b97a5e9c73b876d73375cf36106e74b89dea155","modified":1513837686000},{"_id":"themes/icarus/node_modules/hexo-util/package.json","hash":"01aea8db6d0fac1892f49c6765d7331c749d38bf","modified":1526003751950},{"_id":"themes/icarus/node_modules/highlight.js/LICENSE","hash":"cd25196630fe891662ad77810f0f6dee5bc85ddc","modified":1496198853000},{"_id":"themes/icarus/node_modules/hexo-util/yarn.lock","hash":"f7ffba7f02541ecdb34c63ba45e43754e0504d10","modified":1513819544000},{"_id":"themes/icarus/node_modules/highlight.js/README.md","hash":"087887d419ba2c138058e6fac56c24d838bb4114","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/package.json","hash":"a1079fc5c1bc0f674a77c152843b18e93e9573bf","modified":1526003751949},{"_id":"themes/icarus/node_modules/html-entities/LICENSE","hash":"f10f3a5e9b16a526fbf6abc4be406e6f07ecad93","modified":1408403203000},{"_id":"themes/icarus/node_modules/html-entities/README.md","hash":"15dea62352c313d8eee8b0069a5b0a8a9294e5c6","modified":1493034265000},{"_id":"themes/icarus/node_modules/html-entities/index.js","hash":"eb15b4fed782c7736a02ad14c97901080ab85f44","modified":1421057245000},{"_id":"themes/icarus/node_modules/html-entities/package.json","hash":"06e51dce8dc11418c29fc8a91bb77ff0ff100b05","modified":1526003751949},{"_id":"themes/icarus/node_modules/isexe/.npmignore","hash":"5c82cb364ccc42139031fc6519c138ef81f2df26","modified":1453008305000},{"_id":"themes/icarus/node_modules/isexe/LICENSE","hash":"bb408e929caeb1731945b2ba54bc337edb87cc66","modified":1454992766000},{"_id":"themes/icarus/node_modules/isexe/README.md","hash":"5245f0db9a9f95b37f4cac50e75dc66e2ea8e1a5","modified":1490230327000},{"_id":"themes/icarus/node_modules/isexe/index.js","hash":"9348ece80fd6208f0b8740d43cd652db4a5f06e6","modified":1490229856000},{"_id":"themes/icarus/node_modules/isexe/mode.js","hash":"5f33b53cc6b89f9ebe2ebd1dcfeb434cd96a3192","modified":1490230193000},{"_id":"themes/icarus/node_modules/isexe/package.json","hash":"610a13811b62bb1521fbd62a5fababba60a6878b","modified":1526003751950},{"_id":"themes/icarus/node_modules/isexe/windows.js","hash":"1a6d0c635f67223d5e3890068f32f2da46e45151","modified":1490229931000},{"_id":"themes/icarus/node_modules/keyword-extractor/.npmignore","hash":"1061654723a72f7aa74896d084c9e31fa5f61733","modified":1404913186000},{"_id":"themes/icarus/node_modules/keyword-extractor/LICENSE","hash":"449a0ef15761526ad764889d4e8fffb05a01bcb1","modified":1404913186000},{"_id":"themes/icarus/node_modules/keyword-extractor/Makefile","hash":"c2b750775cd3ab94e3a1420dd128e74e0106ac0c","modified":1404913186000},{"_id":"themes/icarus/node_modules/keyword-extractor/README.md","hash":"a9dfc92f6ebd45aa67267cf4149e0da54dd47127","modified":1513274003000},{"_id":"themes/icarus/node_modules/keyword-extractor/index.js","hash":"cd67fd429bb78e1ae525908abfe9e4731d506f5e","modified":1404913186000},{"_id":"themes/icarus/node_modules/keyword-extractor/package.json","hash":"20825b562b1c70d2747c47e33b55967785424f5c","modified":1526003751950},{"_id":"themes/icarus/node_modules/lower-case/LICENSE","hash":"3a7f886b632d2197676a40020d354e84b7860604","modified":1487963775000},{"_id":"themes/icarus/node_modules/lower-case/README.md","hash":"69ed872fccaa2ae235890bf01958115a178ca713","modified":1487963775000},{"_id":"themes/icarus/node_modules/lower-case/lower-case.d.ts","hash":"f6cc8841b3395a746a5984327edb3c6411302b11","modified":1487963775000},{"_id":"themes/icarus/node_modules/lower-case/lower-case.js","hash":"6c8e4acdd39af940362a63b3fdd40d98bce5b073","modified":1487963775000},{"_id":"themes/icarus/node_modules/lower-case/package.json","hash":"2ce4a6b446b4c17ec980fd47163c79b522c6192f","modified":1526003751949},{"_id":"themes/icarus/node_modules/lru-cache/LICENSE","hash":"bb408e929caeb1731945b2ba54bc337edb87cc66","modified":1431999527000},{"_id":"themes/icarus/node_modules/lru-cache/README.md","hash":"23e985d22bf54841a67846263f0da11cf315e615","modified":1520531848000},{"_id":"themes/icarus/node_modules/lru-cache/index.js","hash":"5d1ecc9cd71a0e10af4e592bec02f943e1cda3b6","modified":1525734908000},{"_id":"themes/icarus/node_modules/lru-cache/package.json","hash":"8127a7922bf7ce33ff3bd294abeb767f5446e3d4","modified":1526003751950},{"_id":"themes/icarus/node_modules/moment/CHANGELOG.md","hash":"cf165a9553be0ebf622d3dfd7f499de07f2ae709","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/LICENSE","hash":"aab97739ef7d50750adbc9ffbfd1cbf9608eb678","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/README.md","hash":"31b38ea6b87c3a24a16c3230cb21bc4e678cb1f9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/ender.js","hash":"f26dfccd69e35b733a945a083bb64b20bf968bb3","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/moment.d.ts","hash":"649e2b4e650569b8dfe0004a667dcb3db02dbdc3","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/package.js","hash":"ea880b139fa5aad30129356d4c2b61c7acad7452","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/package.json","hash":"2de64755491e0755b09e5cd512cde828b83405e9","modified":1526003751950},{"_id":"themes/icarus/node_modules/no-case/LICENSE","hash":"3a7f886b632d2197676a40020d354e84b7860604","modified":1465675303000},{"_id":"themes/icarus/node_modules/no-case/README.md","hash":"c45012f78b5fb4f4423a5e087603003ab34d43b4","modified":1504890971000},{"_id":"themes/icarus/node_modules/no-case/no-case.d.ts","hash":"fd30ef64614965e6d611b05aaa579e1abb2bfa70","modified":1465680417000},{"_id":"themes/icarus/node_modules/no-case/no-case.js","hash":"9c9bed3c4f4bd9cd9c74b8384e17170677e7cdc3","modified":1483587492000},{"_id":"themes/icarus/node_modules/no-case/package.json","hash":"f426bea4f4c2a45df90dad8b10fa5b0cb159de05","modified":1526003751950},{"_id":"themes/icarus/node_modules/pseudomap/LICENSE","hash":"bb408e929caeb1731945b2ba54bc337edb87cc66","modified":1448659987000},{"_id":"themes/icarus/node_modules/pseudomap/README.md","hash":"93ca8dd88b562f1cd856143e500bdb679419e088","modified":1448749520000},{"_id":"themes/icarus/node_modules/pseudomap/map.js","hash":"f0d63208dec47fe1d9c69ab74fdf54298406dce4","modified":1451963009000},{"_id":"themes/icarus/node_modules/pseudomap/package.json","hash":"3e67fbb339634c1bce0c340d8b09200ae914d19d","modified":1526003751950},{"_id":"themes/icarus/node_modules/pseudomap/pseudomap.js","hash":"34cddc0bc6daaf6c7092d2659612969b8700d8fa","modified":1451963009000},{"_id":"themes/icarus/node_modules/striptags/.npmignore","hash":"631c0bbafc06a9acdf32b5f1d0a32b425d5327a9","modified":1484106258000},{"_id":"themes/icarus/node_modules/striptags/.travis.yml","hash":"d9c5d47c7c006d737ad01c2a1f10d42206189a33","modified":1484743147000},{"_id":"themes/icarus/node_modules/striptags/LICENSE","hash":"88d9c86133613e38d3fcdc5766654e81d0cf0205","modified":1484716669000},{"_id":"themes/icarus/node_modules/striptags/README.md","hash":"312dad4632c3d8c145980983451c551bfcf1c352","modified":1484743376000},{"_id":"themes/icarus/node_modules/striptags/bower.json","hash":"721bd625f002394f7422251062ee4cd8e409388c","modified":1484106258000},{"_id":"themes/icarus/node_modules/striptags/package.json","hash":"56519eb6c3c3722387a29aedeee24e4d533cec72","modified":1526003751950},{"_id":"themes/icarus/node_modules/striptags/striptags.js","hash":"b76eaea01c6f86d3a6f187ca1e4b30e7f059ab07","modified":1484743003000},{"_id":"themes/icarus/node_modules/underscore/LICENSE","hash":"4a536006299d219843f69f87ecac4b52158b3efc","modified":1392067000000},{"_id":"themes/icarus/node_modules/underscore/README.md","hash":"fb13eed34314e90170be6290876a9a66c4051272","modified":1390328510000},{"_id":"themes/icarus/node_modules/underscore/package.json","hash":"eecd3c235c5bf9dfb05422ddf7c7e05888498e13","modified":1526003751950},{"_id":"themes/icarus/node_modules/underscore/underscore-min.js","hash":"fb26909af4ad2a6c240b9aa4b35bb983cf4b20e4","modified":1409091109000},{"_id":"themes/icarus/node_modules/underscore.string/.travis.yml","hash":"e33e8d35cf94d67804af457d709d8db1d16f73ae","modified":1373449276000},{"_id":"themes/icarus/node_modules/underscore/underscore.js","hash":"07147a83bfaba702d30426c7c31bccb25e692a2e","modified":1409091073000},{"_id":"themes/icarus/node_modules/underscore.string/Gemfile","hash":"dab95a91b5a30ded482c35d9efbc00d33a9322aa","modified":1373532016000},{"_id":"themes/icarus/node_modules/underscore.string/Gemfile.lock","hash":"9236e4ffa7c8ca067369e9b77bd26bc850fd51ea","modified":1373532016000},{"_id":"themes/icarus/node_modules/underscore.string/README.markdown","hash":"d5862c720a95a582201ff26ff11bdf44911dde36","modified":1373890137000},{"_id":"themes/icarus/node_modules/underscore.string/Rakefile","hash":"aebecba3b976b65bb74d63704f37d6f74d7419cf","modified":1373532016000},{"_id":"themes/icarus/node_modules/underscore.string/component.json","hash":"b929b12ac3a7bdf402fa87863621259a39a883c2","modified":1373890112000},{"_id":"themes/icarus/node_modules/underscore.string/package.json","hash":"ffbcbfd6267ac90c85561d8e8a5f0fd2c795a9b5","modified":1526003751950},{"_id":"themes/icarus/node_modules/upper-case/LICENSE","hash":"3a7f886b632d2197676a40020d354e84b7860604","modified":1450225475000},{"_id":"themes/icarus/node_modules/upper-case/README.md","hash":"2eeec02b0cf6dfdd2881c754a3ef3340f7cbc348","modified":1450225513000},{"_id":"themes/icarus/node_modules/upper-case/package.json","hash":"ffc322ae5e76b0f2b94f96d1f7fb756666b05fb5","modified":1526003751950},{"_id":"themes/icarus/node_modules/upper-case/upper-case.d.ts","hash":"a5610dcff5a1a1f54b45b84e2f82b8393aeaa682","modified":1450225534000},{"_id":"themes/icarus/node_modules/upper-case/upper-case.js","hash":"cc2dbbcf88c099a8c8a4db4c424e663ce9826a5b","modified":1450225475000},{"_id":"themes/icarus/node_modules/which/CHANGELOG.md","hash":"a7bf611b2407953d87e7a4618a18ed3462fc22be","modified":1501548883000},{"_id":"themes/icarus/node_modules/which/LICENSE","hash":"bb408e929caeb1731945b2ba54bc337edb87cc66","modified":1416911659000},{"_id":"themes/icarus/node_modules/which/README.md","hash":"880b21360637c36418d55dd08211f1017c881df1","modified":1501540268000},{"_id":"themes/icarus/node_modules/which/package.json","hash":"38edbdd4cc3efffcd0cd1c52cb07f2c393e1de42","modified":1526003751951},{"_id":"themes/icarus/node_modules/which/which.js","hash":"f39455d215c8d397782f1efbc6e52254d0f9a4ab","modified":1501540268000},{"_id":"themes/icarus/node_modules/yallist/LICENSE","hash":"bb408e929caeb1731945b2ba54bc337edb87cc66","modified":1450425977000},{"_id":"themes/icarus/node_modules/yallist/iterator.js","hash":"baf1ff1052a38543b84d4bba85a62dc78b1ab107","modified":1489370602000},{"_id":"themes/icarus/node_modules/yallist/README.md","hash":"930460a06c3414c2dceeeca378fbe0057779fab5","modified":1450554690000},{"_id":"themes/icarus/node_modules/yallist/package.json","hash":"1a14d0c816bf69c9e0d462506f3da060e5692d31","modified":1526003751950},{"_id":"themes/icarus/node_modules/yallist/yallist.js","hash":"c042dba0e8d1672a56531936a03533f5aaaca9de","modified":1489443354000},{"_id":"themes/icarus/source/css/.DS_Store","hash":"c42d34be6966df9610fc60c5bc1cca4ee1d64be2","modified":1526746324375},{"_id":"themes/icarus/source/css/_extend.styl","hash":"539e02107f35e8b3bdb9bf160dc212a433a7b60e","modified":1526568030155},{"_id":"themes/icarus/source/css/_variables.styl","hash":"f2efa544bfe27b740e214bbc23f5189ce21060f1","modified":1533625338431},{"_id":"themes/icarus/source/css/style.styl","hash":"e5fb456c5bc472c79c56c56ecf96dc756e949e01","modified":1533624126660},{"_id":"themes/icarus/source/js/insight.js","hash":"4ecedcac2b0ed717efcb8ddd8c688e366f8d58b1","modified":1526621433066},{"_id":"themes/icarus/source/js/main.js","hash":"2c148f06c5799b5d7dd165c5162e780535e07a40","modified":1525046792000},{"_id":"themes/icarus/layout/common/post/banner.ejs","hash":"47ced3f03525698c79c6b1c07b48383fb6c496b2","modified":1525046792000},{"_id":"themes/icarus/layout/common/post/category.ejs","hash":"75c9dda2e7ec041943855ca163a6b1c4c8b4f260","modified":1525046792000},{"_id":"themes/icarus/layout/common/post/date.ejs","hash":"45cb0bcad461036cdd1fe2e3fbb5f2f19940025c","modified":1525046792000},{"_id":"themes/icarus/layout/common/post/gallery.ejs","hash":"659f019761116313169148ec61773e7b84abb739","modified":1525046792000},{"_id":"themes/icarus/layout/common/post/nav.ejs","hash":"d7cd611e642327f33dff3963ef869c2b46824a11","modified":1525046792000},{"_id":"themes/icarus/layout/common/post/tag.ejs","hash":"2e966216256321aa0c76fe1b9be689601c76ef31","modified":1525046792000},{"_id":"themes/icarus/layout/common/post/title.ejs","hash":"669ddb46fefa100856588351a7a2d30ad996b755","modified":1525046792000},{"_id":"themes/icarus/node_modules/cross-spawn/lib/enoent.js","hash":"6318a907092ab4ff4ff31305e4a272f5fd877a9e","modified":1474803762000},{"_id":"themes/icarus/node_modules/cross-spawn/lib/hasBrokenSpawn.js","hash":"bb8d6bdbd5251c907d86a4d57cd6623b3ade53c0","modified":1474803762000},{"_id":"themes/icarus/node_modules/cross-spawn/lib/parse.js","hash":"d2fa758fe5a3f564c953b1e9203d4eea138102ab","modified":1474803762000},{"_id":"themes/icarus/node_modules/cross-spawn/lib/resolveCommand.js","hash":"2d7efa07faaa5429bd7ac215ec720fcc3d0657e9","modified":1474803762000},{"_id":"themes/icarus/node_modules/hexo-util/lib/cache_stream.js","hash":"9346587fa9871e9a4529592e91a997e5b0a41ae9","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/camel_case_keys.js","hash":"0e506d8ee3dfae6ab57d1a04c7e6e4d82287b1db","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/escape_diacritic.js","hash":"9b62a42bde7671daba738e6652baee44ba30d6ba","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/escape_html.js","hash":"f3c3f1a422cd2e519a805fbdbf5f20b63a10fd9a","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/escape_regexp.js","hash":"c74fc0d14a2930007336aa11e8af4224cefac86c","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/hash.js","hash":"95b9f5fe38c62a76eff26c6e9771a4267a0e284d","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/highlight.js","hash":"8d40d3394f4351a49ba5fc3a4b56d5b32e93d1b5","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/html_tag.js","hash":"38f1df0d200011332249bf599980f1476f723d9c","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/index.js","hash":"721f908f2c5e561a11cbee61f2f3b753066c1618","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/pattern.js","hash":"384485bf36b97513c7b61bff21af43570ca2c8fb","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/permalink.js","hash":"0ac9bb1eb47dcb79cff07a6631fe2714eac5fe1c","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/slugize.js","hash":"662f671305418218da6c22af093dafa90b21c674","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/spawn.js","hash":"8f3edb5b5268cf7d3abc1c86989d01da95f8a8f5","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/strip_html.js","hash":"f20be4f49702fd241dcbf65fa73f01ff42028550","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/truncate.js","hash":"102d57d757c88853ccc57e81747a843a0acb7eb7","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/scripts/build_highlight_alias.js","hash":"8a999b74cd11da29e6092a26469b29a770981436","modified":1513819513000},{"_id":"themes/icarus/node_modules/hexo-util/lib/word_wrap.js","hash":"68a9e133470558981bf038a75ef9f11c5145955b","modified":1513819513000},{"_id":"themes/icarus/node_modules/highlight.js/docs/api.rst","hash":"7ea61691713b45ca2bba06ccd64d382d05296ccd","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/docs/building-testing.rst","hash":"435f8700ad54ebdc199fea29bd5709cb50c16356","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/docs/css-classes-reference.rst","hash":"639242ca0eaa4a055b47fa12740da8287bc7c74d","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/docs/index.rst","hash":"64ff5bc891f7cc8bf28d987efe64328d981c6f25","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/docs/language-contribution.rst","hash":"755e23f46a3ff5d2f59ebef2a6c6d7a5317bcc40","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/docs/language-guide.rst","hash":"5a7cf267ab1661bef71913c7d16167043e3d2657","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/docs/language-requests.rst","hash":"d5de8f9936fa7450384696c4d2454e28c5102a6f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/docs/line-numbers.rst","hash":"fb2b891ec631ba4e18214d467114bfb026e42c5c","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/docs/reference.rst","hash":"e43d2d965426401ff22c72b60fcfe77a08b745ed","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/docs/release-process.rst","hash":"f33baf124c0d5fc87b00ba4d6a391ee8eff0900e","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/docs/style-guide.rst","hash":"6348488507d7deaf05b6c8c43470b033aef090d5","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/highlight.js","hash":"b7e74cd3626a38b9fa3e14ea310331e16e1d8df6","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/index.js","hash":"0d21a2bfc605735e074bd0fd91a69bff56b758fe","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/agate.css","hash":"8e122b0f00f5a7ec4e6dc492bf1560441eeef7f0","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/androidstudio.css","hash":"958baa24814c06a625612a3b2b478d54bc1bf1b1","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/arduino-light.css","hash":"c6e05580b51b755e229e99eb156940ad2cab192b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/arta.css","hash":"17b23b9fa57ef7a05a6aaeea9b5feb5442a8e584","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/ascetic.css","hash":"6358377b5c25667886aca0d605cbc497cf02405f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-cave-dark.css","hash":"f397d4418ce88b998841fd9135242461ba1a79b5","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-cave-light.css","hash":"2933f0247ac6d84c2954dd4946e359853abbf70a","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-dune-dark.css","hash":"081d73e454db140cd41b2bb595be297cfcab25e0","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-dune-light.css","hash":"7ba074de897e6a5e27d8b97f7cd06c1746474e72","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-estuary-dark.css","hash":"a0c46a0f955e3864f5e967ea93f5e61519a17be8","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-estuary-light.css","hash":"91ae4668c15a085ffce15ca21e93da445b5ecf3a","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-forest-dark.css","hash":"d8a4dc060b3fc719aa2f7d7b3f1019a3964b8101","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-forest-light.css","hash":"8a78a4eea0f32d094d1f9e316d59e990ba739d97","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-heath-dark.css","hash":"b93c2241ff123e62d4edb3dfc20410e4d1da3e78","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-heath-light.css","hash":"172b98f783d213a20211ec6aca9a3840ba524f55","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-lakeside-dark.css","hash":"fa707b252d5d5caccc0589374522bed47b7ca100","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-lakeside-light.css","hash":"8ba595b9ba6e8be6dc029bf80caab38e85aed686","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-plateau-dark.css","hash":"240f79f4e1fd63485c13900875b64c5a0d1bd06d","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-plateau-light.css","hash":"9731db1052f23351c983210701edd3f5ceed343f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-savanna-dark.css","hash":"09d45a218f87b8cb55b5ca7f4e9d76ea89a9404a","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-savanna-light.css","hash":"6bd3a62c32558476d436bd389500e5fbeb693d67","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-seaside-dark.css","hash":"b2800804a21f729a3d0a16b3aadc17679fd0639c","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-seaside-light.css","hash":"3e6b9e2a3a5de455490b8224401f19702df4cde9","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-sulphurpool-dark.css","hash":"1f006f8bd28e2ffbb73f708769605ba766787fff","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atelier-sulphurpool-light.css","hash":"0319c3eea893601b79b6c57652ef49b9222cc9de","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atom-one-dark.css","hash":"a6d28e1c04cee20cd874fc7ac0903d8e2e4bd54e","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/atom-one-light.css","hash":"eda63d8cce440dbf3bb823e10577a134e9941deb","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/brown-paper.css","hash":"a6817d890e58f80ce79d87620791dae821a70fff","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/codepen-embed.css","hash":"c4520e45d18259817b8942d17971f27c94f0fb09","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/color-brewer.css","hash":"96332573db854e7b7411caa94ba29b238fede2d3","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/darcula.css","hash":"7740224d07375ddc321147dffabbfa83e39f0d8b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/dark.css","hash":"fc77519d4f5d731054c5d4b7e7bbdbb510833271","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/darkula.css","hash":"0be948bb84acc05f93a1e5e9b48fe34cf61673a0","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/default.css","hash":"fba68624d1b34a5543fe0bf4b2af2ac1ddf65e74","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/docco.css","hash":"1be7be09a1b927c22c7f11451becdb335145bdd2","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/dracula.css","hash":"2633f2e84680e9f381e9ac1df344b542e28f9774","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/far.css","hash":"67e0658b2376e91e4894636a3522a30c2aec42de","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/foundation.css","hash":"75b0674dd1ed35d61977bd5c35dc29ca35835a7b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/github-gist.css","hash":"77cf684fda415812d6279a52f299c84850244778","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/github.css","hash":"ee593952684a791317ee8b77ad096e729dec649e","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/googlecode.css","hash":"fed3d439d0c305b337dd9c0f68dcbfa51429f445","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/grayscale.css","hash":"5688658c28fc5799517e8f3c224ae3da3797ba44","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/gruvbox-dark.css","hash":"aaf90d076e34bc44016462d70f83985e0e55c8dc","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/gruvbox-light.css","hash":"084699ab0aa326fede86e38bf41ebe49edde3a90","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/hopscotch.css","hash":"03e7b9ec74bbd375db8d476836aac3729a6c86e8","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/hybrid.css","hash":"1e2d54598b5f948b597059909d4bd158b7df021f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/idea.css","hash":"164649ae1e7c891a0d88cca075521af28656e2a1","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/ir-black.css","hash":"95aad65ba77183500ce0f7ad62a7535b647ee20c","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/kimbie.dark.css","hash":"58ed061c204fbc09d221ee9135d66bad976e5fc3","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/kimbie.light.css","hash":"b7aa9b1b21a25dfefcac7649a328962f84c47913","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/magula.css","hash":"ebc291f90e456abeeccb16937e7f51735733f4f7","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/mono-blue.css","hash":"fd0f39cd4c93ccb8d0bb2f6a1f359522e15a89cb","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/monokai-sublime.css","hash":"e4f3df87cbfa634c86b489e9cc43acf44d415986","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/monokai.css","hash":"f69df92e3ccdce28e6f811ec84f11698f2811198","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/obsidian.css","hash":"6ed475813fe4886a58b236297862ff2f181e66f7","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/ocean.css","hash":"fce6858e1c5eb9d2857cb5b5d24069c5994cfc91","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/paraiso-dark.css","hash":"8f5445327ce3c83b62f6bf4c1a0d87fa6f036341","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/paraiso-light.css","hash":"6eb3ab7a3337f9cad3a2e5fa6bf7dd83685228d8","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/pojoaque.css","hash":"4e1e6a431212f5043a550474a1acda63362cdd6f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/pojoaque.jpg","hash":"c5fe6533b88b21f8d90d3d03954c6b29baa67791","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/purebasic.css","hash":"f353a2cedf3f261a0676fce7c824bdd2e8197775","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/qtcreator_dark.css","hash":"213a40d203c4986cdbcb1bdf7d0b9013b29041ba","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/qtcreator_light.css","hash":"9a2a19ac2f6e6a7d5edd7fae67b7de4a3957e878","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/railscasts.css","hash":"a6d2043478fae5915926914cbd96fe9b706d98a6","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/rainbow.css","hash":"1b2d98ccdda36aa926d0e6d069b673fdacd2d33e","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/routeros.css","hash":"fc5db7c8f18d6b31ad92df21a51e7867d459af19","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/school-book.css","hash":"60fe3d8063b1acc4d52de02033095adb0a2b35a9","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/solarized-dark.css","hash":"d02fc2dcbeec4b7af2cadec4bbbfc5b016aed4c7","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/solarized-light.css","hash":"6b70caf1e84d096b1bc6318d5dae78d69e5dd1d3","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/sunburst.css","hash":"8309eab2e5b1765dbee81a626baacbdad869b76a","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/tomorrow-night-blue.css","hash":"cd257d7d6a37cd5a09419b5f5f9d34b6b282423f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/tomorrow-night-bright.css","hash":"04f0af30fdda5e5d6ebdeef5a860b6b7e49cfe89","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/tomorrow-night-eighties.css","hash":"d82b84bcda0588105dbbc0e8e8ba5e62c208a061","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/tomorrow-night.css","hash":"86264dd861d35a8b135f9fcb8ff2675e9fa69c16","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/tomorrow.css","hash":"163593ad70770d0296c5e643fa62e58e63f1b340","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/vs.css","hash":"2ac5e89ceb3d5a0e0fdab1ed6d9a411ec7d221aa","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/vs2015.css","hash":"3c7fa677de2a785d90fc6c3f7520ac1b11bfd37a","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/xcode.css","hash":"65d775a7e11e238c91e0d3c7370547348c92d6b3","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/xt256.css","hash":"bbe28ec69177699cb6300d777598adf6323f7861","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/styles/zenburn.css","hash":"933a3b196d01254dea5e6f48105ea15e210ae000","modified":1496198853000},{"_id":"themes/icarus/node_modules/html-entities/lib/html4-entities.js","hash":"a96528844c399772b52fbbac531b405990b16e5f","modified":1493034265000},{"_id":"themes/icarus/node_modules/html-entities/lib/html5-entities.js","hash":"e596021d6845ca29a00a19e65bc8134b7a642101","modified":1493034265000},{"_id":"themes/icarus/node_modules/html-entities/lib/xml-entities.js","hash":"4cbf15e7e85a5d7b7af287f48efc75bf66701e1f","modified":1493034265000},{"_id":"themes/icarus/node_modules/isexe/test/basic.js","hash":"5ecbf6a46a596324cb23fdca6df539be08a4f4ee","modified":1490230070000},{"_id":"themes/icarus/node_modules/keyword-extractor/.idea/.name","hash":"25819fa8187fbc80a4c54f67ae05ebffa226f150","modified":1404913349000},{"_id":"themes/icarus/node_modules/keyword-extractor/.idea/encodings.xml","hash":"84a3e2826b2b347096020089807b162036f3fd3e","modified":1404913350000},{"_id":"themes/icarus/node_modules/keyword-extractor/.idea/jsLibraryMappings.xml","hash":"3900d881a4380ad08dce82639cea60ed24538b0a","modified":1412218837000},{"_id":"themes/icarus/node_modules/keyword-extractor/.idea/keyword-extractor.iml","hash":"b2c1dea5cbc868c8067425a81cca2d36c555645e","modified":1412218837000},{"_id":"themes/icarus/node_modules/keyword-extractor/.idea/misc.xml","hash":"24b557cec899d489dc38dc5613811262f8643c9c","modified":1404913350000},{"_id":"themes/icarus/node_modules/keyword-extractor/.idea/modules.xml","hash":"62cbada10a437f44be92524e1e52b95c54e7bb00","modified":1404913350000},{"_id":"themes/icarus/node_modules/keyword-extractor/.idea/vcs.xml","hash":"1fc34617f0e3c1b12f38b5bfa9a106c9239a01c1","modified":1404913350000},{"_id":"themes/icarus/node_modules/keyword-extractor/.idea/workspace.xml","hash":"5d1d329f7aab2e0074d0c6681bbf2337a6936616","modified":1412256978000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/keyword_extractor.js","hash":"e1a8f6537d0e3c847fec69fee2ffc9e4f6dbcd6f","modified":1513274003000},{"_id":"themes/icarus/node_modules/moment/min/moment.min.js","hash":"90c1d234f480f543533f124a8079e5e0d03c3bcf","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/af.js","hash":"7a18805d5a28f0c96deeeac13d5ca56cef75d9ad","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ar-dz.js","hash":"8ac888865365d334ba6f58370dbd332ca73e917c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ar-kw.js","hash":"7823f8451f4a72e1c40a7ccac7a9d55c457e7a0d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ar-ly.js","hash":"aa2109379ddf1a7102d2471addbc85804e977a0d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ar-ma.js","hash":"f3427c1243ea5e6a521f3ccad371561f86c13969","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ar-sa.js","hash":"08ebf5e9b659e019225cfaaaa57bdbea0b4cee95","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ar-tn.js","hash":"0700be9e53e991e417a32cbfd87a4d5c9a11ea8a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ar.js","hash":"f0b17833f2fe905781392eefab89d70aed1b7c6a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/az.js","hash":"bb3c08de0ea3d63639d72fe68b87a11f6d8856f4","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/bg.js","hash":"8d589dbf96e69a1b87f5ba0518bf2a2e82acb85e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/be.js","hash":"aa09ea7fa09276d84ba8c0fbba6c2c06eeb70ed7","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/bm.js","hash":"4fff52b5c3a29bd75d870638d1927152b4e9cb84","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/bn.js","hash":"b523ce827fd3cd79f9075f856c40a5ae10f4c35e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/bo.js","hash":"da10cb6ee136f336c3a7b9da9db8a5fc3b5760ab","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/br.js","hash":"da6f5984ffaca3d170b2bf0c0cb3dd728137923b","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/bs.js","hash":"40c2a28c76c4942eca82624a5909ec4b189e8d84","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ca.js","hash":"78249c4dda04ed1291a99d2046493fd6e7f0992d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/cs.js","hash":"cbc658ad96a0c6906a8a04ce4d4d38cf230c990f","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/cv.js","hash":"2c8f2c7b6d382d61db25f8ccb638bf8e608213fa","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/cy.js","hash":"4f5d333c8136e19dffa8d23b9a34bd53c7b7a08d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/da.js","hash":"cea3a91324397c0145e556992a4b0d4778c2421d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/de-at.js","hash":"8e31c6484b7cb3ebfa14cded6501776a9869adee","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/de-ch.js","hash":"2344f650e8e3775a9e6ce5d0fa6e1d600213cd0c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/de.js","hash":"368796e0f5abd1f0c41fc449653e5e9f3dcb9b18","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/dv.js","hash":"ebfd922bceee666bc7afb923da39cdac8d3985e9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/el.js","hash":"6894c70fcb95d4a02938207217fb2944c6ef009c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/en-au.js","hash":"7debae899e29ff4dc7d3cef8f99f8426fb7b32a8","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/en-ca.js","hash":"bfaa124399276688015066a181245bcb2fdd9dc1","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/en-gb.js","hash":"843253c905a0662f95cee328f64d02928f89c872","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/en-ie.js","hash":"a135fdf9ada0bed7ed6cbf902bccdbe67d9ac8b6","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/en-il.js","hash":"bb394c157b5cf6c8a00bc3117393eeef3861afe0","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/en-nz.js","hash":"be0319bb43312db9ee52938f9130595e8dce3e8c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/eo.js","hash":"03d4f6db599d392d94692bedb406b20ff8b0d1ae","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/es-do.js","hash":"1ae4d9521bfb5e66c535490a21839b798dd34517","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/es-us.js","hash":"b5778ca51abc1693331936269d73db0dc6254b18","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/es.js","hash":"d9903b9619290b33e3fce9949abbc6024d51206c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/et.js","hash":"5b326c164970709e3a84ec536291350c46679339","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/eu.js","hash":"9221dad1398fe3a77b854127d999cc5fbb1e4dfd","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/fa.js","hash":"85925053b01b364d1b9c05a4baac695fb5a14bba","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/fi.js","hash":"f9aadb553300816ba78fc6d1a5fa48158044aab8","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/fo.js","hash":"ae37a6673d8cc4a7df0c412f65e083992ba4b67b","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/fr-ca.js","hash":"13b8079f3657a840de5b0209c63bcd0c7db0a016","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/fr-ch.js","hash":"1a39f70e35e6983be7f5f5291e4256bd0a150ae4","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/fr.js","hash":"10e814c08f6adb83f1ca2341bee3d0590c67b135","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/fy.js","hash":"eb2c0e76325b13bd15d3311df0c08e92f1aac714","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/gd.js","hash":"5c76382b5dc283bf5ea4da5ad7b497dc483bdec5","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/gl.js","hash":"1b601378071e3d5632a6696fd4750da05a29a6c7","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/gom-latn.js","hash":"c0ac0f322cafe7c5b81925d6b3355faf9f7e9e58","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/gu.js","hash":"086b6bd8027f235a5d40e102111827272b61fe6a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/he.js","hash":"01a59bb67ad6935cfd7e136bed196388d97e6eb9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/hi.js","hash":"af640c89b47363c9849c2e4cea66faa5f45b4802","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/hr.js","hash":"352046c0ccc045a659592f5ad067f94cbe5e9a29","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/hu.js","hash":"3deb3289981be3a74591ac918b48dafb5809f8ff","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/hy-am.js","hash":"0eab7393bd8f23ce456894966d9732e2aff81e01","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/id.js","hash":"452068deb8769bd11c77fa3845e18b3295868ace","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/is.js","hash":"5a3619d9ffafe03caec67ca3d667cfdbaa4adf84","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/it.js","hash":"d431aac0684e656da30777d01d40a8dd657e61c5","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ja.js","hash":"c1ac986b3d329807103142be9e26a95e30588f4d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/jv.js","hash":"d93e38cab1bfbe9719c2830cb424a1d6f40c4ee4","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ka.js","hash":"75b0ee99ea87b22fed5ad08cb68fd9ccbe8cda4e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/kk.js","hash":"a4c6f6d56befd95dfd940ef2d7c1b579da09c9b9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/km.js","hash":"0baa753920dc73d14d39165b111946f366f2230e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/kn.js","hash":"d654712d951246770111c742a79515ec1ecdcf5e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ko.js","hash":"a9b3a61e302311c270903b0d63073c2ed1e3a740","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ky.js","hash":"6a38f42f952f0a8cf00506c9991b078564fd15cd","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/lb.js","hash":"7798e1628806b494ae71a04de444ce68d3a69745","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/lo.js","hash":"0dce7234487abbd712ead1ee1a5e871ba1da7f8e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/lt.js","hash":"c85dd9dfe01a1185b6f2b9645a916bcdf8fc7be2","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/lv.js","hash":"8d4c910204b36859db85fda7e1cf55510eaa2194","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/me.js","hash":"8df169f8ecfbfad4f0bce96a64c0c7617014dffe","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/mi.js","hash":"77c8ef8934a4615dc097d6156b1c4b21fbf338e5","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/mk.js","hash":"a59a8bd0ecc0571ec31ef3fd3eeaab92c434ebe9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ml.js","hash":"05b7a37ea8a42f91bce38d4791c2aa002788e1bf","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/mn.js","hash":"d377591f821ce95e6534a7fe158c0027328cf554","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/mr.js","hash":"161e116cbf2c037ca4e82eaba68c79625e748cf3","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ms-my.js","hash":"b3530fdec2e61c268e2d1dd826a531a31b37afaa","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ms.js","hash":"9cf193b73f1da6be3dccde8f3b3555778244e768","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/mt.js","hash":"5bde6a700582947e2d57a4958c649fc36c69063b","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/my.js","hash":"7956f41ff19100429cffdd39b38ecd14306ddc96","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/nb.js","hash":"a39dcda5bf24b8dbae67e2ef9eb276fb4cb3bbef","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ne.js","hash":"783f7d2e169ef1f6c599e826e4d18513fb4ddcc3","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/nl-be.js","hash":"102116043c159da2e30f383dfbe148470955e3bd","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/nl.js","hash":"39283b42006028fb544ef023c38c62daddab5974","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/nn.js","hash":"a5ded1d6575bbe66aa963662d9914ed96f2db108","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/pa-in.js","hash":"ca7ba5d154e486c7266b5f6002d69c28dfc8cfcd","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/pl.js","hash":"b85d062b6162fe1f768d5fa992972c6b7460c5e9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/pt-br.js","hash":"561edb965660f310ab3a52d822d64fec47d718b3","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/pt.js","hash":"dbb2c1d6407e6a1ff1cc6eb4ddab9b6f13d2af70","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ro.js","hash":"77e6dea26be28f38eef03d7ced99d7d9c3183d76","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ru.js","hash":"7cda731ebaa5fbad595824eaa0f12dc1e9714a3a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/sd.js","hash":"749cad6431bc7de52010c9e1c46654bcc2e30fe2","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/se.js","hash":"2a634bf0db5ae146a5d4a53c162490c7c9a55d9e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/si.js","hash":"aabd22e0b942bda19c2f84704b38ee145bf95052","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/sk.js","hash":"68615053ccd9164668308a6dec025a7a02319c93","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/sl.js","hash":"5115ea13c6564cf3540d6366d96f7c2dd290426d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/sq.js","hash":"65d2c4d63078805ffedc93b1f09d8ff2f9d07404","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/sr-cyrl.js","hash":"103e68d85b4bc6d0ec88fae5a4b577825083ecd6","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/sr.js","hash":"bd470463543a62e1d4b271bb4afdf18c160a1b62","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ss.js","hash":"b379324b7b62ef551d9448ec34c364bc73f1e85f","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/sv.js","hash":"dddce3b9a2fd9b022c77dbcaba87fe67c8c14d9d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/sw.js","hash":"1bd41c973cd2d3d432efb7dddfa72c456c384ff0","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ta.js","hash":"6d974f43d94ecd61247d9d196b40f900786255b6","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/te.js","hash":"e406ed18830f010223c5838ace8d5e3d06713d5a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/tet.js","hash":"1ab969bf4ccbe42c5ea8d05454d6564fcce88fb2","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/tg.js","hash":"776ede17d0b768e4335a9b6f55f80b1b03529c0b","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/th.js","hash":"2e8d42b131f2a153baa82c4ef4c7f0bef55c4f8e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/tl-ph.js","hash":"6ce617783e065482d4d82daa66be0161fbe8581f","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/tlh.js","hash":"5a20219e730653fb8360e44121d5418cb3bc6c47","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/tr.js","hash":"388b3855ac40410f21e97ba3f3b8471b6b7d18b2","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/tzl.js","hash":"f80596320bf8ee8640101343541b9b6c29140d35","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/tzm-latn.js","hash":"4b3bbba2640c0ecbe8bd2e43f716652abb1ed3d8","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/tzm.js","hash":"d393dc0fcb0bb84a5e47ee65e45046eac1bbaf26","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ug-cn.js","hash":"783e668a3f70e2dcab706744a80104dc2ceda28a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/uk.js","hash":"30314dca3db8040ad10c0c93695f10bb0ab95925","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/ur.js","hash":"a7ec73258b44ad6b8f184709c9b93c192267290a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/uz-latn.js","hash":"8cd93b6aaeb387a91ae8aa9031f4a941ebdd6caa","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/uz.js","hash":"48db36364b538bc241d1bca81aee9e14e88bbce9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/vi.js","hash":"9b250e96cdf0541bba1791d5ea2dc66f4df48e18","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/x-pseudo.js","hash":"2d0450e70a7ce5a160bbded80fb0bb0a606af2a7","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/yo.js","hash":"c857eafd84d1113425932fd6080aa3784d7ef625","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/zh-cn.js","hash":"ada4c9d8ec07ed0b00b86ca3d2911088a25dcc1c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/zh-hk.js","hash":"9daac80dca5fde9af10e66cfd0bb4b70dda9db7a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/locale/zh-tw.js","hash":"fa0e1a70db01ac3b5e45784904652bf703014570","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/moment.js","hash":"6880f206d0b28ae7a9823f1eccbf7cd552fb716b","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/moment.js","hash":"e4a49e5e3fff449eb88bd5eaa00d9f39f8000b9a","modified":1523772394000},{"_id":"themes/icarus/node_modules/no-case/vendor/camel-case-regexp.js","hash":"2614bb1d67a0994aab917ce1fb9d55ccf33d0945","modified":1504891248000},{"_id":"themes/icarus/node_modules/no-case/vendor/camel-case-upper-regexp.js","hash":"5bd9812b844c336eb117c1571ac7ff36ba747077","modified":1504891743000},{"_id":"themes/icarus/node_modules/no-case/vendor/non-word-regexp.js","hash":"921f144a87b5cab2d484a2c256889fc2bed08268","modified":1504891248000},{"_id":"themes/icarus/node_modules/pseudomap/test/basic.js","hash":"dccd686c9c0f397d56318e99c89fa29fe4db84c2","modified":1448663157000},{"_id":"themes/icarus/node_modules/striptags/test/striptags-test.js","hash":"126a9fe92edf65aaa18646fb7d0851dac7c9de9a","modified":1484743089000},{"_id":"themes/icarus/node_modules/underscore.string/dist/underscore.string.min.js","hash":"d9d13d390cbc4422b16a519532491467d938f618","modified":1373889747000},{"_id":"themes/icarus/node_modules/underscore.string/lib/underscore.string.js","hash":"78cd42aa76fc0aedf1d4c87acf5d0dc65df09af1","modified":1373890128000},{"_id":"themes/icarus/node_modules/underscore.string/test/run-qunit.js","hash":"7a2f386db195670628c3d66d229327ebc02dab40","modified":1373531985000},{"_id":"themes/icarus/node_modules/underscore.string/test/speed.js","hash":"1679135dec8aa60e5e8ef8a99c689eec1af61389","modified":1373531985000},{"_id":"themes/icarus/node_modules/underscore.string/test/strings.js","hash":"13f980b52f29d393d3dbc5e70aa4f6a7f31d72bd","modified":1373889747000},{"_id":"themes/icarus/node_modules/underscore.string/test/strings_standalone.js","hash":"d68f872a3522bf6898981a4a95cfaf2f709c66e1","modified":1373532016000},{"_id":"themes/icarus/node_modules/underscore.string/test/test.html","hash":"59ba8455b0abcbcecea5af7dc9daece4621b88d1","modified":1373449276000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_standalone.html","hash":"86605f225d9d22a42e9df65360ee95dbd7ac98db","modified":1373449276000},{"_id":"themes/icarus/node_modules/underscore.string/test/underscore.js","hash":"934a4bcb13026508e16223cf9f00bb4a38c321f1","modified":1373531985000},{"_id":"themes/icarus/node_modules/which/bin/which","hash":"5b6b3e1838316fb3f1b3b4194cdf49db0674eb17","modified":1526003751967},{"_id":"themes/icarus/source/css/_partial/archive.styl","hash":"d35088c83ddd7a197d6d94e16a2ce3a7e29fa1dc","modified":1525046792000},{"_id":"themes/icarus/source/css/_partial/article.styl","hash":"6bde3701818299aff44bc108c41d0f16813fd3e1","modified":1533626537542},{"_id":"themes/icarus/source/css/_partial/comment.styl","hash":"784646796184d4f27918c22395288a2fafbf9554","modified":1525046792000},{"_id":"themes/icarus/source/css/_partial/donate.styl","hash":"a35e3ce37f5f7fac0973c752a5d84e3271dd3d1b","modified":1526723395775},{"_id":"themes/icarus/source/css/_partial/footer.styl","hash":"484776654e4c1691dc844e6e93786a08855c1c99","modified":1525046792000},{"_id":"themes/icarus/source/css/_partial/header.styl","hash":"29f3e74ebcbf67b91e33d60cecf83413a6fea38a","modified":1526659143454},{"_id":"themes/icarus/source/css/_partial/insight.styl","hash":"19833cd127f26ad90b06c115f8a96a30e0c0e53b","modified":1525046792000},{"_id":"themes/icarus/source/css/_partial/profile.styl","hash":"23198daccf3f35624c58e0d2e0c7d9611c511ce5","modified":1533622863917},{"_id":"themes/icarus/source/css/_partial/sidebar.styl","hash":"e68aa9ad6e438b0c1fa445496d78b7528d5e978f","modified":1533625214107},{"_id":"themes/icarus/source/css/_partial/timeline.styl","hash":"c813b98f4fc45b64d2e07e5d944745a654c8c943","modified":1525046792000},{"_id":"themes/icarus/source/css/_util/grid.styl","hash":"93fb6f1e2f40cd7d88ad0d56dd73d3f9a7bc853e","modified":1525046792000},{"_id":"themes/icarus/source/css/_util/mixin.styl","hash":"c8e1ddfc0fe9108bab592c7a73b73ce9344991fd","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/agate.styl","hash":"601eb70448a16b918df132f6fc41e891ae053653","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/androidstudio.styl","hash":"65d09f1b0e81c6a182f549fd3de51e59823c97ae","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/arduino-light.styl","hash":"15e8572585cd708221c513dea4bdd89d8fe56c10","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/arta.styl","hash":"1a5accc115f41d1b669ed708ac6a29abac876599","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/ascetic.styl","hash":"32cff3bef6fac3760fe78f203096477052a90552","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-cave-dark.styl","hash":"bc647b2c1d971d7cc947aa1ed66e9fd115261921","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-cave-light.styl","hash":"a5be0744a7ecf4a08f600ade4cfd555afc67bc15","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-dune-dark.styl","hash":"df50a85a4b14c7ca6e825d665594b91229d0e460","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-dune-light.styl","hash":"931435fbc6f974e8ce9e32722680035d248a9dc1","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-estuary-dark.styl","hash":"d84382bc8298f96730757391d3e761b7e640f406","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-estuary-light.styl","hash":"344276ca9b27e51d4c907f76afe5d13cf8e60bdf","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-forest-dark.styl","hash":"57c154c6045a038dc7df0a25927853e10bf48c4a","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-forest-light.styl","hash":"95228d9f2102fad425536aac44b80b2cba1f5950","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-heath-dark.styl","hash":"b0cf13b2233e7bc38342032d2d7296591a4c2bcf","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-heath-light.styl","hash":"8c8c2e445abef85273be966d59770e9ced6aac21","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-lakeside-dark.styl","hash":"bb0a8c4ad0dd8e3e7de7122ddf268fc42aa94acb","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-lakeside-light.styl","hash":"2c54cb9bdb259ae3b5b29f63ac2469ed34b08578","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-plateau-dark.styl","hash":"09c64f1a7052aec9070c36c0431df25216afaea1","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-plateau-light.styl","hash":"d1a05fdd1ededc9063d181ab25bad55a164aeb4a","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-savanna-dark.styl","hash":"a16c919a1ccf2f845488078fb341381bec46b1f3","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-savanna-light.styl","hash":"f8244c93711c7cb59dd79d2df966806b30d171ea","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-seaside-dark.styl","hash":"ce233a101daea7124cbfcd34add43ccfe2e1e1c7","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-seaside-light.styl","hash":"0597342da6e2d0c5bdcc7d42dabb07322b1a4177","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-sulphurpool-dark.styl","hash":"414b0cfc142f70afe359c16450b651e28bf7325a","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/atelier-sulphurpool-light.styl","hash":"efa52713efc468abeeb2b9299704371583b857de","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/brown-paper.styl","hash":"c2326ba20a5020a66ca7895258d18833327d4334","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/brown-papersq.png","hash":"3a1332ede3a75a3d24f60b6ed69035b72da5e182","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/codepen-embed.styl","hash":"f4dcc84d8e39f9831a5efe80e51923fc3054feb0","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/color-brewer.styl","hash":"2a439d6214430e2f45dd4939b4dfe1fe1a20aa0f","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/dark.styl","hash":"71ce56d311cc2f3a605f6e2c495ccd7236878404","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/darkula.styl","hash":"ad0d5728d21645039c9f199e7a56814170ed3bab","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/docco.styl","hash":"b1c176378bb275f2e8caa759f36294e42d614bf1","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/far.styl","hash":"d9928010ffe71e80b97a5afcba1a4975efdd7372","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/foundation.styl","hash":"bf8ddc94b4ad995b8b8805b5a4cf95004553fdac","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/github-gist.styl","hash":"48211a03d33e7f7ada0b261162bea06676155a71","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/github.styl","hash":"3336aeba324c6d34a6fd41fef9b47bc598f7064c","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/googlecode.styl","hash":"bda816beee7b439814b514e6869dc678822be1bc","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/grayscale.styl","hash":"bf37d8b8d1e602126c51526f0cc28807440228ed","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/highlightjs.styl","hash":"0e198b7a59191c7a39b641a4ddd22c948edb9358","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/hopscotch.styl","hash":"b374c6550b89b4751aedc8fbc3cf98d95bd70ead","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/hybrid.styl","hash":"ea8d7ddc258b073308746385f5cb85aabb8bfb83","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/idea.styl","hash":"a02967cb51c16a34e0ee895d33ded2b823d35b21","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/index.styl","hash":"82ec4e455db2c1db1a210ca9c53392803717fed6","modified":1526731366870},{"_id":"themes/icarus/source/css/_highlight/ir-black.styl","hash":"693078bbd72a2091ed30f506cc55949600b717af","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/kimbie.dark.styl","hash":"45dbb168f22d739d0109745d2decd66b5f94e786","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/kimbie.light.styl","hash":"61f8baed25be05288c8604d5070afbcd9f183f49","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/magula.styl","hash":"16d323f989b1420a0f72ef989242ece9bf17a456","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/mono-blue.styl","hash":"4c89a6ae29de67c0700585af82a60607e85df928","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/monokai-sublime.styl","hash":"25aa2fc1dbe38593e7c7ebe525438a39574d9935","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/monokai.styl","hash":"5a4fe9f957fd7a368c21b62a818403db4270452f","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/obsidian.styl","hash":"55572bbcfee1de6c31ac54681bb00336f5ae826d","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/paraiso-dark.styl","hash":"f1537bd868579fa018ecdbfd2eb922dcf3ba2cac","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/paraiso-light.styl","hash":"d224d1df0eb3395d9eea1344cee945c228af2911","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/pojoaque.jpg","hash":"c5fe6533b88b21f8d90d3d03954c6b29baa67791","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/pojoaque.styl","hash":"77dae9dc41945359d17fe84dbd317f1b40b2ee33","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/railscasts.styl","hash":"acd620f8bb7ff0e3fe5f9a22b4433ceef93a05e6","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/rainbow.styl","hash":"ce73b858fc0aba0e57ef9fb136c083082746bc1d","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/school-book.png","hash":"711ec983c874e093bb89eb77afcbdf6741fa61ee","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/school-book.styl","hash":"d43560fe519a931ce6da7d57416d7aa148441b83","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/solarized-dark.styl","hash":"702b9299a48c90124e3ac1d45f1591042f2beccc","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/solarized-light.styl","hash":"aa0dd3fd25c464183b59c5575c9bee8756b397f2","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/sunburst.styl","hash":"a0b5b5129547a23865d400cfa562ea0ac1ee3958","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/tomorrow-night-blue.styl","hash":"8b3087d4422be6eb800935a22eb11e035341c4ba","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/tomorrow-night-bright.styl","hash":"0ac6af6ecb446b5b60d6226748e4a6532db34f57","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/tomorrow-night-eighties.styl","hash":"fa57b3bb7857a160fc856dbe319b31e30cc5d771","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/tomorrow-night.styl","hash":"19b3080d4b066b40d50d7e7f297472482b5801fd","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/tomorrow.styl","hash":"15779cf6846725c7c35fc56cac39047d7e0aec1c","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/vs.styl","hash":"959a746f4b37aacb5d1d6ff1d57e0c045289d75d","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/xcode.styl","hash":"5e8532ae8366dcf6a4ef5e4813dc3d42ab3d0a50","modified":1525046792000},{"_id":"themes/icarus/source/css/_highlight/zenburn.styl","hash":"fc5ec840435dad80964d04519d3f882ddc03746a","modified":1525046792000},{"_id":"themes/icarus/source/css/images/.DS_Store","hash":"fca8ee80ada71e957d18a75661c673c18225a4b2","modified":1526720562457},{"_id":"themes/icarus/source/css/images/aALEHuN.png","hash":"73d4b1b130e6e96a8b700173600d0a050faedadf","modified":1526666415849},{"_id":"themes/icarus/source/css/images/avatar.png","hash":"55f53b1b934eacdb4e4989f824997b4d3fed4e6f","modified":1526660841137},{"_id":"themes/icarus/source/css/images/avatar2.png","hash":"74d4baf3c13f080a1776617f7e591c1aef818f72","modified":1526661248623},{"_id":"themes/icarus/source/css/images/bad-boy-logo.png","hash":"6c1514e084ab14febe7553033f20c6f5f35eafab","modified":1526664938444},{"_id":"themes/icarus/source/css/images/bad-boy.jpg","hash":"9bfd40e969aa33e02a1c9adc0c71f852d15e8cc4","modified":1526661048606},{"_id":"themes/icarus/source/css/images/badboy.png","hash":"55f53b1b934eacdb4e4989f824997b4d3fed4e6f","modified":1526660841137},{"_id":"themes/icarus/source/css/images/badboyCon.png","hash":"8a66b0e5bf18347612344116bd3ffdaaf1092ede","modified":1526663388492},{"_id":"themes/icarus/source/css/images/badboyCon的副本.png","hash":"d7f9910380561287907c6b8f6f29e6b8c7be1cae","modified":1526666889460},{"_id":"themes/icarus/source/css/images/donate.gif","hash":"b2a187c049caa7c6763fc19b760bbcb3d3ddae2e","modified":1526714638948},{"_id":"themes/icarus/source/css/images/eye.png","hash":"bfe9486ee51f9037434279cf2b0ff5ddc84d886f","modified":1526728484544},{"_id":"themes/icarus/source/css/images/images.png","hash":"60b724653eadade53b2a084f1022b30b1437feae","modified":1526720675330},{"_id":"themes/icarus/source/css/images/logo.png","hash":"74d4baf3c13f080a1776617f7e591c1aef818f72","modified":1526661248623},{"_id":"themes/icarus/source/css/images/logo1.png","hash":"e606a0584f98268b2fe92303f3254520862ef659","modified":1525046792000},{"_id":"themes/icarus/source/css/images/logo2.png","hash":"74d4baf3c13f080a1776617f7e591c1aef818f72","modified":1526661248623},{"_id":"themes/icarus/source/css/images/thumb-default-small.png","hash":"e8403b97ed9251f9f5207765b0ce796c5000b4ba","modified":1525046792000},{"_id":"themes/icarus/source/css/images/logo_badboy.png","hash":"46f72bbb576f2c42e2a9df3a3ffa7f273ab624ff","modified":1526660909358},{"_id":"themes/icarus/source/css/images/venum.gif","hash":"688e098301a1383d682ea9d8e25f3400614fdd34","modified":1526664534934},{"_id":"themes/icarus/source/libs/justified-gallery/jquery.justifiedGallery.min.js","hash":"b2683e7a872bc109b1756a65188a37cef7d0bd5c","modified":1525046792000},{"_id":"themes/icarus/source/libs/justified-gallery/justifiedGallery.min.css","hash":"13fbcba5e97aa88b748d94d3efc4718475279907","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/styles.css","hash":"5ca6e111046232bde112d33201a60532aee7d3c4","modified":1525046792000},{"_id":"themes/icarus/source/libs/source-code-pro/styles.css","hash":"93c308012738728f906cd4c5cfdb34189e0c712b","modified":1525046792000},{"_id":"themes/icarus/source/css/images/alipay.jpg","hash":"952afa18a7049e00fb12078b54b014a3b6c24359","modified":1526716055288},{"_id":"themes/icarus/source/css/images/avatar2323.png","hash":"030b8a6615a5214ad0f86281ece7c9092cbf93e9","modified":1526264802642},{"_id":"themes/icarus/source/css/images/wechatpay.jpg","hash":"0c20ca110c6f9b2ad70c7c5d390df306283cd42c","modified":1526719563377},{"_id":"themes/icarus/node_modules/bluebird/js/browser/bluebird.core.min.js","hash":"ccb5ca34bfea5d0dcbcaecf9c4c09b79805f9647","modified":1507132264000},{"_id":"themes/icarus/node_modules/bluebird/js/release/any.js","hash":"424dfe2a1afeaad729ca2be5ccfd443311716c41","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/assert.js","hash":"da909f13035601fbcdc3a28937dd44e3008327ee","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/async.js","hash":"18b0242372dcdf745ae68b6c0f193b6fb2f40b96","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/bind.js","hash":"6ba64f83e985a1cb7dcf945490e4c280460ccb25","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/bluebird.js","hash":"f68f4e491f49be5a5a3c5d04fafcd8abaf02fc2a","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/call_get.js","hash":"b53dcc39da361dbc72a49ced90d5290d94c70b80","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/cancel.js","hash":"1a6c901863d671048f58095fe9b568b2a8407729","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/catch_filter.js","hash":"cbdedec0db19fe2af76d5594ed8bf819000d7c79","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/context.js","hash":"ea7eb1ab2c8231fa6bfa8446cf730736735396e0","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/debuggability.js","hash":"039b50edc8f5101cbc1002199805b0bc8e07ef67","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/direct_resolve.js","hash":"470ca4d9a7e387ceceb383bc2640202f5fa6bda2","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/each.js","hash":"44949d491571dd38f6ffdd777cb44f96aebb5fc8","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/errors.js","hash":"98deaaee17f36851937108257e028a7e237b3f5e","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/es5.js","hash":"019c4e8b62031ea49aedc86dedd20318c6122698","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/filter.js","hash":"a5f3aee4afbc67d372e5b4fbaeac047d0d9c779b","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/finally.js","hash":"cc388e93ecc2e12145c080cc67b0d8517e481c13","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/generators.js","hash":"6b334d17275b48548e44f74f477b2e8bd49da304","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/join.js","hash":"5ab50e4fd76d09506700b47e7672c9d528816d4b","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/map.js","hash":"4d96c67c999d7f8a155934d25bfe8cc4912ad469","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/method.js","hash":"02a5a493c2cf2045fbb20b2751381e4b4e29dbe8","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/nodeback.js","hash":"46fc87ede6ca68434439a5216fe8e6f89d8d8a1f","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/nodeify.js","hash":"ebd75c010fcf1c4f709f4b444e62b80cfcde2a0f","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/promise.js","hash":"6038f62bb6a4d8d676dfbed9d2b8b36a0ea6b223","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/promise_array.js","hash":"2b05f82b39c408715260b264dbeb886d080efc59","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/promisify.js","hash":"3831be99120a57a3c07406f7012467f02346e92c","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/props.js","hash":"d26e05c3860fd45b7ae9008995493c6d01c2f2e7","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/queue.js","hash":"e6fddd439658a46c57132ec8d07dd04bf1d743e6","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/race.js","hash":"5f5b5403be19793dec5658af27fe996173eb4990","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/reduce.js","hash":"8f0d6b75ed5682d3a7e45254d76618258b45bb2a","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/schedule.js","hash":"bad77d032baf76d04dd40cc5f01c5d4adebd4578","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/settle.js","hash":"36f97c3732ae907af622fd4c859d29da1255fdfa","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/some.js","hash":"b548ddd7eb6b35ae6e97d931e0a8ef6dd512b3b5","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/synchronous_inspection.js","hash":"21be4c93fd5b93b07315c5edc930800e686e4dea","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/thenables.js","hash":"b06e617f4b1e5c9c33c9c4e0baabe709550154b1","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/timers.js","hash":"ad830b808b63bd4b291a3b23210bdaf850f1f818","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/using.js","hash":"5b287a7d967647d38091aec63f880abc46caeb4f","modified":1507132265000},{"_id":"themes/icarus/node_modules/bluebird/js/release/util.js","hash":"fd5ded260b95d0aa8ec6c2c376167a482f26d8f2","modified":1507132265000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/1c.js","hash":"de01bab25b86d1e12cd41b2c151d496d09b07ba3","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/abnf.js","hash":"82addec1b6c07619aca7aada359aed3f34c02642","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/accesslog.js","hash":"348d234c7253b8b308b74376fd92824d7788e745","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/actionscript.js","hash":"0ea1dbcc03123f7d61ac0d69cfe55bfbc517d2a9","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/ada.js","hash":"f03675f08c9750f48c33ea202894cd3b13d8600e","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/apache.js","hash":"c60412ca3c2543b47727c8915286750873a24c09","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/applescript.js","hash":"64c56abbca58a0881b736a5b7519a178a0a1d782","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/arduino.js","hash":"873698f166f498133260f60b37b8c2fea4bbc55b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/armasm.js","hash":"d49048f28a92ae394e0c2fdddad8e46a53732513","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/asciidoc.js","hash":"b7e01e2d240ff73a8695ed1447a47c4a0be7b62d","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/aspectj.js","hash":"53e02075ee7d4923f95bbd47e3697f6b6bf58783","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/autohotkey.js","hash":"a7a96ef42b4f4ceb462bbc1e31562898060fb9f0","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/autoit.js","hash":"5a765d8c3d147df67badc2e4135b8e5df56337ce","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/avrasm.js","hash":"474db1f6917d9ee3b1679286ec1583f1bbd19c2c","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/awk.js","hash":"b749eda162b5bfcb684da29e1c2e71712afd2f57","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/axapta.js","hash":"0f0f21ccab8b6874e965d53791465ed89e3aa13b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/bash.js","hash":"8a9d29a349a9f177a72a4fddcbcef294e8e2f736","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/basic.js","hash":"78824065536502064ecceaf75d9571d3380d9642","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/bnf.js","hash":"ef775edcfecec203e6c67fa994ceb6d7d81d90eb","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/brainfuck.js","hash":"b6d4f266923cc946678a9060be5ad4a02a358db8","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/cal.js","hash":"7d7b9e510ad2ead900b097c38074f755378647e1","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/capnproto.js","hash":"0d947410d260600d3b110c4c427545b2a97f7947","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/ceylon.js","hash":"586cc177a3c7dee9836e9f474bc3947e93424ab4","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/clean.js","hash":"2f08f6fd4734e0a12c39362b34b6312865f5ea80","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/clojure-repl.js","hash":"1e9b837580d563935b9e4a2314e1c618c42cf052","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/clojure.js","hash":"15ea1584ce3ea6837de6dcf9e04b3c9388b62466","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/cmake.js","hash":"72aef704f3645ffc6cbfb8d534fa31c0181e1455","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/coffeescript.js","hash":"149af704b2951ac39ff2be20d99466cbe5fc1629","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/coq.js","hash":"f0d48985501355c966a12d54ce236c2c92982b03","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/cos.js","hash":"112dc8b9329863653b8e9d648b8eec0f7f0cb45d","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/cpp.js","hash":"fbc740e6517cbeba0d7608bc595f0c04ef5465ca","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/crmsh.js","hash":"c8ad2da6b6603b62172e2865eea4add11fd79800","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/crystal.js","hash":"137b69f99262fdac465331cefc12bb5355332a52","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/cs.js","hash":"a8104e1d7f7096087de7138a14c77042966a98a6","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/csp.js","hash":"8a4c03add8f3c7815fbbe8dbc28748e6fd19cb1a","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/css.js","hash":"e3e285a4fde1eeed5120211054b1e5f0e4cbf92b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/d.js","hash":"229d02046adf47a87e6229917998b564a0726ea5","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/dart.js","hash":"06e74990e6dabb0d55b90bfa1e32786b26ed49b6","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/delphi.js","hash":"4771dd9abe3394931dc8e9cf717612617f94e22b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/diff.js","hash":"ce359050293199c8667739f88ecb435a4d08111b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/django.js","hash":"f2fdd57b3a4aad786b3cb81aaa1151abbac9de2f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/dns.js","hash":"de0553c09bac9edacdb1c492dd05960cc7cf971f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/dockerfile.js","hash":"6a16033eb47225bc4bdc3957c3e685874263b8dc","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/dos.js","hash":"bed63b430d0245c8b6c3e430d849c64a8e40db4a","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/dsconfig.js","hash":"e1b080939fb6e23065003841f1b81e84cbd9f411","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/dts.js","hash":"23612dd62fc09eb91bff8e7eafd684d469ef3392","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/dust.js","hash":"2996fd8585906dd1ae8f1dc9e8b8abd7ccd568fc","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/ebnf.js","hash":"a2668958d3f99499f250158dd0adc549793c0ec3","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/elixir.js","hash":"4d352797253b7fd81a0836ad9d590e515f79d0c6","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/elm.js","hash":"f6a06178460a203c8c49f889be4b51f40832914e","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/erb.js","hash":"3e67ebd4a54e38f7c4352facbb0114b914df773d","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/erlang-repl.js","hash":"b40072b8f60479a3b2fda37c6ffa5c6065e160bb","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/erlang.js","hash":"eab318990e71134e5ffb248663d141dc418a43f7","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/excel.js","hash":"741ee6d1f9ddbf7614a75d90f30c4afd6278b9e9","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/fix.js","hash":"225c32c97aad884ad5c3d256b13aac89ab7b3569","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/flix.js","hash":"dcccd8fe672c40b1db80c26ad3389d621f3304be","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/fortran.js","hash":"a9d0aa92cb7a7ee12ae3b89233ef4e3f68a1b5ea","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/fsharp.js","hash":"51446ecccedbe24c8477af70d4fbf15d5f090ce5","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/gams.js","hash":"5dc186a0cfd8115c28d82f020d0618566f14e69b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/gauss.js","hash":"c40d2988e593edd4d434336a372ab8a04a975129","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/gcode.js","hash":"1c35a95ccd30d0949d1f09f586989e036c97187b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/gherkin.js","hash":"68eddb4dc9b866bad4fc3b555be92199716489d8","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/glsl.js","hash":"c348157dec60d1f1d723b0f88cfc0a9bcc0d74ca","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/go.js","hash":"b4b17b3d63424736f00c4b58afdc313e9094dd7b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/golo.js","hash":"8d53d68e90a908e33670d075a3727bc3bac48f12","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/gradle.js","hash":"3c7ba57616e77ac59b59810515bb6d2bebd6b6f2","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/groovy.js","hash":"cb9e4ed4363a68859e7fd8365b7024835691d669","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/haml.js","hash":"84491f05df1fc74d1084fb253dc12d2bed0a2fca","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/handlebars.js","hash":"3586163cb9c029239185209044f91567bc9388a6","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/haskell.js","hash":"65d0e2f2dd60d21107438f98bf06b5673a0e395c","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/haxe.js","hash":"a2804355ea896b573c3c4c4d4bbc611f65601dde","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/hsp.js","hash":"ace444817c7832f9947752ae72f51329502c8d53","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/htmlbars.js","hash":"2b42fc812e7bb39a5e69e2c5a0365a6952689e18","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/http.js","hash":"202176d2e4fc551d11162a797c1f2ec002412b28","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/hy.js","hash":"6456d54268279752bde50e4ced70f8d5a3fa5c30","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/inform7.js","hash":"f7019518299290c408b65cae8a5d9a2170f260b8","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/ini.js","hash":"ff3f6c019db17271a60c0110d1c0d8bb4b20ecfb","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/irpf90.js","hash":"25b18b62a7eac03d7cd196c12480437ddd957196","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/java.js","hash":"90fa24c718396fe72c34f56c34649d104e31cb2a","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/javascript.js","hash":"6aca7e933f0e8d404f1e62a88756328b4e9b0b15","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/jboss-cli.js","hash":"11af05d6fe833154c0fc13a8817c325ecafd0c8b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/json.js","hash":"75b75bff84a88d6b0b630442d4b588719e3ccf22","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/julia-repl.js","hash":"09b83b2d540e2106024dc7fc67c3b65513318f14","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/julia.js","hash":"f35a2ccbac0bdfcf3039b5ba04f231f8526237d8","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/kotlin.js","hash":"c8d946ed41119a9801702dc36da57ec52a060e50","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/lasso.js","hash":"e519685b2d7e0f11dd850041085b97fcacd8fd3e","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/ldif.js","hash":"2f6f2bf8d3ff343e8f3d661fb7b97333a84615a0","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/leaf.js","hash":"7a74ea9bf9a65ea1b3b6632a5c7b4725cea86c05","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/less.js","hash":"6c470c8d7a92f1b436536e469b362b92f1b13c67","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/lisp.js","hash":"b6d09c4efc9f62bd400d3b4e3a830e86433b3650","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/livecodeserver.js","hash":"7966dc033093c1555cbf626b58b9cbbd5f889614","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/livescript.js","hash":"b3a1310bf1b1c812fc28467c136bbd77610c50e8","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/llvm.js","hash":"14449c574014420aaa9131e36c68550b1b46d996","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/lsl.js","hash":"460ee52b1bc77438274d2a4b7dde478e52798c2f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/lua.js","hash":"0bbe0ee424e83c548d6ce21a12f8885b23419030","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/makefile.js","hash":"dd503cb449593fa6bfed24c922023213e62a367d","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/markdown.js","hash":"226a75a5d3bae188408a8d388ddaa8d702a38194","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/matlab.js","hash":"08d33bc43836283c2181a1077c117c77fc8cc8c9","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/maxima.js","hash":"3522fa58e0e93d36c3267dea5172e33fe64cc843","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/mel.js","hash":"6f19c9f0b7e78725e6391a1e2d3990610cf8c006","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/mercury.js","hash":"154f4ac390a395eae2bb4246c93c18b7412b3bd5","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/mipsasm.js","hash":"adfac05b0dbabb7a5053abdeb7f25b0a2b8df178","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/mizar.js","hash":"009acb028481b4f3ad95bec4d0386bb2184e7acf","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/mojolicious.js","hash":"7edb53b8770a591523bbe90601a1097d589a5dc0","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/monkey.js","hash":"64fc8e856e773939f5f50fb03b3e207e77ae5ea6","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/moonscript.js","hash":"485510191b2c8d67c96710cb85dc1a37ebb05af8","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/n1ql.js","hash":"c6ab9f27fd4a711387a0d15d75926aa2087b54a9","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/nginx.js","hash":"2d35617087edad25bec36f549a82ae574e6c9a45","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/nimrod.js","hash":"e892415387bd17ed7f62205b11704e5f0751943d","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/nix.js","hash":"0e799905fad5eb9a7e86b07654d540eb67d74529","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/nsis.js","hash":"859fa05107b664b9ee9180070a3344b7ee5ac18f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/objectivec.js","hash":"b7b8e950fc63070c2187bba4e00439b492f1af5a","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/ocaml.js","hash":"03e65e7f07732e214a0d32a0b539e76012ed3970","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/openscad.js","hash":"a732c333aa1adaeea2f125e086012305382b674e","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/oxygene.js","hash":"a8e62344bc5a3d7db691bcf28d53fc305f790cef","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/parser3.js","hash":"ac15e9d440ec885a71761109fe40ba8a06c7b933","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/perl.js","hash":"c1872392209d696ab1ec8a34d5a9a2e8ba51ec35","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/pf.js","hash":"a7b05b0bce721379a7c5245ab2e87f887d64c100","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/php.js","hash":"a4f9f78b7786797c7f8488220525209393d76c11","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/pony.js","hash":"ebf4ae15519e16eb80c3ab984c648155109ad52b","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/powershell.js","hash":"c2459c6b38115c02b5edb267042da49c998cf08e","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/processing.js","hash":"d13b510492292fb16f8d8a8ca7787b5fdf83f9d3","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/profile.js","hash":"2199270f25b8e1c6e934f44111d3654d7d1b3c00","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/prolog.js","hash":"16087064562b8ee336c7373f2f3f377904a7809a","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/protobuf.js","hash":"abbcec351964992b81f75caade03b077850c3949","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/puppet.js","hash":"2b7644a72813e256632310ec9edef0459727f096","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/purebasic.js","hash":"684640f179954b84907545c0039460e35da287c6","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/python.js","hash":"624da737f0410e476b81d65e02b95693b8e88860","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/q.js","hash":"7228dbc817b650907d60ee957a8303d53f727f80","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/qml.js","hash":"945828adcbf8c801ad38ed551e7213bc97958c1d","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/r.js","hash":"d4e0224fa019df6fbfdf32173aa6667748e9ea11","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/rib.js","hash":"6ac4095f443d77730802603b2b9161a9bcf72100","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/roboconf.js","hash":"f39efee84c9b8c3308746c82176ff6a305da7eea","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/routeros.js","hash":"0aeadff1b7b75dc83356842ac17240695d34d987","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/rsl.js","hash":"ab3dfe5e601148d016c76aab50fb17afb794c435","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/ruby.js","hash":"0f92409522686f18e7382800f9f3aba287869ec9","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/ruleslanguage.js","hash":"5600035cd1c59e034c8b710276ec4029e1594da8","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/rust.js","hash":"efeab19cc69a21478e6c2acc21de4c01a5940275","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/scala.js","hash":"fc8d7c529fd4be604eea90cd0a12358fad5f49b9","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/scheme.js","hash":"7511d70a0bd2aba70315c32a5656ce30571615d5","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/scilab.js","hash":"1ff368f39960bba8051de0706aa97d46ffdeb94d","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/scss.js","hash":"3e770e661b2c57b9ee3ead36ef1f358f1a87d6e2","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/shell.js","hash":"1bf20ca21d54e843cea24a675a8842fa44ce419f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/smali.js","hash":"9a8ccd28ca3405e5ba86f86d2fcbe82095d81391","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/smalltalk.js","hash":"45d649415c43d5989bc9bb2548885f0820bfa119","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/sml.js","hash":"03e1e7cf2041103f55d57dda1624ff0fba83d47a","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/sqf.js","hash":"3a740091b22b725518953d6710865b48cdb0c8ca","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/sql.js","hash":"4e039f7ea433c538390fd8b4afdc6240af5a3341","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/stan.js","hash":"305e8b20c4ac5762dfb13892f87b1127e65ad2cd","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/stata.js","hash":"52b5b3f4bf075e3d31df1b6c432cd48f5af2071a","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/step21.js","hash":"f9c30d58ef59c96f151a74736dfe48260fa90ebe","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/stylus.js","hash":"a6260a532b4ee8ffcbcace1bbbf502a587f56628","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/subunit.js","hash":"58b09d36e6de77766fee9976fa20676fd97914a2","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/swift.js","hash":"7bdb6ef3b8bedeb2064366105577cf3b74637e85","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/taggerscript.js","hash":"7fc8221bb34ff42d867a68a98f93a33e11cac11f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/tap.js","hash":"986dce0e93f335db4c120876daf523904ba84922","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/tcl.js","hash":"fae3052ea5a3ecb70f4a12d38f2d7fb387cefc94","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/tex.js","hash":"956a6f3ac9e1e2bc40a49191a4f7558f267008eb","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/thrift.js","hash":"47c807c1cc33a57f6f2ecb9e609f56ae74d5ef77","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/tp.js","hash":"7690f71ab4b1616b96873b43116e8ae936b1590c","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/twig.js","hash":"30b6777246eec12be33ba24fc5dfc95565491edc","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/typescript.js","hash":"ba777d301245055bc7ceffa2ec4a0e3dd5d99433","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/vala.js","hash":"8cf4daa0d64e112a0eede09353cfc4c95be37619","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/vbnet.js","hash":"b95eecf676909dab14be6fbbe03d4a4ffd2cdd8f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/vbscript-html.js","hash":"af6c349a4ce7cabf9e94168aa75dfa350664a021","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/vbscript.js","hash":"29df599716e626e26b384e259f052c9a68fd29f0","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/verilog.js","hash":"2385a8df75d73b3a1aebc591432a53d2e58c41c3","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/vhdl.js","hash":"3baf1eb9aef43fc2aa1c64a258b639bc4e69219d","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/vim.js","hash":"a726fda2007d43cd0734e96e1530056f32cd0033","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/x86asm.js","hash":"5eef051aad64bac3c549311cea1f76521f890580","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/xl.js","hash":"51b7b4ec88350dbaf8bd115e4c9f4a3c3cee32d4","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/xml.js","hash":"9d6ec52a3d002850caff607948da16e7888979d3","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/xquery.js","hash":"ee7fee4c24dd6112e16b513dd26ca5937d9d637f","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/yaml.js","hash":"bf63ff94dee5f4680436ecf12daf15d2e6675f25","modified":1496198853000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/zephir.js","hash":"fc7c87476ff8fe68e51d159eddb99d8421505a3e","modified":1496198853000},{"_id":"themes/icarus/node_modules/keyword-extractor/.idea/libraries/keyword_extractor_node_modules.xml","hash":"dac58a34689afec66c1ad736386893732f2a09fb","modified":1412218837000},{"_id":"themes/icarus/node_modules/keyword-extractor/.idea/scopes/scope_settings.xml","hash":"fa07d9d70692f2d31ace346398287a038e2bdc2e","modified":1404913350000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/da.js","hash":"bb5a01d9007faba1d212578ede92ea817015a088","modified":1466442650000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/de.js","hash":"f12b4f778b25fc05a5bfa5fc37980db1ea2e9635","modified":1425996583000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/en.js","hash":"79aaa0c732cb118a377ffa3bbd775443c23c817d","modified":1404913186000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/es.js","hash":"d88322794e4a9553f26a4a52c4a2116b7e0d38d9","modified":1513274003000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/fr.js","hash":"fdcbeed607cea6ca86bed93aeb580d293aaba57f","modified":1425996583000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/gl.js","hash":"430c60b0516871709dda368762e4e073266d9491","modified":1466442650000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/it.js","hash":"0d8a905855b6865f80467b3510453db2fc2665c3","modified":1425996583000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/nl.js","hash":"08691dc133e26b535f2d245bada2e61caf329951","modified":1425996583000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/pl.js","hash":"ae20014ce3213fda328e22e98aa081ff8b12e4d0","modified":1404913186000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/pt.js","hash":"c3b79bbe45724cb835ae66c2a9cf7f42341c6192","modified":1466442650000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/ru.js","hash":"299c5b172d51738e89ae277fa1d0a353038f3505","modified":1425996583000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/se.js","hash":"d89283e481307a322f891beab6892b6fe2245f86","modified":1466442650000},{"_id":"themes/icarus/node_modules/keyword-extractor/lib/stopwords/stopwords.js","hash":"0e90de1bec285130395364512019e671881250fe","modified":1466442650000},{"_id":"themes/icarus/node_modules/moment/src/locale/af.js","hash":"ac5ff680c99be60771cb95b94033ee65d93145a5","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ar-dz.js","hash":"6febdcf069fbe70c3b8cf5352c11a46e4ce22876","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ar-kw.js","hash":"581d8747c5e7a105a084efd2bc74c6a1bf99648a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ar-ly.js","hash":"c01d6ddf9b131efaec124e0c14c2a4d03d27d3ca","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ar-ma.js","hash":"e4a39ae854d8bd7d706d24429817dda22b221212","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ar-sa.js","hash":"4a4622969d2e070ddcab1c005d5effa165ada8b1","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ar-tn.js","hash":"c79b5b540b68d2fb3a23a312008235538af6f0ca","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ar.js","hash":"5324b5c85ae3fa662fd95e08449d3d7014e4fea8","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/az.js","hash":"8a43efd61055a86cf0531d982cfd81348988e1c0","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/be.js","hash":"10190bfec58d89083c512e1ebbde5f80cfc4ac17","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/bg.js","hash":"012ca3b100deef5030a3ea6ed9af2f792a2aceec","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/bm.js","hash":"2a97184fb97b2605e04af399cca5532912b7983b","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/bn.js","hash":"58f985337f1dd831a758afa38babca12d557e8b2","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/bo.js","hash":"1783eda907ec6620059630e02b80aa17746c50c2","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/br.js","hash":"79c14f5ea08d73b68aa84da46b5ea7ab1f808cce","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/bs.js","hash":"8128d66afc8ee846d8d6fda8ab5f8ad699806f64","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ca.js","hash":"2b36199c3eb0ed76a10d8bee31bcaafefb0e13c9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/cs.js","hash":"c19c3a0f77ea6a6a24b1629f4dec82c73b483430","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/cv.js","hash":"7a05863876627106417c9db7c8fc5f2ce7cf14c2","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/cy.js","hash":"028591f29174b4aa5b504556c9c5cc6255f54f1a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/da.js","hash":"7ba35551af756710edf1e6cca7a27e0f2f53de1d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/de-at.js","hash":"2546d5fb6979ea74f1db733e143655aaa61f07d4","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/de-ch.js","hash":"57a5b76dc472aad4d5090ec78957ed6adeb90156","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/de.js","hash":"e2acae7f77b642d7639d3b8d779cee7544080c99","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/dv.js","hash":"c6d9c1d065141839a14e686b9aecb0c5f6783561","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/el.js","hash":"749e51cad21907e86816ece21e839b365545aaf5","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/en-au.js","hash":"41a5b563e72149ace8388f67395eab8876807c71","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/en-ca.js","hash":"cc9a8726625e3dc1906e03b2501eb66a99317698","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/en-gb.js","hash":"e86f168e4b3b9416c12a0468d767216f60826811","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/en-ie.js","hash":"618e88287d6aeb0d39bfd63de555f87c820d070e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/en-il.js","hash":"0c83db55b1417b769f1064de65143fa0a4326c2f","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/en-nz.js","hash":"4b692f73e4206e7750b6e038c74b03d828eb8485","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/eo.js","hash":"ba607e3997534ffc01ed0c7cbf23b1a60b32d263","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/es-do.js","hash":"71c0ed8abf285c7d8ad9763f73f0b7d0132be90b","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/es-us.js","hash":"4b11e9c9be9d87980de6b8f42ec15390eedc898d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/es.js","hash":"4f3acef6ee8a9293fedc7b517909e5a5c33a0b53","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/et.js","hash":"e02eee0c41fe9f4877fcd723dd81553e2ad03ceb","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/eu.js","hash":"28841f30b7256ff5241ce0afe49c36162a0995c8","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/fa.js","hash":"0344681ec8729d2a0c9c3b7b87d97ab9cff59ef4","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/fi.js","hash":"44a76394ecc288e0c4dc3eed8528eb2ba572c7b6","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/fo.js","hash":"19ee212ae3a6f64aa30c5e30724f0f9066e3908f","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/fr-ca.js","hash":"3797324e2f787f09eed489f676afcafe064e0cfe","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/fr-ch.js","hash":"d6c40a8e0529a355c0ffa5dce52d224ba589a8d5","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/fr.js","hash":"5a07a5ace7f9dced84a8bf5d4dae64a5f0fe869c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/fy.js","hash":"e2d3702e84e239e5fb1a358e826c0f8964666af5","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/gd.js","hash":"b1899743984b9ec8dbab571d8a2fd94f707bb2de","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/gl.js","hash":"7a2d161546a93c2d477e6fea616d4075b165c589","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/gom-latn.js","hash":"924fd8eee431cf4ad919c04f9102dd0116e2cd27","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/gu.js","hash":"9ef8f5cf003ec4728505e7d791834931163fb44b","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/he.js","hash":"8619223ced826ca18b62c39312389c00bbd890cf","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/hi.js","hash":"e1a113ef65a86f3118064c6f281a79237beb26ae","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/hr.js","hash":"2d4658ac8ba0856a8e1288dbdbe7cc36b04cc8d7","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/hu.js","hash":"855d874411a1198097d554820ac20b8b7a9ecc63","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/hy-am.js","hash":"daf8fda48775b2c0a33b3e96be2a60134bbe4586","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/id.js","hash":"03c33b9561bc3aee1ac44a544ace72e830f46049","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/is.js","hash":"d3497bb900502e0af5c7403f38b50cbe266b3d0a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/it.js","hash":"afdd36e64115ccc6bd1b7747cde6d3049383cef8","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ja.js","hash":"39ebcd9f6a3d8c67ba530891549cea4d67e35874","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/jv.js","hash":"cb096bf3925bd14d0aa7b0545ca6d67bd49a2d1d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ka.js","hash":"5409878fe4b86a59123f912bdaa90e0ce1a179a6","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/kk.js","hash":"109c2acd753b2932a92e95d0742cb941035def5d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/km.js","hash":"5bd7d672c124320e55fd16b49cf5fe1126f95069","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/kn.js","hash":"975b18c48b3d6a27b2d8ab93c40609d9ebc1bb4d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ko.js","hash":"7a22aeb97f8c370bd6fd69b6f389f67c553bd5a2","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ky.js","hash":"b066a8b4e412c357b9556b2bf0544564b9eb42fb","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/lb.js","hash":"3e0ef81e8937b8bd6ec7696958f332a661b5ac5e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/lo.js","hash":"4f77e604bb626d7ff59a2d986ed021f133ccdd8b","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/lt.js","hash":"fb4061522b9262cfa7c7d26d202dd9d45d62bc90","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/lv.js","hash":"c9ae70a190565ee74304599c58d6f11294ed7718","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/me.js","hash":"6326ccd2c1ea3b9498871ffb1a93d7deb4a5f6c1","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/mi.js","hash":"1977aae187d58c9dc9e4117da813a1e45018dee1","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/mk.js","hash":"de0546b3ff14cbc3ed6640cec3e59d13db81a899","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ml.js","hash":"4b15fa7c6526b55e967d41a60b1420fa1a4b0473","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/mn.js","hash":"c423d0ff60bdcdfe4ce628624e8d8b843db106ae","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/mr.js","hash":"7a7cf990ea4a2a048fb918d58eb3314e0e508f42","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ms-my.js","hash":"50cfe2bbbe50c9efaed3fd640111960d26764f20","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ms.js","hash":"fa793930b9f46dadd3516bb6e2cb65b5b4dd0173","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/mt.js","hash":"9d86aec00fe6a64534c02a4ff500219f93f7c3f7","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/my.js","hash":"a8f7e7eb34291103462a112b3a683dfc35ddb564","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/nb.js","hash":"3adc2af0a2205f598b246011ecaf7d6691fc2a44","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ne.js","hash":"8d444edfbad24a60a15f63455a2f3bba6227fcf2","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/nl-be.js","hash":"40a0b01063373a1e93e19fc1b993d99eab31a0dc","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/nl.js","hash":"d6965f24cdebf568ad5d07c52558a43f3ebd3e32","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/nn.js","hash":"5e07c4e16121cebf7268f0689c7f1cd70a2b1265","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/pa-in.js","hash":"2b4ce960c1ed48462507807b952917d2a60c174f","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/pl.js","hash":"92f4d242832549aa85103cc8319e7408f5e2e12d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/pt-br.js","hash":"50a90cf4226ba407b5859a4a1615982709d94215","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/pt.js","hash":"ed788b32628a2c90a644a112cfb5817fa6b010cb","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ro.js","hash":"c1942483f1d2265b9b7f1077aba982e7b42df331","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ru.js","hash":"00f48e560d9b58d68751fbd0e86d397236c2771c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/sd.js","hash":"d4ce3733857631ad948883226e31c619016ef6be","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/se.js","hash":"98bee45dc130c64323f8069a777cd5b0b51c9867","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/si.js","hash":"a4b47e77673d3a03509d177ab828ea2c6bb1263f","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/sk.js","hash":"8b777a0856f06bca4abe139c182e8fe8d1a10ab4","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/sl.js","hash":"cf48cc40c468d252ff4c8178dcbe976466ef2d84","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/sq.js","hash":"898e17e2aa6fb75463f0020c80643718c5d4d607","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/sr-cyrl.js","hash":"50f8ec26fd5832198700686be025dfaf95b46341","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/sr.js","hash":"727e77cd7954e3d3ddbfa6369c29d94d7adc0951","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ss.js","hash":"54bc48f4e4b7e506ad9e8239cc154be56680f75d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/sv.js","hash":"a1aa110bdba48f5798cb280519593cc35f9df378","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/sw.js","hash":"94fd74642c29abd6d357109fe6bec5261cfbc670","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ta.js","hash":"b1157adcd15b0a50f607863d045cff9449012799","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/te.js","hash":"926145cefe85c78532d12f4c3804a36ffe4a0887","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/tet.js","hash":"3c1a5422d539b8184caf039d434f3a343e4ab4d9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/tg.js","hash":"e474f6ae34d9d006afafed3283b105705afbd53d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/th.js","hash":"03d25fe883d2c858690f95dcb7474341971183a1","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/tl-ph.js","hash":"fd0a84e0def5d6df400d965238e074c0a1cb9812","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/tlh.js","hash":"b138d39fcb6a41a69ee60e1e8e3dbe2c9ece7fc6","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/tr.js","hash":"fb803a9a5b7b8704da4b0a97bc8d994468daa7ac","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/tzl.js","hash":"86420d019381afd6d8db2a88e612c3f1d373652d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/tzm-latn.js","hash":"52ff85c0aa3630af9df1b4d13b01e3e014bec0ef","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/tzm.js","hash":"4b822dd07c06ff8c83be838ed323af1619a117e0","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ug-cn.js","hash":"0193ee5f2f551b038da22ee2371ce0f20671e872","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/uk.js","hash":"0d7e396a3f1088d91a83336f012469e2df373f76","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/ur.js","hash":"c2c49af559acd65c6dd2451109a4ea0a6e01ff2d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/uz-latn.js","hash":"3a6ae404d60630ba82d80efec0c2d03d345203dd","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/uz.js","hash":"d55ad8dd1d9b5163a5f33a3b989338b5e91a29a0","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/vi.js","hash":"dd9439513471e03ed52a50cb515dcf55e4707580","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/x-pseudo.js","hash":"47c1e140d76ebbba6109df6a4cbc69ebdbf7e959","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/zh-cn.js","hash":"f522d9fd9f4a173859e6b4fd7d4cdfca3c982274","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/yo.js","hash":"b6bc7769387697880ee79acf3591318b30d717d3","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/zh-hk.js","hash":"b7b23eef28faf4ab31ecd5c16b62612a35c9b235","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/locale/zh-tw.js","hash":"5011c856772a775d21e26f3db381de9fb69d1a48","modified":1523772394000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_underscore/arrays.js","hash":"7e65b546ec41d89756176cca0662bee605054f3a","modified":1373531985000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_underscore/chaining.js","hash":"be55887afb5f05515fc969449c166e924965a0e9","modified":1373531985000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_underscore/collections.js","hash":"735c1cc88fc76df7c4a73c1735d29728404ffe4c","modified":1373531985000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_underscore/functions.js","hash":"51f27a367d732c718ad869b9ed2ab15bc7cadc0b","modified":1373531985000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_underscore/index.html","hash":"a39a08f5018ae1b231aedc06363ae08acc0b19bc","modified":1373531985000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_underscore/objects.js","hash":"ce9b3cc415da73c599fecfb63cb1359439fee3f8","modified":1373531985000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_underscore/speed.js","hash":"395b81dd34c9267a308c055547e28d8d02198e56","modified":1373531985000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_underscore/utility.js","hash":"39dc56c4a644fb09006b6d967109ab9f94466bd6","modified":1373531985000},{"_id":"themes/icarus/source/libs/font-awesome/css/font-awesome.css","hash":"b5020c3860669185ba3f316fa7332cdf5c06f393","modified":1525046792000},{"_id":"themes/icarus/source/libs/font-awesome/css/font-awesome.min.css","hash":"7cd5a3384333f95c3d37d9488ad82cd6c4b03761","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/css/lg-fb-comment-box.css","hash":"844ce27b8488968bccb3e50bb49184ba2aae0625","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/css/lg-fb-comment-box.css.map","hash":"51e9df39edf0faa3f38c1bab0c1fa6c922b9edcb","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/css/lg-fb-comment-box.min.css","hash":"05830fadb8454f39dcc98c8686eb4d5c24b71fc0","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/css/lg-transitions.css.map","hash":"50c3348638b4d82fa08a449c690e8d2bb593005d","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/css/lg-transitions.css","hash":"7871c28498d74451d6aa438c8d3a1817810a1e19","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/css/lg-transitions.min.css","hash":"5c22e2073a4c96d6212c72135391b599e8d1359f","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/css/lightgallery.css","hash":"bef55316a32e512d5a8940e5d0bfe8bf7a9c5c61","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/css/lightgallery.css.map","hash":"3175b4107078674d25798979f7666f4daf31e624","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/css/lightgallery.min.css","hash":"c9a2e19c932b56f4a2ce30c98910d10b74edb38a","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/fonts/lg.svg","hash":"9a732790adc004b22022cc60fd5f77ec4c8e3e5a","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/img/loading.gif","hash":"607810444094b8619fa4efa6273bc2a7e38dd4b4","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/img/video-play.png","hash":"3ea484cdc04d2e4547f80cbf80001dcf248c94ef","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/img/vimeo-play.png","hash":"6190254f2804904a4a1fa1eb390dfd334e416992","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/img/youtube-play.png","hash":"fea6df9d9d43151f9c9d15f000adb30eb3e26fc4","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-autoplay.js","hash":"426bb78b93acfc39d533ea2bab1cec8dc289cf24","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-autoplay.min.js","hash":"d845741bcaf961579622880eb2a445257efad1ac","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-fullscreen.js","hash":"65c47ac65362854ba44b00a010bb01e3630209d8","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-fullscreen.min.js","hash":"b6b9e4022700b7faf2a5a175ba44a3bd938fdd20","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-hash.js","hash":"15d16516c5642d3de1566ff8fc9160136ccaa405","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-hash.min.js","hash":"43f1e1e720ab0e241c19b83aa26bd6848eab8edc","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-pager.js","hash":"8092c692b244bb26343eb03b91bd97deb9dafc9c","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-pager.min.js","hash":"25caa6ff65b1c6dee09941e795ae2633bdbab211","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-share.js","hash":"b7fb5f6474911060a351b0a6fe9dbb9ac3fb22aa","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-share.min.js","hash":"39c615f07c5d3aaa65a2c3068a30fdd6dd5c372d","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-thumbnail.js","hash":"3a6476b6df1d2bef4a21861a78776282a7a11ef1","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-thumbnail.min.js","hash":"18dd7d2909d1bfd6852f031d03e774b4428c512b","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-video.js","hash":"4f99b598f6bb18de9eca8c45c5b4373a03962367","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-video.min.js","hash":"032c001ab045a69856f9c3ed4a2a3bf12a8e310f","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-zoom.js","hash":"a758e2c8fcf710f9ff761da0eea0ab9321f3484d","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lg-zoom.min.js","hash":"15b49f9728439819ece15e4295cce254c87a4f45","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lightgallery.min.js","hash":"956ef9b706755318da69ad0b5d7786339d831251","modified":1525046792000},{"_id":"themes/icarus/source/libs/lightgallery/js/lightgallery.js","hash":"3cd19b33ba99efd5ba1d167da91720566d274b2c","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/59ZRklaO5bWGqF5A9baEERJtnKITppOI_IvcXXDNrsc.woff2","hash":"c4248ea800bd5608344ce163f5658b57e7ef9410","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/K88pR3goAWT7BTt32Z01mxJtnKITppOI_IvcXXDNrsc.woff2","hash":"e0350190d720a8fec0557ab47b318ec4e4486448","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/LWCjsQkB6EMdfHrEVqA1KRJtnKITppOI_IvcXXDNrsc.woff2","hash":"2c5b039b57f62625e88226a938679ec937431ad1","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNShWV49_lSm1NYrwo-zkhivY.woff2","hash":"22413bb8bfb78608c1e25aa1ed5c1f38557df79f","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSj0LW-43aMEzIO6XUTLjad8.woff2","hash":"63eb74ef040aade256f2274a7f31a914edddb0ea","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSpX5f-9o1vgP2EXwfjgl7AY.woff2","hash":"328a22fe3eec71ad9e5ece4d67dd62e79dab6b7f","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSq-j2U0lmluP9RWlSytm3ho.woff2","hash":"4dc6d7174ea6d89f4c45e43e1bfc3e03d8ffebaf","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSqaRobkAwv3vxw3jMhVENGA.woff2","hash":"415eee05976ab8b2471602a5ddb78a6c58fc21aa","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSugdm0LZdjqr5-oayXSOefg.woff2","hash":"a0b0c389cf46d63c850e61fed572485ff0b68183","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/MTP_ySUJH_bn48VBG8sNSv8zf_FOSsgRmwsS7Aa9k2w.woff2","hash":"c5f29fed6632efe0aa83318369f0d8c4061b775b","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/RjgO7rYTmqiVp7vzi-Q5URJtnKITppOI_IvcXXDNrsc.woff2","hash":"be201d32a9aa5d186723ebb3c538be691aa8c53a","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/cJZKeOuBrn4kERxqtaUH3VtXRa8TVwTICgirnJhmVJw.woff2","hash":"afc44700053c9a28f9ab26f6aec4862ac1d0795d","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/u-WUoqrET9fUeobQW7jkRRJtnKITppOI_IvcXXDNrsc.woff2","hash":"113978181dcac77baecef6115a9121d8f6e4fc3a","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBiYE0-AqJ3nfInTTiDXDjU4.woff2","hash":"5067c81462c15422853c94d21a1726865a61634f","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBjTOQ_MqJVwkKsUn0wKzc2I.woff2","hash":"b366f2fda2e524eb5ef50058eefff249a3b96e6c","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBjUj_cnvWIuuBMVgbX098Mw.woff2","hash":"d22904914469be735490e3c8cb093c7862896dd5","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBkbcKLIaa1LC45dFaAfauRA.woff2","hash":"ae80fb3cd16339aa7b5da280ab53975523dcaac2","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBmo_sUJ8uO4YLWRInS22T3Y.woff2","hash":"b85efde42fa3a03c32b1d31c6cd74c622fc7916c","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBr6up8jxqWt8HVA3mDhkV_0.woff2","hash":"d0b40a7848703556c6631f24e961a98ca5829255","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/xjAJXh38I15wypJXxuGMBo4P5ICox8Kq3LLUNMylGO4.woff2","hash":"e75607ba1417181397c700775b84303d5a2957b9","modified":1525046792000},{"_id":"themes/icarus/source/libs/open-sans/fonts/xozscpT2726on7jbcb_pAhJtnKITppOI_IvcXXDNrsc.woff2","hash":"be365eca44760ce3fc9b377c43d4634958479c69","modified":1525046792000},{"_id":"themes/icarus/source/libs/source-code-pro/fonts/mrl8jkM18OlOQN8JLgasD9V_2ngZ8dMf8fLgjYEouxg.woff2","hash":"942addaec4d3a60af33947a84a3d85f926015947","modified":1525046792000},{"_id":"themes/icarus/source/libs/source-code-pro/fonts/mrl8jkM18OlOQN8JLgasDy2Q8seG17bfDXYR_jUsrzg.woff2","hash":"b0e0bb5ef78db8b15d430d0b9be9d4329289a310","modified":1525046792000},{"_id":"themes/icarus/node_modules/bluebird/js/browser/bluebird.core.js","hash":"77b537ba30a5592024b64e92b535cc73376f70c2","modified":1507132264000},{"_id":"themes/icarus/node_modules/bluebird/js/browser/bluebird.min.js","hash":"2a531eb53b1867033dd5297acf2f084ad3babdb1","modified":1507132266000},{"_id":"themes/icarus/node_modules/highlight.js/lib/languages/mathematica.js","hash":"78cee2f480b2ddb3e23dc7cebfb0bc07b60e0869","modified":1496198853000},{"_id":"themes/icarus/source/css/images/Basketball-icon.png","hash":"7c78739236a9a6b14a16d0af2646d8cab0ce88ce","modified":1526665017785},{"_id":"themes/icarus/source/css/images/avatar3.png","hash":"076e9b8855366459e0af7ecc8a4d9f1549fb8a7e","modified":1526319649799},{"_id":"themes/icarus/source/css/images/i.ico","hash":"406505911fe153cf4ffddad03340d4dd2e48a20b","modified":1526664174850},{"_id":"themes/icarus/source/libs/font-awesome/fonts/FontAwesome.otf","hash":"1b22f17fdc38070de50e6d1ab3a32da71aa2d819","modified":1525046792000},{"_id":"themes/icarus/source/libs/font-awesome/fonts/fontawesome-webfont.eot","hash":"965ce8f688fedbeed504efd498bc9c1622d12362","modified":1525046792000},{"_id":"themes/icarus/source/libs/font-awesome/fonts/fontawesome-webfont.woff","hash":"6d7e6a5fc802b13694d8820fc0138037c0977d2e","modified":1525046792000},{"_id":"themes/icarus/source/libs/jquery/2.1.3/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1525046792000},{"_id":"themes/icarus/source/libs/font-awesome/fonts/fontawesome-webfont.woff2","hash":"97e438cc545714309882fbceadbf344fcaddcec5","modified":1525046792000},{"_id":"source/_posts/attachments/json-tool.jar","hash":"0984a87b8dcf45f7f1a8de8df7c9cafef3a6173f","modified":1527644841669},{"_id":"themes/icarus/node_modules/bluebird/js/browser/bluebird.js","hash":"7e43d39c21a8e049ca38d316ac8ea3c5e551b4b9","modified":1507132266000},{"_id":"themes/icarus/node_modules/moment/min/locales.min.js","hash":"e925042defd75b759330182c4843824cb9158047","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/min/moment-with-locales.min.js","hash":"d35121a6b2c818ce3da0f2f75b6a6e2c17bbca33","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/create/check-overflow.js","hash":"5eca9703243cdaaf5648cce596d5d9964eb94c98","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/create/date-from-array.js","hash":"163a82cbe1f1c1fa2518826943c282523d65192b","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/create/from-anything.js","hash":"f213f5115195de90f372ee44a51ba9eadb25c0ba","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/create/from-array.js","hash":"05f770fc99ac189acc5b179f2c3c9cd3d3be0c13","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/create/from-object.js","hash":"dcb90c6dcfaa874ccb1dda84424137aaf565a244","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/create/from-string-and-array.js","hash":"0dd8f1960db97f65ac71395c5c627327ebe7b769","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/create/from-string-and-format.js","hash":"afbd37e1bbc2b59746635b8439a8732177203518","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/create/from-string.js","hash":"8ab34061ffe7a829e19dfa4e99bc55de7ee1876d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/create/local.js","hash":"6b7c65e9ca16eba14ddddeecc1b003b984c78fde","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/create/parsing-flags.js","hash":"99da4478b26bb4c1ed50d5feabd4d1049193986b","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/create/utc.js","hash":"7b05ddc73ee99b7f13f526adbe5ef6d25b87b8b9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/create/valid.js","hash":"50818acc3aa93f580376bd30916a9359547a74dd","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/abs.js","hash":"7970d96eb35228bb3ecb4603b63b27d3756367ad","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/add-subtract.js","hash":"0fd8c21e60721fe9945b83988cd2b47122f45c6f","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/as.js","hash":"22402f358f58d8ec75e09851075d4da87690f92c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/bubble.js","hash":"fd42b8707944aca1780433a52b74ce2f2889fd41","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/clone.js","hash":"7ae86719b47c41d77be37909771ebfbd4cc22b8c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/constructor.js","hash":"889aa29ea856ff5c07e0bda7706ab000da2ec184","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/create.js","hash":"d83090cae06a90aab3571a633f15b19fb6c6dde1","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/duration.js","hash":"31e369b9d7179286568d61783bcf491e13b85f3c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/get.js","hash":"a11b95805a9a67d70f3f83d683c880481ec826c5","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/humanize.js","hash":"6dbf4405af5bc44e8688b012e1855e52e62cba92","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/iso-string.js","hash":"99d9214c155c2e5dd622d3c1cd95ea9318589c98","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/prototype.js","hash":"1e5e93f365cc7d4e328a80faeb22970a18c560a5","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/duration/valid.js","hash":"3d579ae000556bcd8fb905c7526e201510baf906","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/format/format.js","hash":"022f3b11c33b096245dd7d727fd31f66f23e33b7","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/base-config.js","hash":"a54ba739ac31f745d39f56c85908ebff3bbd2178","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/calendar.js","hash":"422e8f0d039ab047c6e94cde981fcf4afd48179e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/constructor.js","hash":"4c99dfd1739fa75db2c2fd920dd5defbdd1f0030","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/en.js","hash":"b8cd576b547ee4dcfde714315f3c6741f77bedcc","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/formats.js","hash":"745faed207ad9922484cf0b551ca0faf45e2cbb0","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/invalid.js","hash":"f719dd1276dceb5593675314c1bd3c4afb33f3c0","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/lists.js","hash":"17d12b7ae719d7edee5106b5e6565a3d634945af","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/locale.js","hash":"2e00925beeb939bef0a1fac476b6c76f9b58c180","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/locales.js","hash":"364121637257401d49b023ebb1bc30c598c31a86","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/ordinal.js","hash":"16794f45255732ba8f8a8c20b72eb427ea4c1a97","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/pre-post-format.js","hash":"da591dacc6f6be5e6a38dd204a2c3aba8ebff07f","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/prototype.js","hash":"22a17915e51aa0eeda3e13fffc76762822fa32a1","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/relative.js","hash":"a115cccfbd6d0bfec41c1d53114f2299e6cd05b3","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/locale/set.js","hash":"f53c6d0f3feb85db364fe539143faa1b212821c5","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/add-subtract.js","hash":"dae6987d13e30ae9952f0773bef1ce3fd9fc3a58","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/calendar.js","hash":"757c364df57f2948148d1d41f7db9bc5cf5cdfb9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/clone.js","hash":"28653ed90f185af48a3f59fe843b7691bde47ce2","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/compare.js","hash":"11baed31ee42bd8a8436cd1a403208bdc46963ca","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/constructor.js","hash":"ef0d02f717bf7737daddf6a529885dfdf3af54f4","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/creation-data.js","hash":"4673ff4ae13812dd63731aefa0f98fdb58d08bd7","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/diff.js","hash":"3358733b790896c5c3df04316c3f405869a9eeb9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/format.js","hash":"8f91c4d9adc4a38278e25a1cc997b9b5d60a9a51","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/from.js","hash":"76885ca5df3fa448d85db79d21417328c47a7526","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/get-set.js","hash":"a341d70c3b001da07a3bf7b6535a9bf5d9bc4521","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/locale.js","hash":"65a2e75f9d3aba7873ab65c5ce2ff635aa161676","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/min-max.js","hash":"8fc7d30f7ccfc15fbfab5b340b9349434cbad7ac","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/moment.js","hash":"87ec5cd2be62750d3d911d7fa48bed7e65d58b72","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/now.js","hash":"693a36ace148dca0c85c462e8807c754bc5a1826","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/prototype.js","hash":"6bbc0787dc92dbd8bef33f638c6ef7b87777b7c4","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/start-end-of.js","hash":"9998dca34d44284857ef3de6d069c2a6c1ef526a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/to-type.js","hash":"eb9e331061cdfe96224c395ee32cd13ba01c291f","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/to.js","hash":"c783783792c1e65861196899493580d99e7f5ed9","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/moment/valid.js","hash":"358c8898e1991e71977230a26ef5f96aa4ca03e3","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/parse/regex.js","hash":"1e6595550e6d854f2d9537f9678aee421becec5a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/parse/token.js","hash":"70c25cc4e7ac2138c7d7b87881200c2faa143d50","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/aliases.js","hash":"d05738b4a8499cac86c552907da325391b84c0aa","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/constants.js","hash":"191bac7e3f512f7c878840d9ac4142f83be90ec6","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/day-of-month.js","hash":"61cbb6d09cdcf4c7be366187a55a2e5b451d3395","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/day-of-week.js","hash":"599a5661160abc0ddda5a351c2caa1604080c678","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/day-of-year.js","hash":"3332403b269764733b11b74570f7878b548ddcdd","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/hour.js","hash":"d325fe7bc479546d94550ee994e1509d48f33005","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/millisecond.js","hash":"821ccd1797c28cc384cd21a871913c484ff2611e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/minute.js","hash":"f8a800a3077d4ac78af3ac0ac80f2cb1bb8d3cf1","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/month.js","hash":"8e9da1b70b137f25a20ea7bdbde23bc24c809f7f","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/offset.js","hash":"f51fc54c4bfab304106ebbe864bdd7e15e8b170a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/priorities.js","hash":"055a4899805f6d3b40504072426e146e666f9cc2","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/quarter.js","hash":"482e071ff9d870d4f0db66ff97d957f4175fd790","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/second.js","hash":"ed0266435250c25609ad62c4fc2b578ba719015a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/timestamp.js","hash":"f88f2f7a738c12ac183a8a6c75529c99390c892e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/timezone.js","hash":"6003686691e1b58e7a5704077d597c403f01a72a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/units.js","hash":"44ff48f40c6c72cc04eae845d963653d758ff130","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/week-calendar-utils.js","hash":"da0d4141725fb331359ab7c7e4e66f476bb38a90","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/week-year.js","hash":"876efd2a0d40a20d1f29fd8dbc96deba5796ecf4","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/week.js","hash":"fca9262205c450e5045cd59e211950df412ecc8e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/units/year.js","hash":"0966b292f2b6e42e92aa2af8a9c524cd04324fa7","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/abs-ceil.js","hash":"cbe89a30190bc1ed24d22f97c4cf824d9ecfe22c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/abs-floor.js","hash":"14e835a73463b7638602313a8bb999abd879dc1e","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/abs-round.js","hash":"ab565f07682ac2f9526861c3714925d0d516f3a8","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/compare-arrays.js","hash":"98362fe208ec6819fb10c1377e4c29f60df2cd67","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/defaults.js","hash":"23d889aa1c1ebf29489955df68ee0863df09325d","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/deprecate.js","hash":"1f46e2931aa788295eec0f3af7e36b524481cf8b","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/extend.js","hash":"8ddf09c4d51a5cca4a7f389a13530a97abf9267f","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/has-own-prop.js","hash":"29cad3d80496e33e738306d8e33d76ef6f52fafd","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/hooks.js","hash":"83fde0451f1c8d15c73978bbb8d941b083a0177a","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/index-of.js","hash":"b8060c497bbd6f76364bfeffccfab2ef55776ef8","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/is-array.js","hash":"fefa05cb72285be0bdd3bbcd667ef4c4fc007a41","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/is-date.js","hash":"de623e76e5abbc4a989105530cb3ea56a7d358ca","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/is-function.js","hash":"0129d069077b5c9ea4e0b39281a0554489372c23","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/is-number.js","hash":"83b1e4ec2836fe295de1b24ffef39ed569c586db","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/is-object-empty.js","hash":"6ffeb0aaee85dcfe657aaa9be2dc8d5ff91cf5dd","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/is-object.js","hash":"2b4153f7a3170e5db5ca8e179726abc31f72e30c","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/is-undefined.js","hash":"073c7156a8b0a1580ef38bed126f0b981ea025bb","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/keys.js","hash":"2a83c998133df29806128760f38e07f9e8540695","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/map.js","hash":"76a82ff4dfdb612ce4233339372b902fbef84768","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/mod.js","hash":"b10c21f16f9d8fa200d1119e98149cde8caecbf2","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/some.js","hash":"f3f99788200e51c767a85fe41efd40409fc817e7","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/to-int.js","hash":"2584cdc4e7102dfa932a8ba84d4051d4f3dde5ed","modified":1523772394000},{"_id":"themes/icarus/node_modules/moment/src/lib/utils/zero-fill.js","hash":"19208d299f6befe694591fd2912306e0f28f1e0d","modified":1523772394000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_underscore/vendor/jslitmus.js","hash":"141af7191b0f8be947e616cab9816e68231230ca","modified":1373449276000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_underscore/vendor/qunit.css","hash":"1cf8b4bfff4b27bfdff531b3f51a34ba1bc7c236","modified":1373531985000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_underscore/vendor/qunit.js","hash":"0d8fdaeec180bcc70af77f58ca31d99f6bded201","modified":1373531985000},{"_id":"themes/icarus/source/css/images/avatar4.png","hash":"fa5344826c80dcc523a4a041ff50ee868f39ebea","modified":1526661210145},{"_id":"themes/icarus/source/css/images/c02a6475bff33996304bbb5e104883f4.jpg","hash":"fa5344826c80dcc523a4a041ff50ee868f39ebea","modified":1526661210145},{"_id":"themes/icarus/source/css/images/logo4.png","hash":"fa5344826c80dcc523a4a041ff50ee868f39ebea","modified":1526661210145},{"_id":"themes/icarus/source/libs/font-awesome/fonts/fontawesome-webfont.ttf","hash":"61d8d967807ef12598d81582fa95b9f600c3ee01","modified":1525046792000},{"_id":"themes/icarus/node_modules/moment/min/locales.js","hash":"c7453bcc6fc2fa05063528879273f0500949fe54","modified":1523772394000},{"_id":"themes/icarus/node_modules/underscore.string/test/test_underscore/vendor/jquery.js","hash":"0d7896e2bb23f88e26e52b22a075350b354df447","modified":1373531985000},{"_id":"themes/icarus/source/libs/font-awesome/fonts/fontawesome-webfont.svg","hash":"c0522272bbaef2acb3d341912754d6ea2d0ecfc0","modified":1525046792000},{"_id":"themes/icarus/node_modules/moment/min/moment-with-locales.js","hash":"3135b5356bdb9aee6240ae86e963f0c719a7273a","modified":1523772394000},{"_id":"themes/icarus/source/css/images/badboygq.jpg","hash":"0ffb9677a1bbd06ec6c72b324dd17b9437877d48","modified":1526664810837}],"Category":[{"name":"安装部署","_id":"cjlok43va00062tpbsn87crvh"},{"name":"大数据","_id":"cjlok43vn000n2tpbns393yk3"},{"name":"Docker","_id":"cjlok43vp000v2tpb5801fyth"},{"name":"环境配置","_id":"cjlok43vu00182tpblgtu3x3s"},{"name":"总结","_id":"cjlok43w300202tpbendm8jbm"},{"name":"组件","_id":"cjlok43w500292tpb0wq4lf0d"},{"name":"Linux","_id":"cjlok43wd002p2tpb0gnvyov2"},{"name":"框架","_id":"cjlok43wg002x2tpbovicivly"},{"name":"语言","_id":"cjlok43wk00352tpb06rvninj"},{"name":"Spark-On-Yarn","_id":"cjlok43wn003e2tpbwk8quxi2"},{"name":"工程框架","_id":"cjlok43x5004q2tpbhepy6efy"},{"name":"tool","_id":"cjlok43xb00552tpbtq455xuk"},{"name":"Spring","_id":"cjlok43xf005e2tpb9895g1ke"},{"name":"博客","_id":"cjlok43xh005n2tpb168skoel"},{"name":"Hexo","_id":"cjlok43xk005w2tpbb3x86nto"},{"name":"快捷键","_id":"cjlok43xl00632tpbzlm83aej"},{"name":"碎片知识","_id":"cjlok43xm00692tpbzljm8o8q"}],"Data":[],"Page":[{"_content":"WHXmBFaAkY","source":"baidu_verify_WHXmBFaAkY.html","raw":"WHXmBFaAkY","date":"2018-05-18T06:50:40.014Z","updated":"2018-05-17T01:53:01.988Z","path":"baidu_verify_WHXmBFaAkY.html","title":"","comments":1,"layout":"page","_id":"cjlok43qs00002tpbte8o3had","content":"WHXmBFaAkY","site":{"data":{}},"excerpt":"","more":"WHXmBFaAkY"},{"_content":"google-site-verification: google00655d7c846aab3a.html","source":"google00655d7c846aab3a.html","raw":"google-site-verification: google00655d7c846aab3a.html","date":"2018-05-18T06:50:40.022Z","updated":"2018-05-17T02:15:40.515Z","path":"google00655d7c846aab3a.html","title":"","comments":1,"layout":"page","_id":"cjlok43qt00012tpbzylon45i"},{"title":"404 Not Found：该页无法显示","toc":false,"comments":0,"_content":"","source":"404/index.md","raw":"title: 404 Not Found：该页无法显示\ntoc: false\ncomments: false\npermalink: /404\n---\n","date":"2018-05-18T06:50:42.172Z","updated":"2018-05-08T10:00:51.508Z","path":"/404.html","layout":"page","_id":"cjlok43v300022tpb7re766br"},{"title":"关于","date":"2018-05-08T07:52:07.000Z","_content":"Nothing\n","source":"about/index.md","raw":"---\ntitle: 关于\ndate: 2018-05-08 15:52:07\n---\nNothing\n","updated":"2018-08-07T07:27:50.398Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjlok43v800042tpbsph2mqus"},{"title":"分类","layout":"categories","_content":"","source":"categories/index.md","raw":"title: \"分类\"\nlayout: \"categories\"\n---\n","date":"2018-08-13T06:25:13.838Z","updated":"2018-08-07T07:28:04.841Z","path":"categories/index.html","comments":1,"_id":"cjlok43vc00082tpby089hltg"},{"title":"标签","layout":"tags","_content":"","source":"tags/index.md","raw":"title: \"标签\"\nlayout: \"tags\"\n---\n","date":"2018-08-07T07:27:29.140Z","updated":"2018-08-07T07:27:29.135Z","path":"tags/index.html","comments":1,"_id":"cjlok43zo009l2tpb44xbt6dv"}],"Post":[{"title":"Ambari镜像搭建","date":"2018-07-09T17:08:44.939Z","toc":true,"_content":"\n\n[TOC]\n\n# 创建基础容器\n\n```bash\ndocker run -itd  --net=br  --name ambari-agent --hostname  ambari-agent yaosong5/centosbase:1.0 &> /dev/null\n```\n\n关闭 selinux , 需要重启\n`vim /etc/selinux/config` \n\n```\nSELINUX=disabled\n```\n\n<!--more -->\n\n# server端\n\n## 更换yum源\n\n```\nwget http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.0.1/ambari.repo\ncp ambari.repo /etc/yum.repos.d\n```\n\n## 安装依赖及其server\n\n```\nyum install epel-release \nyum repolist\nyum install ambari-server  \n```\n\n## 启动初始化\n\n`ambari-server setup`\n会有一连串的提示\n\n会提示安装 jdk，网速好的可以确定，否则可以下载 jdk-6u31-linux-x64.bin，放到 /var/lib/ambari-server/resources/ 下面，可以指定已经安装的jdk\n接着会提示配置用的数据库，可以选择 Oracle 或 postgresql，选择 n 会按默认配置\n数据库类型：postgresql\n数据库：ambari\n用户名：ambari\n密码：bigdata\n如果提示 Oracle JDK license，yes\n等待安装完成\n\n\n\n# agent端\n\n安装 ambari-agent\n\n```\nyum install -y ambari-agent\nchkconfig --add ambari-agent\n```\n\n将 ambari.server 上的 3 个. repo 文件复制到 hadoop 集群的三台服务器上；并完成 yum 源更新的命令。\n\n 安装 ambari-agent：在集群的 3 台电脑上执行添加，并添加成开机自启动服务：　　\n\n yum install -y ambari-agent\n chkconfig --add ambari-agent\n sudo ambari-agent start\n\n# 分别启动server agent \n\n在server和agent上分别执行\n\n```\nambari-agent start\nambari-server start  \n```\n\n## 访问\n\nhttp://192.168.1.133:8080  \n\n用户名密码: admin,admin\n\n\n\n# 保存容器为镜像\n\n```bash\ndocker commit -m \"bigdata:ambari-server\"  --author=\"yaosong\"  ambr  yaosong5/ambari-server:1.0\n```\n\n# 根据镜像创建容器\n\n```bash\ndocker run -itd  --net=br  --name ambari1 --hostname ambari1 yaosong5/ambari-server:1.0 &> /dev/null\ndocker run -itd  --net=br  --name ambari2 --hostname ambari2 yaosong5/ambari-server:1.0 &> /dev/null\ndocker run -itd  --net=br  --name ambari3 --hostname ambari3 yaosong5/ambari-server:1.0 &> /dev/null\n```\n\n# 停止and删除容器\n\n```bash\ndocker stop ambari1\ndocker stop ambari2\ndocker stop ambari3\n\ndocker rm ambari1\ndocker rm ambari2\ndocker rm ambari3\n```","source":"_posts/Ambari搭建.md","raw":"---\ntitle:  Ambari镜像搭建\ndate: 2018年08月06日 22时15分52秒\ntags:  [Docker,Ambari]\ncategories: 安装部署\ntoc: true\n---\n\n\n[TOC]\n\n# 创建基础容器\n\n```bash\ndocker run -itd  --net=br  --name ambari-agent --hostname  ambari-agent yaosong5/centosbase:1.0 &> /dev/null\n```\n\n关闭 selinux , 需要重启\n`vim /etc/selinux/config` \n\n```\nSELINUX=disabled\n```\n\n<!--more -->\n\n# server端\n\n## 更换yum源\n\n```\nwget http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.0.1/ambari.repo\ncp ambari.repo /etc/yum.repos.d\n```\n\n## 安装依赖及其server\n\n```\nyum install epel-release \nyum repolist\nyum install ambari-server  \n```\n\n## 启动初始化\n\n`ambari-server setup`\n会有一连串的提示\n\n会提示安装 jdk，网速好的可以确定，否则可以下载 jdk-6u31-linux-x64.bin，放到 /var/lib/ambari-server/resources/ 下面，可以指定已经安装的jdk\n接着会提示配置用的数据库，可以选择 Oracle 或 postgresql，选择 n 会按默认配置\n数据库类型：postgresql\n数据库：ambari\n用户名：ambari\n密码：bigdata\n如果提示 Oracle JDK license，yes\n等待安装完成\n\n\n\n# agent端\n\n安装 ambari-agent\n\n```\nyum install -y ambari-agent\nchkconfig --add ambari-agent\n```\n\n将 ambari.server 上的 3 个. repo 文件复制到 hadoop 集群的三台服务器上；并完成 yum 源更新的命令。\n\n 安装 ambari-agent：在集群的 3 台电脑上执行添加，并添加成开机自启动服务：　　\n\n yum install -y ambari-agent\n chkconfig --add ambari-agent\n sudo ambari-agent start\n\n# 分别启动server agent \n\n在server和agent上分别执行\n\n```\nambari-agent start\nambari-server start  \n```\n\n## 访问\n\nhttp://192.168.1.133:8080  \n\n用户名密码: admin,admin\n\n\n\n# 保存容器为镜像\n\n```bash\ndocker commit -m \"bigdata:ambari-server\"  --author=\"yaosong\"  ambr  yaosong5/ambari-server:1.0\n```\n\n# 根据镜像创建容器\n\n```bash\ndocker run -itd  --net=br  --name ambari1 --hostname ambari1 yaosong5/ambari-server:1.0 &> /dev/null\ndocker run -itd  --net=br  --name ambari2 --hostname ambari2 yaosong5/ambari-server:1.0 &> /dev/null\ndocker run -itd  --net=br  --name ambari3 --hostname ambari3 yaosong5/ambari-server:1.0 &> /dev/null\n```\n\n# 停止and删除容器\n\n```bash\ndocker stop ambari1\ndocker stop ambari2\ndocker stop ambari3\n\ndocker rm ambari1\ndocker rm ambari2\ndocker rm ambari3\n```","slug":"Ambari搭建","published":1,"updated":"2018-08-10T18:48:37.181Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43v500032tpbaz8j4r72","content":"<p>[TOC]</p>\n<h1 id=\"创建基础容器\"><a href=\"#创建基础容器\" class=\"headerlink\" title=\"创建基础容器\"></a>创建基础容器</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -itd  --net=br  --name ambari-agent --hostname  ambari-agent yaosong5/centosbase:1.0 &amp;&gt; /dev/null</span><br></pre></td></tr></table></figure>\n<p>关闭 selinux , 需要重启<br><code>vim /etc/selinux/config</code> </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELINUX=disabled</span><br></pre></td></tr></table></figure>\n<a id=\"more\"></a>\n<h1 id=\"server端\"><a href=\"#server端\" class=\"headerlink\" title=\"server端\"></a>server端</h1><h2 id=\"更换yum源\"><a href=\"#更换yum源\" class=\"headerlink\" title=\"更换yum源\"></a>更换yum源</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.0.1/ambari.repo</span><br><span class=\"line\">cp ambari.repo /etc/yum.repos.d</span><br></pre></td></tr></table></figure>\n<h2 id=\"安装依赖及其server\"><a href=\"#安装依赖及其server\" class=\"headerlink\" title=\"安装依赖及其server\"></a>安装依赖及其server</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install epel-release </span><br><span class=\"line\">yum repolist</span><br><span class=\"line\">yum install ambari-server</span><br></pre></td></tr></table></figure>\n<h2 id=\"启动初始化\"><a href=\"#启动初始化\" class=\"headerlink\" title=\"启动初始化\"></a>启动初始化</h2><p><code>ambari-server setup</code><br>会有一连串的提示</p>\n<p>会提示安装 jdk，网速好的可以确定，否则可以下载 jdk-6u31-linux-x64.bin，放到 /var/lib/ambari-server/resources/ 下面，可以指定已经安装的jdk<br>接着会提示配置用的数据库，可以选择 Oracle 或 postgresql，选择 n 会按默认配置<br>数据库类型：postgresql<br>数据库：ambari<br>用户名：ambari<br>密码：bigdata<br>如果提示 Oracle JDK license，yes<br>等待安装完成</p>\n<h1 id=\"agent端\"><a href=\"#agent端\" class=\"headerlink\" title=\"agent端\"></a>agent端</h1><p>安装 ambari-agent</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y ambari-agent</span><br><span class=\"line\">chkconfig --add ambari-agent</span><br></pre></td></tr></table></figure>\n<p>将 ambari.server 上的 3 个. repo 文件复制到 hadoop 集群的三台服务器上；并完成 yum 源更新的命令。</p>\n<p> 安装 ambari-agent：在集群的 3 台电脑上执行添加，并添加成开机自启动服务：　　</p>\n<p> yum install -y ambari-agent<br> chkconfig –add ambari-agent<br> sudo ambari-agent start</p>\n<h1 id=\"分别启动server-agent\"><a href=\"#分别启动server-agent\" class=\"headerlink\" title=\"分别启动server agent\"></a>分别启动server agent</h1><p>在server和agent上分别执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ambari-agent start</span><br><span class=\"line\">ambari-server start</span><br></pre></td></tr></table></figure>\n<h2 id=\"访问\"><a href=\"#访问\" class=\"headerlink\" title=\"访问\"></a>访问</h2><p><a href=\"http://192.168.1.133:8080\" target=\"_blank\" rel=\"noopener\">http://192.168.1.133:8080</a>  </p>\n<p>用户名密码: admin,admin</p>\n<h1 id=\"保存容器为镜像\"><a href=\"#保存容器为镜像\" class=\"headerlink\" title=\"保存容器为镜像\"></a>保存容器为镜像</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker commit -m <span class=\"string\">\"bigdata:ambari-server\"</span>  --author=<span class=\"string\">\"yaosong\"</span>  ambr  yaosong5/ambari-server:1.0</span><br></pre></td></tr></table></figure>\n<h1 id=\"根据镜像创建容器\"><a href=\"#根据镜像创建容器\" class=\"headerlink\" title=\"根据镜像创建容器\"></a>根据镜像创建容器</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -itd  --net=br  --name ambari1 --hostname ambari1 yaosong5/ambari-server:1.0 &amp;&gt; /dev/null</span><br><span class=\"line\">docker run -itd  --net=br  --name ambari2 --hostname ambari2 yaosong5/ambari-server:1.0 &amp;&gt; /dev/null</span><br><span class=\"line\">docker run -itd  --net=br  --name ambari3 --hostname ambari3 yaosong5/ambari-server:1.0 &amp;&gt; /dev/null</span><br></pre></td></tr></table></figure>\n<h1 id=\"停止and删除容器\"><a href=\"#停止and删除容器\" class=\"headerlink\" title=\"停止and删除容器\"></a>停止and删除容器</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker stop ambari1</span><br><span class=\"line\">docker stop ambari2</span><br><span class=\"line\">docker stop ambari3</span><br><span class=\"line\"></span><br><span class=\"line\">docker rm ambari1</span><br><span class=\"line\">docker rm ambari2</span><br><span class=\"line\">docker rm ambari3</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>[TOC]</p>\n<h1 id=\"创建基础容器\"><a href=\"#创建基础容器\" class=\"headerlink\" title=\"创建基础容器\"></a>创建基础容器</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -itd  --net=br  --name ambari-agent --hostname  ambari-agent yaosong5/centosbase:1.0 &amp;&gt; /dev/null</span><br></pre></td></tr></table></figure>\n<p>关闭 selinux , 需要重启<br><code>vim /etc/selinux/config</code> </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELINUX=disabled</span><br></pre></td></tr></table></figure>","more":"<h1 id=\"server端\"><a href=\"#server端\" class=\"headerlink\" title=\"server端\"></a>server端</h1><h2 id=\"更换yum源\"><a href=\"#更换yum源\" class=\"headerlink\" title=\"更换yum源\"></a>更换yum源</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.0.1/ambari.repo</span><br><span class=\"line\">cp ambari.repo /etc/yum.repos.d</span><br></pre></td></tr></table></figure>\n<h2 id=\"安装依赖及其server\"><a href=\"#安装依赖及其server\" class=\"headerlink\" title=\"安装依赖及其server\"></a>安装依赖及其server</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install epel-release </span><br><span class=\"line\">yum repolist</span><br><span class=\"line\">yum install ambari-server</span><br></pre></td></tr></table></figure>\n<h2 id=\"启动初始化\"><a href=\"#启动初始化\" class=\"headerlink\" title=\"启动初始化\"></a>启动初始化</h2><p><code>ambari-server setup</code><br>会有一连串的提示</p>\n<p>会提示安装 jdk，网速好的可以确定，否则可以下载 jdk-6u31-linux-x64.bin，放到 /var/lib/ambari-server/resources/ 下面，可以指定已经安装的jdk<br>接着会提示配置用的数据库，可以选择 Oracle 或 postgresql，选择 n 会按默认配置<br>数据库类型：postgresql<br>数据库：ambari<br>用户名：ambari<br>密码：bigdata<br>如果提示 Oracle JDK license，yes<br>等待安装完成</p>\n<h1 id=\"agent端\"><a href=\"#agent端\" class=\"headerlink\" title=\"agent端\"></a>agent端</h1><p>安装 ambari-agent</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install -y ambari-agent</span><br><span class=\"line\">chkconfig --add ambari-agent</span><br></pre></td></tr></table></figure>\n<p>将 ambari.server 上的 3 个. repo 文件复制到 hadoop 集群的三台服务器上；并完成 yum 源更新的命令。</p>\n<p> 安装 ambari-agent：在集群的 3 台电脑上执行添加，并添加成开机自启动服务：　　</p>\n<p> yum install -y ambari-agent<br> chkconfig –add ambari-agent<br> sudo ambari-agent start</p>\n<h1 id=\"分别启动server-agent\"><a href=\"#分别启动server-agent\" class=\"headerlink\" title=\"分别启动server agent\"></a>分别启动server agent</h1><p>在server和agent上分别执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ambari-agent start</span><br><span class=\"line\">ambari-server start</span><br></pre></td></tr></table></figure>\n<h2 id=\"访问\"><a href=\"#访问\" class=\"headerlink\" title=\"访问\"></a>访问</h2><p><a href=\"http://192.168.1.133:8080\" target=\"_blank\" rel=\"noopener\">http://192.168.1.133:8080</a>  </p>\n<p>用户名密码: admin,admin</p>\n<h1 id=\"保存容器为镜像\"><a href=\"#保存容器为镜像\" class=\"headerlink\" title=\"保存容器为镜像\"></a>保存容器为镜像</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker commit -m <span class=\"string\">\"bigdata:ambari-server\"</span>  --author=<span class=\"string\">\"yaosong\"</span>  ambr  yaosong5/ambari-server:1.0</span><br></pre></td></tr></table></figure>\n<h1 id=\"根据镜像创建容器\"><a href=\"#根据镜像创建容器\" class=\"headerlink\" title=\"根据镜像创建容器\"></a>根据镜像创建容器</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run -itd  --net=br  --name ambari1 --hostname ambari1 yaosong5/ambari-server:1.0 &amp;&gt; /dev/null</span><br><span class=\"line\">docker run -itd  --net=br  --name ambari2 --hostname ambari2 yaosong5/ambari-server:1.0 &amp;&gt; /dev/null</span><br><span class=\"line\">docker run -itd  --net=br  --name ambari3 --hostname ambari3 yaosong5/ambari-server:1.0 &amp;&gt; /dev/null</span><br></pre></td></tr></table></figure>\n<h1 id=\"停止and删除容器\"><a href=\"#停止and删除容器\" class=\"headerlink\" title=\"停止and删除容器\"></a>停止and删除容器</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker stop ambari1</span><br><span class=\"line\">docker stop ambari2</span><br><span class=\"line\">docker stop ambari3</span><br><span class=\"line\"></span><br><span class=\"line\">docker rm ambari1</span><br><span class=\"line\">docker rm ambari2</span><br><span class=\"line\">docker rm ambari3</span><br></pre></td></tr></table></figure>"},{"title":"Centos7上搭建Jenkins","date":"2018-06-21T14:11:28.272Z","toc":true,"_content":"\n\n之前用yum模式安装，总是启动报错，解决了一番，未找到解决方案，后直接下载war包进行安装部署\n\n默认安装了Java\n<!-- more -->\n\n# 1. 安装 jenkins\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=480580003&auto=1&height=66\"></iframe>\n\n``` bash\ncd /opt\nmkdir /jenkins\ncd jenkins\nmkdir jenkins_home\nmkdir jenkins_node\nwget http://mirrors.jenkins-ci.org/war/latest/jenkins.war\n```\n\n\n# 2. 编写可执行文件\n\n  ` vim start_jenkins.sh`\n```bash\n     #!/bin/bash\n     JENKINS_ROOT=/opt/jenkins\n     export JENKINS_HOME=$JENKINS_ROOT/jenkins_home\n     java -jar $JENKINS_ROOT/jenkins.war --httpPort=8000\n```\n   修改文件的权限： ` chmod  a+x   start_jenkins.sh`\n\n   启动 jenkins:  `   nohup ./start_jenkins.sh > jenkins.log 2>& 1& `               \n# 3 访问 jenkins\n   输入 http:// 服务器地址: 8000\n\n注意：在启动日志中会出现初始密码，这个用来首次登陆Jenkins使用\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fu5j2mlpouj31d60lwtfs.jpg)\n\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fu5j4it7u3j31600nawfp.jpg)\n\n参考\n[在 Centos7 上搭建 jenkins](https://blog.csdn.net/python_tty/article/details/52884314)","source":"_posts/Centos7上搭建Jenkins.md","raw":"---\ntitle:  Centos7上搭建Jenkins\ndate: 2018年06月21日 22时15分52秒\ntags:  [Jenkins]\ncategories: 安装部署\ntoc: true\n---\n\n\n之前用yum模式安装，总是启动报错，解决了一番，未找到解决方案，后直接下载war包进行安装部署\n\n默认安装了Java\n<!-- more -->\n\n# 1. 安装 jenkins\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=480580003&auto=1&height=66\"></iframe>\n\n``` bash\ncd /opt\nmkdir /jenkins\ncd jenkins\nmkdir jenkins_home\nmkdir jenkins_node\nwget http://mirrors.jenkins-ci.org/war/latest/jenkins.war\n```\n\n\n# 2. 编写可执行文件\n\n  ` vim start_jenkins.sh`\n```bash\n     #!/bin/bash\n     JENKINS_ROOT=/opt/jenkins\n     export JENKINS_HOME=$JENKINS_ROOT/jenkins_home\n     java -jar $JENKINS_ROOT/jenkins.war --httpPort=8000\n```\n   修改文件的权限： ` chmod  a+x   start_jenkins.sh`\n\n   启动 jenkins:  `   nohup ./start_jenkins.sh > jenkins.log 2>& 1& `               \n# 3 访问 jenkins\n   输入 http:// 服务器地址: 8000\n\n注意：在启动日志中会出现初始密码，这个用来首次登陆Jenkins使用\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fu5j2mlpouj31d60lwtfs.jpg)\n\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fu5j4it7u3j31600nawfp.jpg)\n\n参考\n[在 Centos7 上搭建 jenkins](https://blog.csdn.net/python_tty/article/details/52884314)","slug":"Centos7上搭建Jenkins","published":1,"updated":"2018-08-11T02:00:14.159Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43v800052tpbj3jeh386"},{"title":"CoudearManager搭建","date":"2018-07-10T14:10:04.881Z","toc":true,"_content":"\n[TOC]\n\n\n\n# 创建基础容器\n\n```bash\ndocker run -itd  --net=br  --name cm --hostname cm yaosong5/centosbase:1.0 &> /dev/null\n```\n\n\n\n# 将下载的包进行解压然后进行拷贝\n\n```bash\ndocker cp /Users/yaosong/Yao/cloudera-manager-el6-cm5.9.0_x86_64/cloudera cm:/opt/\ndocker cp /Users/yaosong/Yao/cloudera-manager-el6-cm5.9.0_x86_64/cm-5.9.0 cm:/opt/\ndocker cp /Users/yaosong/Yao/mysql-connector-java-5.1.40-bin.jar cm:/opt/cm-5.9.0/share/cmf/lib/\ndocker cp /Users/yaosong/Yao/mysql-connector-java.jar cm:/usr/share/java/\ndocker cp /Users/yaosong/Yao/jdk1.8 cm:/usr/local/\n```\n\n<!--more -->\n\n# 将 parcel 文件放至 /opt/cloudera/parcel-repo\n\n```bash\ndocker cp /Users/yaosong/Yao/CDH-5.9.0-1.cdh5.9.0.p0.23-el6.parcel  cm:/opt/cloudera/parcel-repo\ndocker cp /Users/yaosong/Yao/CDH-5.9.0-1.cdh5.9.0.p0.23-el6.parcel.sha cm:/opt/cloudera/parcel-repo\ndocker cp /Users/yaosong/Yao/manifest.json cm:/opt/cloudera/parcel-repo\n```\n\n\n`vim /etc/profile`\n\n```bash\nexport JAVA_HOME=/usr/local/jdk1.8\nexport PATH=$JAVA_HOME/bin:$PATH\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\n```\n\n# 初始化 mysql 库\n\n```Bash\n /opt/cm-5.9.0/share/cmf/schema/scm_prepare_database.sh mysql cm -hlocalhost -uroot -proot --scm-host localhost scm scm scm\n```\n\n# 创建用户（所有节点执行）\n\n```\nuseradd --system --home=/opt/cm-5.9.0/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment \"Cloudera SCM User\" cloudera-scm\n```\n\n# Agent 配置 \n\n`vim /opt/cm-5.9.0/etc/cloudera-scm-agent/config.ini`\n\n 将 server_host 改为主节点主机名\n\n```\nserver_host=cm1\n```\n\n\n安装mysql\n`chkconfig mysqld on`\n\n设置允许远程登录\n\n```Bash\nmysql -u root -p \nGRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION; \n```\n\n\n\n# 创建CM用的数据库\n\n安装集群时按需创建，详见第七章第13步\n\n```Sql\n--hive数据库\ncreate database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n--oozie数据库\ncreate database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n--hue数据库\ncreate database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n\ncreate database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n\ncreate database monitor DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n```\n\n\n\n# Cloudera推荐设置\n\n在试安装的过程，发现Cloudera给出了一些警告\n\n身为一个有洁癖的码农，自然是连黄色的感叹号都要消灭的。因此在安装CM/CDH之前就先全部设置好。\n\n## 1、设置swap空间\n\n`vim /etc/sysctl.conf`\n末尾加上\n`vm.swappiness=10`\n\n## 2、关闭大页面压缩\n\n试过只设置defrag，但貌似个别节点还是会有警告，干脆全部设置\n\n`vim /etc/rc.local`\n末尾加上(永久生效)\n`echo never > /sys/kernel/mm/transparent_hugepage/enabled`\n`echo never > /sys/kernel/mm/transparent_hugepage/defrag`\n\n\n\n\n\n# 启动cloudera manager 服务\n\n/opt/cm-5.9.0/etc/init.d/cloudera-scm-server start\n\n/opt/cm-5.9.0/etc/init.d/cloudera-scm-agent start\n\n端口 7180\n\n\n\n# 保存为镜像\n\n\tdocker commit -m \"cloudera manger image\"  cm   yaosong5/cm59:1.0\n## 创建容器\n\n\tdocker run -itd  --net=br  --name cm1 --hostname cm1 yaosong5/cm59:1.0 &> /dev/null\n\tdocker run -itd  --net=br  --name cm2 --hostname cm2 yaosong5/cm59:1.0 &> /dev/null\n\tdocker run -itd  --net=br  --name cm3 --hostname cm3 yaosong5/cm59:1.0 &> /dev/null\n\n\n\n```\ndocker stop cm1\ndocker stop cm2\ndocker stop cm3\n\ndocker rm cm1\ndocker rm cm2\ndocker rm cm3\n```\n\n\n\n\n\n# 调错\n\n> .SearchRepositoryManager: No read permission to the server storage directory [/var/lib/cloudera-scm-server]\n> 2018-07-11 10:20:39,788 ERROR SearchRepositoryManager-0:com.cloudera.server.web.cmf.search.components.SearchRepositoryManager: No write permission to the server storage directory [/var/lib/cloudera-scm-server]\n>\n\n## 链接hue连接不上\n\n节点的 cm-5.x.0/log/cloudera-scm-server/cloudera-scm-server.log，一般情况下应该会说到\n\nImportError:libxslt.so.1:cannot open shared object file:No such file ordirectory\n\n```\nyum -y install libxml2-python \n```\n\n## 提示hue测试连接连接不上，安装依赖：\n\n```bash\nyum install libxml2-python  mod_ssl install krb5-devel cyrus-sasl-gssapi cyrus-sasl-deve libxml2-devel libxslt-devel mysql mysql-devel openldap-devel python-devel python-simplejson sqlite-devel -y\n```\n\n","source":"_posts/CoudearManager搭建.md","raw":"---\ntitle:  CoudearManager搭建\ndate: 2018年08月11日 02时55分44秒\ntags:  [Docker,CDH]\ncategories: 安装部署\ntoc: true\n---\n\n[TOC]\n\n\n\n# 创建基础容器\n\n```bash\ndocker run -itd  --net=br  --name cm --hostname cm yaosong5/centosbase:1.0 &> /dev/null\n```\n\n\n\n# 将下载的包进行解压然后进行拷贝\n\n```bash\ndocker cp /Users/yaosong/Yao/cloudera-manager-el6-cm5.9.0_x86_64/cloudera cm:/opt/\ndocker cp /Users/yaosong/Yao/cloudera-manager-el6-cm5.9.0_x86_64/cm-5.9.0 cm:/opt/\ndocker cp /Users/yaosong/Yao/mysql-connector-java-5.1.40-bin.jar cm:/opt/cm-5.9.0/share/cmf/lib/\ndocker cp /Users/yaosong/Yao/mysql-connector-java.jar cm:/usr/share/java/\ndocker cp /Users/yaosong/Yao/jdk1.8 cm:/usr/local/\n```\n\n<!--more -->\n\n# 将 parcel 文件放至 /opt/cloudera/parcel-repo\n\n```bash\ndocker cp /Users/yaosong/Yao/CDH-5.9.0-1.cdh5.9.0.p0.23-el6.parcel  cm:/opt/cloudera/parcel-repo\ndocker cp /Users/yaosong/Yao/CDH-5.9.0-1.cdh5.9.0.p0.23-el6.parcel.sha cm:/opt/cloudera/parcel-repo\ndocker cp /Users/yaosong/Yao/manifest.json cm:/opt/cloudera/parcel-repo\n```\n\n\n`vim /etc/profile`\n\n```bash\nexport JAVA_HOME=/usr/local/jdk1.8\nexport PATH=$JAVA_HOME/bin:$PATH\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\n```\n\n# 初始化 mysql 库\n\n```Bash\n /opt/cm-5.9.0/share/cmf/schema/scm_prepare_database.sh mysql cm -hlocalhost -uroot -proot --scm-host localhost scm scm scm\n```\n\n# 创建用户（所有节点执行）\n\n```\nuseradd --system --home=/opt/cm-5.9.0/run/cloudera-scm-server/ --no-create-home --shell=/bin/false --comment \"Cloudera SCM User\" cloudera-scm\n```\n\n# Agent 配置 \n\n`vim /opt/cm-5.9.0/etc/cloudera-scm-agent/config.ini`\n\n 将 server_host 改为主节点主机名\n\n```\nserver_host=cm1\n```\n\n\n安装mysql\n`chkconfig mysqld on`\n\n设置允许远程登录\n\n```Bash\nmysql -u root -p \nGRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION; \n```\n\n\n\n# 创建CM用的数据库\n\n安装集群时按需创建，详见第七章第13步\n\n```Sql\n--hive数据库\ncreate database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n--oozie数据库\ncreate database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n--hue数据库\ncreate database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n\ncreate database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n\ncreate database monitor DEFAULT CHARSET utf8 COLLATE utf8_general_ci;\n```\n\n\n\n# Cloudera推荐设置\n\n在试安装的过程，发现Cloudera给出了一些警告\n\n身为一个有洁癖的码农，自然是连黄色的感叹号都要消灭的。因此在安装CM/CDH之前就先全部设置好。\n\n## 1、设置swap空间\n\n`vim /etc/sysctl.conf`\n末尾加上\n`vm.swappiness=10`\n\n## 2、关闭大页面压缩\n\n试过只设置defrag，但貌似个别节点还是会有警告，干脆全部设置\n\n`vim /etc/rc.local`\n末尾加上(永久生效)\n`echo never > /sys/kernel/mm/transparent_hugepage/enabled`\n`echo never > /sys/kernel/mm/transparent_hugepage/defrag`\n\n\n\n\n\n# 启动cloudera manager 服务\n\n/opt/cm-5.9.0/etc/init.d/cloudera-scm-server start\n\n/opt/cm-5.9.0/etc/init.d/cloudera-scm-agent start\n\n端口 7180\n\n\n\n# 保存为镜像\n\n\tdocker commit -m \"cloudera manger image\"  cm   yaosong5/cm59:1.0\n## 创建容器\n\n\tdocker run -itd  --net=br  --name cm1 --hostname cm1 yaosong5/cm59:1.0 &> /dev/null\n\tdocker run -itd  --net=br  --name cm2 --hostname cm2 yaosong5/cm59:1.0 &> /dev/null\n\tdocker run -itd  --net=br  --name cm3 --hostname cm3 yaosong5/cm59:1.0 &> /dev/null\n\n\n\n```\ndocker stop cm1\ndocker stop cm2\ndocker stop cm3\n\ndocker rm cm1\ndocker rm cm2\ndocker rm cm3\n```\n\n\n\n\n\n# 调错\n\n> .SearchRepositoryManager: No read permission to the server storage directory [/var/lib/cloudera-scm-server]\n> 2018-07-11 10:20:39,788 ERROR SearchRepositoryManager-0:com.cloudera.server.web.cmf.search.components.SearchRepositoryManager: No write permission to the server storage directory [/var/lib/cloudera-scm-server]\n>\n\n## 链接hue连接不上\n\n节点的 cm-5.x.0/log/cloudera-scm-server/cloudera-scm-server.log，一般情况下应该会说到\n\nImportError:libxslt.so.1:cannot open shared object file:No such file ordirectory\n\n```\nyum -y install libxml2-python \n```\n\n## 提示hue测试连接连接不上，安装依赖：\n\n```bash\nyum install libxml2-python  mod_ssl install krb5-devel cyrus-sasl-gssapi cyrus-sasl-deve libxml2-devel libxslt-devel mysql mysql-devel openldap-devel python-devel python-simplejson sqlite-devel -y\n```\n\n","slug":"CoudearManager搭建","published":1,"updated":"2018-08-10T19:05:49.436Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vc00092tpbqbncqomu"},{"title":"DataStream DataSet简介","date":"2018-09-04T12:03:16.710Z","toc":true,"_content":"\n[TOC]\n# 什么是DStream\nDiscretized Stream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，DStream是一系列连续的RDD来表示。每个RDD含有一段时间间隔内的数据，如下图：\n![](http://pebgsxjpj.bkt.clouddn.com/15360627803806.jpg)\n\n对数据的操作也是按照RDD为单位来进行的\n![](http://pebgsxjpj.bkt.clouddn.com/15360627860827.jpg)\n\n\n计算过程由Spark engine来完成\n![](http://pebgsxjpj.bkt.clouddn.com/15360627909583.jpg)\n\n\nDatasets 与DataFrames 与RDDs的关系\n![](http://pebgsxjpj.bkt.clouddn.com/15360698161149.jpg)\n\nSpark引入DataFrame，它可以提供high-level functions让Spark更好的处理结构数据的计算。这让Catalyst optimizer 和Tungsten（钨丝） execution engine自动加速大数据分析。\n发布DataFrame之后开发者收到了很多反馈，其中一个主要的是大家反映缺乏编译时类型安全。为了解决这个问题，Spark采用新的Dataset API (DataFrame API的类型扩展)。\nDataset API扩展DataFrame API支持静态类型和运行已经存在的Scala或Java语言的用户自定义函数。对比传统的RDD API，Dataset API提供更好的内存管理，特别是在长任务中有更好的性能提升\n\n# DStream相关操作\n\nDStream上的原语与RDD的类似，分为Transformations（转换）和OutputOperations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。\n\n## Transformations on DStreams\n\n| **Transformation**               | **Meaning**                              |\n| -------------------------------- | ---------------------------------------- |\n| map(func)                        | Return a new DStream by passing each  element of the source DStream through a function func. |\n| flatMap(func)                    | Similar to map, but each input item can  be mapped to 0 or more output items. |\n| filter(func)                     | Return a new DStream by selecting only  the records of the source DStream on which func returns true. |\n| repartition(numPartitions)       | Changes the level of parallelism in this  DStream by creating more or fewer partitions. |\n| union(otherStream)               | Return a new DStream that contains the  union of the elements in the source DStream and otherDStream. |\n| count()                          | Return a new DStream of single-element  RDDs by counting the number of elements in each RDD of the source DStream. |\n| reduce(func)                     | Return a new DStream of single-element  RDDs by aggregating the elements in each RDD of the source DStream using a  function func (which takes two arguments and returns one). The function  should be associative so that it can be computed in parallel. |\n| countByValue()                   | When called on a DStream of elements of  type K, return a new DStream of (K, Long) pairs where the value of each key  is its frequency in each RDD of the source DStream. |\n| reduceByKey(func, [numTasks])    | When called on a DStream of (K, V) pairs,  return a new DStream of (K, V) pairs where the values for each key are  aggregated using the given reduce function. Note: By default, this uses Spark's  default number of parallel tasks (2 for local mode, and in cluster mode the  number is determined by the config property spark.default.parallelism) to do  the grouping. You can pass an optional numTasks argument to set a different  number of tasks. |\n| join(otherStream, [numTasks])    | When called on two DStreams of (K, V) and  (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of  elements for each key. |\n| cogroup(otherStream, [numTasks]) | When called on a DStream of (K, V) and  (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples. |\n| transform(func)                  | Return a new DStream by applying a  RDD-to-RDD function to every RDD of the source DStream. This can be used to  do arbitrary RDD operations on the DStream. |\n| updateStateByKey(func)           | Return a new \"state\" DStream  where the state for each key is updated by applying the given function on the  previous state of the key and the new values for the key. This can be used to  maintain arbitrary state data for each key. |\n\n \n\n### 特殊的Transformations\n\n** **\n\n1. UpdateStateByKeyOperation\n\nUpdateStateByKey原语用于记录历史记录，上文中Word Count示例中就用到了该特性。若不用UpdateStateByKey来更新状态，那么每次数据进来后分析完成后，结果输出后将不在保存\n\n2. TransformOperation\n\nTransform原语允许DStream上执行任意的RDD-to-RDD函数。通过该函数可以方便的扩展Spark API。此外，MLlib（机器学习）以及Graphx也是通过本函数来进行结合的。\n\n\n\n3. WindowOperations\n\nWindow Operations有点类似于Storm中的State，可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态\n![](http://pebgsxjpj.bkt.clouddn.com/15360679610176.jpg)\n\n\n## Output Operations on DStreams\n\nOutput Operations可以将DStream的数据输出到外部的数据库或文件系统，当某个Output Operations原语被调用时（与RDD的Action相同），streaming程序才会开始真正的计算过程。\n\n| Output Operation                    | Meaning                                  |\n| ----------------------------------- | ---------------------------------------- |\n| print()                             | Prints the first ten elements of every  batch of data in a DStream on the driver node running the streaming  application. This is useful for development and debugging. |\n| saveAsTextFiles(prefix, [suffix])   | Save this DStream's contents as text  files. The file name at each batch interval is generated based on prefix and  suffix: \"prefix-TIME_IN_MS[.suffix]\". |\n| saveAsObjectFiles(prefix, [suffix]) | Save this DStream's contents as  SequenceFiles of serialized Java objects. The file name at each batch  interval is generated based on prefix and suffix:  \"prefix-TIME_IN_MS[.suffix]\". |\n| saveAsHadoopFiles(prefix, [suffix]) | Save this DStream's contents as Hadoop  files. The file name at each batch interval is generated based on prefix and  suffix: \"prefix-TIME_IN_MS[.suffix]\". |\n| foreachRDD(func)                    | The most generic output operator that  applies a function, func, to each RDD generated from the stream. This  function should push the data in each RDD to an external system, such as  saving the RDD to files, or writing it over the network to a database. Note  that the function func is executed in the driver process running the  streaming application, and will usually have RDD actions in it that will  force the computation of the streaming RDDs. |\n\n##  用Spark Streaming实现实时WordCount\n\n架构图：\n\n![](http://pebgsxjpj.bkt.clouddn.com/15360681467801.jpg)\n\n\n1.安装并启动生成者\n\n首先在一台Linux（ip：192.168.10.101）上用YUM安装nc工具\n\nyum install -y nc\n\n \n\n启动一个服务端并监听9999端口\n\nnc -lk 9999\n\n \n\n2.编写Spark Streaming程序\n\n```scala\n\npackage me.yao.spark.streaming\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nobject NetworkWordCount {\n    def main(args: Array[String]) {         //设置日志级别\n        LoggerLevel.setStreamingLogLevels()     //创建SparkConf并设置为本地模式运行     //注意local[2]代表开两个线程\n    val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\")     //设置DStream批次时间间隔为2秒\n    val ssc = new StreamingContext(conf, Seconds(2))     //通过网络读取数据\n    val lines = ssc.socketTextStream(\"192.168.10.101\", 9999)     //将读到的数据用空格切成单词\n    val words = lines.flatMap(_.split(\" \"))     //将单词和1组成一个pair\n    val pairs = words.map(word => (word, 1))     //按单词进行分组求相同单词出现的次数\n    val wordCounts = pairs.reduceByKey(_ + _)     //打印结果到控制台\n    wordCounts.print()     //开始计算\n    ssc.start()     //等待停止\n    ssc.awaitTermination()\n        }\n    }\n```\n\n问题：结[果每次在]()Linux段输入的单词次数都被正确的统计出来，但是结果不能累加！如果需要累加需要使用updateStateByKey(func)来更新状态，下面给出一个例子：\n\n\n\n```scala\n  package me.yao.spark.streaming\n\n  import org.apache.spark.{HashPartitioner, SparkConf}\n  import org.apache.spark.streaming.{StreamingContext, Seconds}\n\n  object NetworkUpdateStateWordCount {\n  val updateFunc = (iter: Iterator[(String, Seq[Int], Option[Int])]) => {\n          //iter.flatMap(it=>Some(it._2.sum + it._3.getOrElse(0)).map(x=>(it._1,x)))\n            iter.flatMap{\n             case(x,y,z)=>Some(y.sum + z.getOrElse(0)).map(m=>(x, m))}\n           }\n\n  def main(args: Array[String]) {\n    LoggerLevel.setStreamingLogLevels*()\n    val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkUpdateStateWordCount\")\n    val ssc = new StreamingContext(conf, Seconds(5))\n    //做checkpoint 写入共享存储中\n    ssc.checkpoint(\"c://aaa\")\n    **val **lines = ssc.socketTextStream(\"192.168.10.100\", 9999)\n    //reduceByKey **结果不累加\n    //val result = lines.flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_+_)\n    //updateStateByKey结果可以累加但是需要传入一个自定义的累加函数：updateFunc\n   val results = lines.flatMap(_.split(\" \")).map((_,1)).updateStateByKey(updateFunc, new HashPartitioner(ssc.sparkContext.defaultParallelism), true)\n    results.print()\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n```\n\n\n# Dataset\n比RDD执行速度快很多倍，占用的内存更小，是从dataFrame发展而来，包含dataFrame \ndataFrame是处理结构化数据，有表头，有类型，\n\ndataSet从1.6.0开始出现，2.0做了重大改进，对dataFrame进行了整合 \ndataFrame在1.4系列出现的，现在很多公司都是用的RDD\n\n在spark的命令行里面： \n将dataFrame转成dataSet \nval ds = df.as[person] \n调用dataSet的方法 \nds.map \nds.show \nval ds = sqlContext.read.text(\"hdfs://bigdata1:9000/wc/).as[String] \nval res5 = ds.flatmap(.split(\" \")).map((,1)) \nflatmap将文本里面的每一行进行切分， \nrest.reduceByKey();会发现dataSet里面没有这个方法，在dataSet里面应该调用更高级的做法 \nds.flatmap(_.split(\" \")).groupBy($\"\"value).count.show 或者collect\n\n！在import里面打开idea查看类里面有哪些方法。 \n在spark1.6里面sqlContext.read....读取的就是dataFrame，和dataSet还未统一，需要将dataFrame用as转为dataSet\n\n\n\n\n\n\n","source":"_posts/DataStream简介.md","raw":"---\ntitle: DataStream DataSet简介\ndate: 2018年09月04日\ntags: [Spark]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n# 什么是DStream\nDiscretized Stream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，DStream是一系列连续的RDD来表示。每个RDD含有一段时间间隔内的数据，如下图：\n![](http://pebgsxjpj.bkt.clouddn.com/15360627803806.jpg)\n\n对数据的操作也是按照RDD为单位来进行的\n![](http://pebgsxjpj.bkt.clouddn.com/15360627860827.jpg)\n\n\n计算过程由Spark engine来完成\n![](http://pebgsxjpj.bkt.clouddn.com/15360627909583.jpg)\n\n\nDatasets 与DataFrames 与RDDs的关系\n![](http://pebgsxjpj.bkt.clouddn.com/15360698161149.jpg)\n\nSpark引入DataFrame，它可以提供high-level functions让Spark更好的处理结构数据的计算。这让Catalyst optimizer 和Tungsten（钨丝） execution engine自动加速大数据分析。\n发布DataFrame之后开发者收到了很多反馈，其中一个主要的是大家反映缺乏编译时类型安全。为了解决这个问题，Spark采用新的Dataset API (DataFrame API的类型扩展)。\nDataset API扩展DataFrame API支持静态类型和运行已经存在的Scala或Java语言的用户自定义函数。对比传统的RDD API，Dataset API提供更好的内存管理，特别是在长任务中有更好的性能提升\n\n# DStream相关操作\n\nDStream上的原语与RDD的类似，分为Transformations（转换）和OutputOperations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。\n\n## Transformations on DStreams\n\n| **Transformation**               | **Meaning**                              |\n| -------------------------------- | ---------------------------------------- |\n| map(func)                        | Return a new DStream by passing each  element of the source DStream through a function func. |\n| flatMap(func)                    | Similar to map, but each input item can  be mapped to 0 or more output items. |\n| filter(func)                     | Return a new DStream by selecting only  the records of the source DStream on which func returns true. |\n| repartition(numPartitions)       | Changes the level of parallelism in this  DStream by creating more or fewer partitions. |\n| union(otherStream)               | Return a new DStream that contains the  union of the elements in the source DStream and otherDStream. |\n| count()                          | Return a new DStream of single-element  RDDs by counting the number of elements in each RDD of the source DStream. |\n| reduce(func)                     | Return a new DStream of single-element  RDDs by aggregating the elements in each RDD of the source DStream using a  function func (which takes two arguments and returns one). The function  should be associative so that it can be computed in parallel. |\n| countByValue()                   | When called on a DStream of elements of  type K, return a new DStream of (K, Long) pairs where the value of each key  is its frequency in each RDD of the source DStream. |\n| reduceByKey(func, [numTasks])    | When called on a DStream of (K, V) pairs,  return a new DStream of (K, V) pairs where the values for each key are  aggregated using the given reduce function. Note: By default, this uses Spark's  default number of parallel tasks (2 for local mode, and in cluster mode the  number is determined by the config property spark.default.parallelism) to do  the grouping. You can pass an optional numTasks argument to set a different  number of tasks. |\n| join(otherStream, [numTasks])    | When called on two DStreams of (K, V) and  (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of  elements for each key. |\n| cogroup(otherStream, [numTasks]) | When called on a DStream of (K, V) and  (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples. |\n| transform(func)                  | Return a new DStream by applying a  RDD-to-RDD function to every RDD of the source DStream. This can be used to  do arbitrary RDD operations on the DStream. |\n| updateStateByKey(func)           | Return a new \"state\" DStream  where the state for each key is updated by applying the given function on the  previous state of the key and the new values for the key. This can be used to  maintain arbitrary state data for each key. |\n\n \n\n### 特殊的Transformations\n\n** **\n\n1. UpdateStateByKeyOperation\n\nUpdateStateByKey原语用于记录历史记录，上文中Word Count示例中就用到了该特性。若不用UpdateStateByKey来更新状态，那么每次数据进来后分析完成后，结果输出后将不在保存\n\n2. TransformOperation\n\nTransform原语允许DStream上执行任意的RDD-to-RDD函数。通过该函数可以方便的扩展Spark API。此外，MLlib（机器学习）以及Graphx也是通过本函数来进行结合的。\n\n\n\n3. WindowOperations\n\nWindow Operations有点类似于Storm中的State，可以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态\n![](http://pebgsxjpj.bkt.clouddn.com/15360679610176.jpg)\n\n\n## Output Operations on DStreams\n\nOutput Operations可以将DStream的数据输出到外部的数据库或文件系统，当某个Output Operations原语被调用时（与RDD的Action相同），streaming程序才会开始真正的计算过程。\n\n| Output Operation                    | Meaning                                  |\n| ----------------------------------- | ---------------------------------------- |\n| print()                             | Prints the first ten elements of every  batch of data in a DStream on the driver node running the streaming  application. This is useful for development and debugging. |\n| saveAsTextFiles(prefix, [suffix])   | Save this DStream's contents as text  files. The file name at each batch interval is generated based on prefix and  suffix: \"prefix-TIME_IN_MS[.suffix]\". |\n| saveAsObjectFiles(prefix, [suffix]) | Save this DStream's contents as  SequenceFiles of serialized Java objects. The file name at each batch  interval is generated based on prefix and suffix:  \"prefix-TIME_IN_MS[.suffix]\". |\n| saveAsHadoopFiles(prefix, [suffix]) | Save this DStream's contents as Hadoop  files. The file name at each batch interval is generated based on prefix and  suffix: \"prefix-TIME_IN_MS[.suffix]\". |\n| foreachRDD(func)                    | The most generic output operator that  applies a function, func, to each RDD generated from the stream. This  function should push the data in each RDD to an external system, such as  saving the RDD to files, or writing it over the network to a database. Note  that the function func is executed in the driver process running the  streaming application, and will usually have RDD actions in it that will  force the computation of the streaming RDDs. |\n\n##  用Spark Streaming实现实时WordCount\n\n架构图：\n\n![](http://pebgsxjpj.bkt.clouddn.com/15360681467801.jpg)\n\n\n1.安装并启动生成者\n\n首先在一台Linux（ip：192.168.10.101）上用YUM安装nc工具\n\nyum install -y nc\n\n \n\n启动一个服务端并监听9999端口\n\nnc -lk 9999\n\n \n\n2.编写Spark Streaming程序\n\n```scala\n\npackage me.yao.spark.streaming\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nobject NetworkWordCount {\n    def main(args: Array[String]) {         //设置日志级别\n        LoggerLevel.setStreamingLogLevels()     //创建SparkConf并设置为本地模式运行     //注意local[2]代表开两个线程\n    val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\")     //设置DStream批次时间间隔为2秒\n    val ssc = new StreamingContext(conf, Seconds(2))     //通过网络读取数据\n    val lines = ssc.socketTextStream(\"192.168.10.101\", 9999)     //将读到的数据用空格切成单词\n    val words = lines.flatMap(_.split(\" \"))     //将单词和1组成一个pair\n    val pairs = words.map(word => (word, 1))     //按单词进行分组求相同单词出现的次数\n    val wordCounts = pairs.reduceByKey(_ + _)     //打印结果到控制台\n    wordCounts.print()     //开始计算\n    ssc.start()     //等待停止\n    ssc.awaitTermination()\n        }\n    }\n```\n\n问题：结[果每次在]()Linux段输入的单词次数都被正确的统计出来，但是结果不能累加！如果需要累加需要使用updateStateByKey(func)来更新状态，下面给出一个例子：\n\n\n\n```scala\n  package me.yao.spark.streaming\n\n  import org.apache.spark.{HashPartitioner, SparkConf}\n  import org.apache.spark.streaming.{StreamingContext, Seconds}\n\n  object NetworkUpdateStateWordCount {\n  val updateFunc = (iter: Iterator[(String, Seq[Int], Option[Int])]) => {\n          //iter.flatMap(it=>Some(it._2.sum + it._3.getOrElse(0)).map(x=>(it._1,x)))\n            iter.flatMap{\n             case(x,y,z)=>Some(y.sum + z.getOrElse(0)).map(m=>(x, m))}\n           }\n\n  def main(args: Array[String]) {\n    LoggerLevel.setStreamingLogLevels*()\n    val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkUpdateStateWordCount\")\n    val ssc = new StreamingContext(conf, Seconds(5))\n    //做checkpoint 写入共享存储中\n    ssc.checkpoint(\"c://aaa\")\n    **val **lines = ssc.socketTextStream(\"192.168.10.100\", 9999)\n    //reduceByKey **结果不累加\n    //val result = lines.flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_+_)\n    //updateStateByKey结果可以累加但是需要传入一个自定义的累加函数：updateFunc\n   val results = lines.flatMap(_.split(\" \")).map((_,1)).updateStateByKey(updateFunc, new HashPartitioner(ssc.sparkContext.defaultParallelism), true)\n    results.print()\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n```\n\n\n# Dataset\n比RDD执行速度快很多倍，占用的内存更小，是从dataFrame发展而来，包含dataFrame \ndataFrame是处理结构化数据，有表头，有类型，\n\ndataSet从1.6.0开始出现，2.0做了重大改进，对dataFrame进行了整合 \ndataFrame在1.4系列出现的，现在很多公司都是用的RDD\n\n在spark的命令行里面： \n将dataFrame转成dataSet \nval ds = df.as[person] \n调用dataSet的方法 \nds.map \nds.show \nval ds = sqlContext.read.text(\"hdfs://bigdata1:9000/wc/).as[String] \nval res5 = ds.flatmap(.split(\" \")).map((,1)) \nflatmap将文本里面的每一行进行切分， \nrest.reduceByKey();会发现dataSet里面没有这个方法，在dataSet里面应该调用更高级的做法 \nds.flatmap(_.split(\" \")).groupBy($\"\"value).count.show 或者collect\n\n！在import里面打开idea查看类里面有哪些方法。 \n在spark1.6里面sqlContext.read....读取的就是dataFrame，和dataSet还未统一，需要将dataFrame用as转为dataSet\n\n\n\n\n\n\n","slug":"DataStream简介","published":1,"updated":"2018-09-05T03:01:40.805Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vd000a2tpbh5flt34n"},{"title":"Docker-machine的创建，mac宿主机和docker容器网络互通Docker容器与宿主机在同一ip段下","date":"2018-07-19T19:02:08.124Z","toc":true,"_content":"\n\n\n\n此文纯属命令记录，后续更新原理解说\n\n # 更改virtual0的ip\n VBoxManage hostonlyif ipconfig vboxnet0 --ip 192.168.33.253 --netmask 255.255.255.0\n\n<!-- more -->\n\n# ifconfig 查看\n创建虚拟机配置文件  Vagrantfile\n\n也可以vagrant init  会生成一个空白的Vagrantfile\n# vi Vagrantfile\n\n```SHELL\n\tVagrant.configure(2) do |config|\n\t\t config.vm.box = \"dolbager/centos-7-docker\"\n\t\t config.vm.hostname = \"default\"\n\t\t  config.vm.network \"private_network\", ip: \"192.168.33.1\",netmask: \"255.255.255.0\"\n\t\t config.vm.provider \"virtualbox\" do |v|\n\t\t   v.name = \"default\"\n\t\t   v.memory = \"2048\"\n\t\t   # Change the network adapter type and promiscuous mode\n\t\t   v.customize ['modifyvm', :id, '--nictype1', 'Am79C973']\n\t\t   v.customize ['modifyvm', :id, '--nicpromisc1', 'allow-all']\n\t\t   v.customize ['modifyvm', :id, '--nictype2', 'Am79C973']\n\t\t   v.customize ['modifyvm', :id, '--nicpromisc2', 'allow-all']\n\t\t end\n\t\t # Install bridge-utils\n\t\t config.vm.provision \"shell\", inline: <<-SHELL\n\t\t    curl -o /etc/yum.repos.d/CentOS-Base.repohttp://mirrors.aliyun.com/repo/Centos-7.repo\n\t\t    curl -o /etc/yum.repos.d/epel.repohttp://mirrors.aliyun.com/repo/epel-7.repo\n\t\t   yum clean all\n\t\t   yum makecache\n\t\t   yum update -y\n\t\t   yum install bridge-utils net-tools -y\n\t\t SHELL\n\tend\n\n```\nvagrant up\nvagrant ssh\n\nvagrant ssh-config\n\n```bash\nscp ~/.vagrant.d/boxes/dolbager-VAGRANTSLASH-centos-7-docker/0.2/virtualbox/vagrant_private_key .vagrant/machines/default/virtualbox/private_key\n\n```\n\n`vagrant exit`\n\n\n\n\n\n\n```bash\ndocker-machine create \\\n --driver \"generic\" \\\n --generic-ip-address 192.168.33.1 \\\n --generic-ssh-user vagrant \\\n --generic-ssh-key .vagrant/machines/default/virtualbox/private_key \\\n --generic-ssh-port 22 \\\n default\n```\n\n创建网桥docker1 和 docker network br\n通过vagrant 从虚拟机的 eth0 登录到虚拟机\n\nvagrant ssh\nip -4 addr\n\n创建 docker network br\n\n\n```bash\nsudo docker network create \\\n    --driver bridge \\\n    --subnet=192.168.33.0/24 \\\n    --gateway=192.168.33.1 \\\n    --opt \"com.docker.network.bridge.enable_icc\"=\"true\" \\\n    --opt \"com.docker.network.bridge.enable_ip_masquerade\"=\"true\" \\\n    --opt \"com.docker.network.bridge.name\"=\"docker1\" \\\n    --opt \"com.docker.network.driver.mtu\"=\"1500\" \\\n    br\n\n```\n\n\n创建网桥配置文件docker1\n\n`vim /etc/sysconfig/network-scripts/ifcfg-docker1`\n\n```bash\nDEVICE=docker1\nTYPE=Bridge\nBOOTPROTO=static\nONBOOT=yes\nSTP=on\nIPADDR=\nNETMASK=\nGATEWAY=\nDNS1=\n```\n\n\n## 修改网卡配置 eth1 :\n\n`sudo vi /etc/sysconfig/network-scripts/ifcfg-eth1`\n\n```bash\nDEVICE=eth1\nBOOTPROTO=static\nHWADDR=\nONBOOT=yes\nNETMASK=\nGATEWAY=\nBRIDGE=docker1\nTYPE=Ethernet\n```\n\n\n初始化docker-machine变量\n\n提示报错Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? \n\n```bash\n eval $(docker-machine env default) \n```\n\n\n\n参考：[docker-install-mac-vm-centos](https://github.com/SixQuant/engineering-excellence/blob/master/docker/docker-install-mac-vm-centos.md)","source":"_posts/Docker-machine的搭建(与宿主机在同一ip段下).md","raw":"---\ntitle:  Docker-machine的创建，mac宿主机和docker容器网络互通Docker容器与宿主机在同一ip段下\ndate: 2018年07月20日 03时02分26秒\ntags:  [Docker,Docker-machine,安装部署]\ncategories: Docker\ntoc: true\n---\n\n\n\n\n此文纯属命令记录，后续更新原理解说\n\n # 更改virtual0的ip\n VBoxManage hostonlyif ipconfig vboxnet0 --ip 192.168.33.253 --netmask 255.255.255.0\n\n<!-- more -->\n\n# ifconfig 查看\n创建虚拟机配置文件  Vagrantfile\n\n也可以vagrant init  会生成一个空白的Vagrantfile\n# vi Vagrantfile\n\n```SHELL\n\tVagrant.configure(2) do |config|\n\t\t config.vm.box = \"dolbager/centos-7-docker\"\n\t\t config.vm.hostname = \"default\"\n\t\t  config.vm.network \"private_network\", ip: \"192.168.33.1\",netmask: \"255.255.255.0\"\n\t\t config.vm.provider \"virtualbox\" do |v|\n\t\t   v.name = \"default\"\n\t\t   v.memory = \"2048\"\n\t\t   # Change the network adapter type and promiscuous mode\n\t\t   v.customize ['modifyvm', :id, '--nictype1', 'Am79C973']\n\t\t   v.customize ['modifyvm', :id, '--nicpromisc1', 'allow-all']\n\t\t   v.customize ['modifyvm', :id, '--nictype2', 'Am79C973']\n\t\t   v.customize ['modifyvm', :id, '--nicpromisc2', 'allow-all']\n\t\t end\n\t\t # Install bridge-utils\n\t\t config.vm.provision \"shell\", inline: <<-SHELL\n\t\t    curl -o /etc/yum.repos.d/CentOS-Base.repohttp://mirrors.aliyun.com/repo/Centos-7.repo\n\t\t    curl -o /etc/yum.repos.d/epel.repohttp://mirrors.aliyun.com/repo/epel-7.repo\n\t\t   yum clean all\n\t\t   yum makecache\n\t\t   yum update -y\n\t\t   yum install bridge-utils net-tools -y\n\t\t SHELL\n\tend\n\n```\nvagrant up\nvagrant ssh\n\nvagrant ssh-config\n\n```bash\nscp ~/.vagrant.d/boxes/dolbager-VAGRANTSLASH-centos-7-docker/0.2/virtualbox/vagrant_private_key .vagrant/machines/default/virtualbox/private_key\n\n```\n\n`vagrant exit`\n\n\n\n\n\n\n```bash\ndocker-machine create \\\n --driver \"generic\" \\\n --generic-ip-address 192.168.33.1 \\\n --generic-ssh-user vagrant \\\n --generic-ssh-key .vagrant/machines/default/virtualbox/private_key \\\n --generic-ssh-port 22 \\\n default\n```\n\n创建网桥docker1 和 docker network br\n通过vagrant 从虚拟机的 eth0 登录到虚拟机\n\nvagrant ssh\nip -4 addr\n\n创建 docker network br\n\n\n```bash\nsudo docker network create \\\n    --driver bridge \\\n    --subnet=192.168.33.0/24 \\\n    --gateway=192.168.33.1 \\\n    --opt \"com.docker.network.bridge.enable_icc\"=\"true\" \\\n    --opt \"com.docker.network.bridge.enable_ip_masquerade\"=\"true\" \\\n    --opt \"com.docker.network.bridge.name\"=\"docker1\" \\\n    --opt \"com.docker.network.driver.mtu\"=\"1500\" \\\n    br\n\n```\n\n\n创建网桥配置文件docker1\n\n`vim /etc/sysconfig/network-scripts/ifcfg-docker1`\n\n```bash\nDEVICE=docker1\nTYPE=Bridge\nBOOTPROTO=static\nONBOOT=yes\nSTP=on\nIPADDR=\nNETMASK=\nGATEWAY=\nDNS1=\n```\n\n\n## 修改网卡配置 eth1 :\n\n`sudo vi /etc/sysconfig/network-scripts/ifcfg-eth1`\n\n```bash\nDEVICE=eth1\nBOOTPROTO=static\nHWADDR=\nONBOOT=yes\nNETMASK=\nGATEWAY=\nBRIDGE=docker1\nTYPE=Ethernet\n```\n\n\n初始化docker-machine变量\n\n提示报错Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? \n\n```bash\n eval $(docker-machine env default) \n```\n\n\n\n参考：[docker-install-mac-vm-centos](https://github.com/SixQuant/engineering-excellence/blob/master/docker/docker-install-mac-vm-centos.md)","slug":"Docker-machine的搭建(与宿主机在同一ip段下)","published":1,"updated":"2018-08-24T02:01:15.797Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43ve000b2tpbibqbm1nn"},{"title":"Docker常用命令汇集","date":"2018-06-26T02:33:37.714Z","toc":true,"_content":"\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu55c5h3g1j319i0eu0tg.jpg)\n\n[TOC]\n\n\n\n# Docker源配置\n\n  安装过程中需要重国外 docker 仓库下载文件，速度太慢，建议配置 docker 国内镜像仓库：\n  vi /etc/docker/daemon.json\n  {\"registry-mirrors\":[\"http://c1f0a193.m.daocloud.io\"] }\n\n启动容器\n\n\tdocker run -itd  --net=br  --name slave02 --hostname slave02 centos:hadoop-spark &> /dev/null\n\t如果以 /bin/bash启动的话，sshd服务不会启动(docker未知bug)\n## 创建容器\n\n\t--name    --hostname (同-h)  --net=    -d表示后台启动\n\t此命令不会打印出容器id\n\tdocker run -itd  --net=br  --name hm --hostname hadoop-master kiwenlau/hadoop:1.0 &> /dev/null   （hadoop镜像）\n\t设置静态固定ip\n\tdocker run -d --net=br --name=c6 --ip=192.168.33.6 nginx\n\n\n    自动分配Ip\n    docker run -d --net=br --name=c1 nginx\n    \n    设置docker默认ip段命令\n    docker run -itd   -P -p 50070:50070 -p 8088:8088 -p 8080:8080 --name master -h master --add-host slave01:172.17.0.3 --add-host slave02:172.17.0.4 centos:ssh-spark-hadoop\n\n## 容器挂载目录\n\ncompose文件：\n```Bash\nvolumes:\n\t- /Users/yaosong/Yao/dev/hadoop/dfs/name:/root/hadoop/dfs/name\n```\n\nshell命令：\n\n```bash\n-v : \ndocker run -it -v /test:/soft centos /bin/bash\n```\n\n\":\"前目录为宿主机目录，后目录为容器目录\n\n\n\n> 在virtualbox中设置共享文件夹的share名称对应mac的目录\n> 虚拟机中的目录\n> sudo mount -t vboxsf share  /Users/yaosong/Yao/share/\n> sudo mount -t vboxsf vagrant /Users/yaosong\n\n## 删除所有未用的 Data volumes\n\n```\ndocker volume prune\n```\n\n\n\n## run 命令解释\n\n```Bash\n-d 是后台启动\ndocker run -itd  --net=br  --name spark --hostname spark yaosong5/spark:2.1.0 &> /dev/null\nsudo docker exec -it spark bash（进入后台启动的容器）\n和下面一样（直接进入）\ndocker run -it --net=br  --name spark --hostname spark yaosong5/spark:2.1.0 bash\n```\n\n## exec 进入后台容器\n```Bash\ndocker exec -it spark bash\ndocker exec -it 容器名 bash\n执行命令 docker exec -it 容器名 ip addr 可以拿到 a0 容器的 ip\n```\n\n## logs查看容器启动日志\n\n```Bash\ndocker logs -f -t --tail 100  kanbigdata_namenode_1\n```\n\n\n\n\n\n## 查看容器信息\n\n```shell\ndocker inspect hm\n执行命令 docker exec -it 容器名 ip addr 可以拿到 a0 容器的 ip\n```\n\n\n\n## 启动 关闭 删除容器\n\n```shell\ndocker start \ndocker stop 容器名\ndocker rm 容器名\n```\n\n## cp容器宿主互拷文件\n\n```shell\ndocker cp /Users/yaosong/Yao/etc.tar  f7e795c0fddd:/\n后为容器id:/目录\n```\n\n## 删除镜像\n\n```shell\n（根据镜像id删除）\ndocker rmi 00de07ebadff\n用docker images -a 查看image id，\n也可docker rmi 镜像名:版本号\n```\n\n\n## 保存镜像\n```shell\ndocker commit -m \"centos-6.9 with spark 2.2.0 and hadoop 2.8.0\"  os   centos:hadoop-spark\n\ndocker commit -m \"bigdata:spark,hadoop,hive,mysql and shell  foundation\"   --author=\"yaosong\"  master   yao/os/bigdata:2.1\n```\n\n\n## Docker用DockerFile创建镜像\n\n```shell\ndocker build -t hadoop:v1- <Dockerfile\ndocker build -t=\"hadoop:v1\" .  （.表示是当前文件夹，也就是dockerfile所在文件夹）\ndocker build -f Dockerfile -t hadoop:v1 . 此命令也可\n```\n\n## 一键启动docker-compose.yml编排的所有服务\n\n```shell\ndocker-compose -f docker-compose.yml up d\n```\n\n## Docker改变标签\n\ndocker tag IMAGEID(镜像id) REPOSITORY:TAG（仓库：标签）\n\n`docker tag  b7a66cb0e8ba yaosong5/bigdata:1.0`\n\n## 搜索docker镜像\n\n```shell\ndocker search yaosong5\n```\n\n\n\n## 登录docker账户\n\n`docker login` 登录docker hub中注册的账户\n\n\n\n\n## 上传仓库\n\n`docker push yaosong5/elk:1.0`\n\n## 容器保存为镜像，加载本地镜像 引用\n\n```shell\ndocker save imageID > filename\ndocker load <filename\n如：\ndocker save 4f9e92e56941>  /Users/yaosong/centosSparkHadoop.tar\ndocker load </Users/yaosong/centosSparkHadoop.tar\n\n通过 image 保存的镜像会保存操作历史，可以回滚到历史版本。\n```\n\n## 保存，加载容器命令：\n```powershell\ndocker export containID > filename\ndocker import filename [newname]\n```\n通过容器保存的镜像不会保存操作历史，所以文件小一点。\n如果要运行通过容器加载的镜像， 需要在运行的时候加上相关命令。\n\n#  Docker-machine命令\n## 列出docker-machine\n\n```shell\ndocker-machine ls\n```\n\n## 开启虚拟机\n\n```shell\ndocker-machine start default\n```\n\n## 关闭虚拟机\n\n```shell\ndocker-machine stop default\n```\n\n## 重启虚拟机\n\n```bash\ndocker-machine restart default\n```\n\n## 删除虚拟机\n\n```bash\ndocker-machine rm default\n```\n\n## 设置环境变量docker-machine\n\n```bash\neval $(docker-machine env default) # Setup the environment\n```\n\n\n\n","source":"_posts/Docker命令汇集.md","raw":"---\ntitle:  Docker常用命令汇集\ndate: 2018年08月06日 22时15分52秒\ntags:  [Docker]\ncategories: Docker\ntoc: true\n\n---\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu55c5h3g1j319i0eu0tg.jpg)\n\n[TOC]\n\n\n\n# Docker源配置\n\n  安装过程中需要重国外 docker 仓库下载文件，速度太慢，建议配置 docker 国内镜像仓库：\n  vi /etc/docker/daemon.json\n  {\"registry-mirrors\":[\"http://c1f0a193.m.daocloud.io\"] }\n\n启动容器\n\n\tdocker run -itd  --net=br  --name slave02 --hostname slave02 centos:hadoop-spark &> /dev/null\n\t如果以 /bin/bash启动的话，sshd服务不会启动(docker未知bug)\n## 创建容器\n\n\t--name    --hostname (同-h)  --net=    -d表示后台启动\n\t此命令不会打印出容器id\n\tdocker run -itd  --net=br  --name hm --hostname hadoop-master kiwenlau/hadoop:1.0 &> /dev/null   （hadoop镜像）\n\t设置静态固定ip\n\tdocker run -d --net=br --name=c6 --ip=192.168.33.6 nginx\n\n\n    自动分配Ip\n    docker run -d --net=br --name=c1 nginx\n    \n    设置docker默认ip段命令\n    docker run -itd   -P -p 50070:50070 -p 8088:8088 -p 8080:8080 --name master -h master --add-host slave01:172.17.0.3 --add-host slave02:172.17.0.4 centos:ssh-spark-hadoop\n\n## 容器挂载目录\n\ncompose文件：\n```Bash\nvolumes:\n\t- /Users/yaosong/Yao/dev/hadoop/dfs/name:/root/hadoop/dfs/name\n```\n\nshell命令：\n\n```bash\n-v : \ndocker run -it -v /test:/soft centos /bin/bash\n```\n\n\":\"前目录为宿主机目录，后目录为容器目录\n\n\n\n> 在virtualbox中设置共享文件夹的share名称对应mac的目录\n> 虚拟机中的目录\n> sudo mount -t vboxsf share  /Users/yaosong/Yao/share/\n> sudo mount -t vboxsf vagrant /Users/yaosong\n\n## 删除所有未用的 Data volumes\n\n```\ndocker volume prune\n```\n\n\n\n## run 命令解释\n\n```Bash\n-d 是后台启动\ndocker run -itd  --net=br  --name spark --hostname spark yaosong5/spark:2.1.0 &> /dev/null\nsudo docker exec -it spark bash（进入后台启动的容器）\n和下面一样（直接进入）\ndocker run -it --net=br  --name spark --hostname spark yaosong5/spark:2.1.0 bash\n```\n\n## exec 进入后台容器\n```Bash\ndocker exec -it spark bash\ndocker exec -it 容器名 bash\n执行命令 docker exec -it 容器名 ip addr 可以拿到 a0 容器的 ip\n```\n\n## logs查看容器启动日志\n\n```Bash\ndocker logs -f -t --tail 100  kanbigdata_namenode_1\n```\n\n\n\n\n\n## 查看容器信息\n\n```shell\ndocker inspect hm\n执行命令 docker exec -it 容器名 ip addr 可以拿到 a0 容器的 ip\n```\n\n\n\n## 启动 关闭 删除容器\n\n```shell\ndocker start \ndocker stop 容器名\ndocker rm 容器名\n```\n\n## cp容器宿主互拷文件\n\n```shell\ndocker cp /Users/yaosong/Yao/etc.tar  f7e795c0fddd:/\n后为容器id:/目录\n```\n\n## 删除镜像\n\n```shell\n（根据镜像id删除）\ndocker rmi 00de07ebadff\n用docker images -a 查看image id，\n也可docker rmi 镜像名:版本号\n```\n\n\n## 保存镜像\n```shell\ndocker commit -m \"centos-6.9 with spark 2.2.0 and hadoop 2.8.0\"  os   centos:hadoop-spark\n\ndocker commit -m \"bigdata:spark,hadoop,hive,mysql and shell  foundation\"   --author=\"yaosong\"  master   yao/os/bigdata:2.1\n```\n\n\n## Docker用DockerFile创建镜像\n\n```shell\ndocker build -t hadoop:v1- <Dockerfile\ndocker build -t=\"hadoop:v1\" .  （.表示是当前文件夹，也就是dockerfile所在文件夹）\ndocker build -f Dockerfile -t hadoop:v1 . 此命令也可\n```\n\n## 一键启动docker-compose.yml编排的所有服务\n\n```shell\ndocker-compose -f docker-compose.yml up d\n```\n\n## Docker改变标签\n\ndocker tag IMAGEID(镜像id) REPOSITORY:TAG（仓库：标签）\n\n`docker tag  b7a66cb0e8ba yaosong5/bigdata:1.0`\n\n## 搜索docker镜像\n\n```shell\ndocker search yaosong5\n```\n\n\n\n## 登录docker账户\n\n`docker login` 登录docker hub中注册的账户\n\n\n\n\n## 上传仓库\n\n`docker push yaosong5/elk:1.0`\n\n## 容器保存为镜像，加载本地镜像 引用\n\n```shell\ndocker save imageID > filename\ndocker load <filename\n如：\ndocker save 4f9e92e56941>  /Users/yaosong/centosSparkHadoop.tar\ndocker load </Users/yaosong/centosSparkHadoop.tar\n\n通过 image 保存的镜像会保存操作历史，可以回滚到历史版本。\n```\n\n## 保存，加载容器命令：\n```powershell\ndocker export containID > filename\ndocker import filename [newname]\n```\n通过容器保存的镜像不会保存操作历史，所以文件小一点。\n如果要运行通过容器加载的镜像， 需要在运行的时候加上相关命令。\n\n#  Docker-machine命令\n## 列出docker-machine\n\n```shell\ndocker-machine ls\n```\n\n## 开启虚拟机\n\n```shell\ndocker-machine start default\n```\n\n## 关闭虚拟机\n\n```shell\ndocker-machine stop default\n```\n\n## 重启虚拟机\n\n```bash\ndocker-machine restart default\n```\n\n## 删除虚拟机\n\n```bash\ndocker-machine rm default\n```\n\n## 设置环境变量docker-machine\n\n```bash\neval $(docker-machine env default) # Setup the environment\n```\n\n\n\n","slug":"Docker命令汇集","published":1,"updated":"2018-08-24T02:13:02.239Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vg000e2tpbuzdp9k6y"},{"title":"Docker安装Hadoop集群【引用】","date":"2018-06-03T06:36:21.404Z","toc":true,"_content":" Docker配置Hadoop集群环境\n 在网上找到一个网友自制的镜像，拉取配置都是参考的，记录一下。\n<!--more-->\n# 拉取镜像\n>sudo docker pull kiwenlau/hadoop-master:0.1.0\n>sudo docker pull kiwenlau/hadoop-slave:0.1.0\n>sudo docker pull kiwenlau/hadoop-base:0.1.0\n>sudo docker pull kiwenlau/serf-dnsmasq:0.1.0\n\n\n## 查看下载的镜像\n![下载的镜像](https://www.github.com/yaosong5/tuchuang/raw/master/mdtc/2018/6/3/1528038081307.jpg)\n\n\n\n\n> sudo docker images\n\n# 在github中拉取源代码\n(或者在oschina中拉取)\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n开源中国\ngit clone http://git.oschina.net/kiwenlau/hadoop-cluster-docker\n\n\n# 运行容器\n拉取镜像后，打开源代码文件夹，并且运行脚本\n>cd hadoop-cluster-docker\n\n\n![源包下文件](https://www.github.com/yaosong5/tuchuang/raw/master/mdtc/2018/6/3/1528038513960.jpg)\n\n注意：运行脚本时,需要先启动docker服务\n>./start-container.sh\n>![enter description here](https://www.github.com/yaosong5/tuchuang/raw/master/mdtc/2018/6/3/1528039457334.jpg)\n\n一共开启了 3 个容器，1 个 master, 2 个 slave。开启容器后就进入了 master 容器 root 用户的根目录（/root）\n\n## 查看root目录下文件\n![enter description here](https://www.github.com/yaosong5/tuchuang/raw/master/mdtc/2018/6/3/1528039642562.jpg)\n\n## 测试容器是否正常运行\n```serf members```\n<!-- more -->\n\n--------\n参考：[基于 Docker 快速搭建多节点 Hadoop 集群](http://dockone.io/article/395)","source":"_posts/Docker安装Hadoop集群【引用】.md","raw":"---\ntitle:  Docker安装Hadoop集群【引用】\ndate: 2018年06月03日 14时36分28秒\ntags:  [Docker,Hadoop]\ncategories: 环境配置\ntoc: true\n---\n Docker配置Hadoop集群环境\n 在网上找到一个网友自制的镜像，拉取配置都是参考的，记录一下。\n<!--more-->\n# 拉取镜像\n>sudo docker pull kiwenlau/hadoop-master:0.1.0\n>sudo docker pull kiwenlau/hadoop-slave:0.1.0\n>sudo docker pull kiwenlau/hadoop-base:0.1.0\n>sudo docker pull kiwenlau/serf-dnsmasq:0.1.0\n\n\n## 查看下载的镜像\n![下载的镜像](https://www.github.com/yaosong5/tuchuang/raw/master/mdtc/2018/6/3/1528038081307.jpg)\n\n\n\n\n> sudo docker images\n\n# 在github中拉取源代码\n(或者在oschina中拉取)\ngit clone https://github.com/kiwenlau/hadoop-cluster-docker\n开源中国\ngit clone http://git.oschina.net/kiwenlau/hadoop-cluster-docker\n\n\n# 运行容器\n拉取镜像后，打开源代码文件夹，并且运行脚本\n>cd hadoop-cluster-docker\n\n\n![源包下文件](https://www.github.com/yaosong5/tuchuang/raw/master/mdtc/2018/6/3/1528038513960.jpg)\n\n注意：运行脚本时,需要先启动docker服务\n>./start-container.sh\n>![enter description here](https://www.github.com/yaosong5/tuchuang/raw/master/mdtc/2018/6/3/1528039457334.jpg)\n\n一共开启了 3 个容器，1 个 master, 2 个 slave。开启容器后就进入了 master 容器 root 用户的根目录（/root）\n\n## 查看root目录下文件\n![enter description here](https://www.github.com/yaosong5/tuchuang/raw/master/mdtc/2018/6/3/1528039642562.jpg)\n\n## 测试容器是否正常运行\n```serf members```\n<!-- more -->\n\n--------\n参考：[基于 Docker 快速搭建多节点 Hadoop 集群](http://dockone.io/article/395)","slug":"Docker安装Hadoop集群【引用】","published":1,"updated":"2018-08-06T19:27:34.504Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vh000f2tpbbwnzdnh3"},{"title":"Docker构建免密ssh免密镜像","date":"2018-07-19T18:49:08.996Z","toc":true,"_content":"\n##  免密登录\n参考的是http://www.shushilvshe.com/data/docker-ssh.html\n文中涉及命令\n```\nsudo yum -y install openssh-server openssh-clients\nssh-keygen -t rsa\ncp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\nchmod 700 ~/.ssh\nchmod 600 ~/.ssh/authorized_keys\n```\n\n<!--more-->\n**vim /etc/ssh/sshd_config**\n　　找到以下内容，并去掉注释符”#“\n\n```\nRSAAuthentication yes\nPubkeyAuthentication yes\nAuthorizedKeysFile      .ssh/authorized_keys\n```\n\n**vim /etc/ssh/ssh_config**\n\n```\nHost *\n\tStrictHostKeyChecking no\n\tUserKnownHostsFile=/dev/null\n```\n\n​\t\t此文也可参考 http://www.voidcn.com/article/p-gxkeusey-ma.html\n\n\n\n> https://blog.csdn.net/a85820069/article/details/78745899\n> 坑\n> 使用 docker run -i -t –name c1 centos6.6:basic /bin/bash 运行容器，sshd 服务是不开启的，必须先 - d 在用 exec 切入。\n\n\nhttps://www.cnblogs.com/aiweixiao/p/5516974.html\n\n\n1.【查看是否启动】\n\n　　启动 SSH 服务 “/etc/init.d/sshd start”。然后用 netstat -antulp | grep ssh 看是否能看到相关信息就可以了。\n\n2.【设置自动启动】\n\n　　如何设置把 ssh 等一些服务随系统开机自动启动？\n\n\t\t方法一：[root@localhost ~]# vi /etc/rc.local\n加入：service sshd start 或  /etc/init.d/sshd start\n\n\n\n**chmod 777 /etc/ssh/ssh_host_ecdsa_key**\n\n```\n# 免密登录\n\nmkdir -p /root/.ssh  \ntouch /root/.ssh/config  \necho \"StrictHostKeyChecking no\" > /root/.ssh/config  \nsed -i \"a UserKnownHostsFile /dev/null\" /root/.ssh/config \n\n# 开机启动\n\nRUN yum install -y openssh-server sudo  \nRUN sed -i 's/UsePAM yes/UsePAM no/g' /etc/ssh/sshd_config  \n\n# 下面这两句比较特殊，在centos6上必须要有，否则创建出来的容器sshd不能登录\n\nRUN ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key  \nRUN ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key \n```\n\n\n\n## ssh服务文章当中的\n```bash\ncurl http://mirrors.aliyun.com/repo/Centos-6.repo > /etc/yum.repos.d/CentOS-Base-6-aliyun.repo\nmv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak\nyum makecache\nyum install -y net-tools which openssh-clients openssh-server iproute.x86_64 wget\n\nservice sshd start\n\nsed -i 's/UsePAM yes/UsePAM no/g' /etc/ssh/sshd_config\nsed -ri 's/session required pam_loginuid.so/#session required pam_loginuid.so/g' /etc/pam.d/sshd\n\nchkconfig sshd on\n\ncd ~;ssh-keygen -t rsa -P '' -f ~/.ssh/id_dsa;cd .ssh;cat id_dsa.pub >> authorized_keys\n```\n","source":"_posts/Docker构建免密登录.md","raw":"---\ntitle:  Docker构建免密ssh免密镜像\ndate: 2018年06月21日 22时15分52秒\ntags:  [Docker]\ncategories: Docker\ntoc: true\n---\n\n##  免密登录\n参考的是http://www.shushilvshe.com/data/docker-ssh.html\n文中涉及命令\n```\nsudo yum -y install openssh-server openssh-clients\nssh-keygen -t rsa\ncp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\nchmod 700 ~/.ssh\nchmod 600 ~/.ssh/authorized_keys\n```\n\n<!--more-->\n**vim /etc/ssh/sshd_config**\n　　找到以下内容，并去掉注释符”#“\n\n```\nRSAAuthentication yes\nPubkeyAuthentication yes\nAuthorizedKeysFile      .ssh/authorized_keys\n```\n\n**vim /etc/ssh/ssh_config**\n\n```\nHost *\n\tStrictHostKeyChecking no\n\tUserKnownHostsFile=/dev/null\n```\n\n​\t\t此文也可参考 http://www.voidcn.com/article/p-gxkeusey-ma.html\n\n\n\n> https://blog.csdn.net/a85820069/article/details/78745899\n> 坑\n> 使用 docker run -i -t –name c1 centos6.6:basic /bin/bash 运行容器，sshd 服务是不开启的，必须先 - d 在用 exec 切入。\n\n\nhttps://www.cnblogs.com/aiweixiao/p/5516974.html\n\n\n1.【查看是否启动】\n\n　　启动 SSH 服务 “/etc/init.d/sshd start”。然后用 netstat -antulp | grep ssh 看是否能看到相关信息就可以了。\n\n2.【设置自动启动】\n\n　　如何设置把 ssh 等一些服务随系统开机自动启动？\n\n\t\t方法一：[root@localhost ~]# vi /etc/rc.local\n加入：service sshd start 或  /etc/init.d/sshd start\n\n\n\n**chmod 777 /etc/ssh/ssh_host_ecdsa_key**\n\n```\n# 免密登录\n\nmkdir -p /root/.ssh  \ntouch /root/.ssh/config  \necho \"StrictHostKeyChecking no\" > /root/.ssh/config  \nsed -i \"a UserKnownHostsFile /dev/null\" /root/.ssh/config \n\n# 开机启动\n\nRUN yum install -y openssh-server sudo  \nRUN sed -i 's/UsePAM yes/UsePAM no/g' /etc/ssh/sshd_config  \n\n# 下面这两句比较特殊，在centos6上必须要有，否则创建出来的容器sshd不能登录\n\nRUN ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key  \nRUN ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key \n```\n\n\n\n## ssh服务文章当中的\n```bash\ncurl http://mirrors.aliyun.com/repo/Centos-6.repo > /etc/yum.repos.d/CentOS-Base-6-aliyun.repo\nmv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak\nyum makecache\nyum install -y net-tools which openssh-clients openssh-server iproute.x86_64 wget\n\nservice sshd start\n\nsed -i 's/UsePAM yes/UsePAM no/g' /etc/ssh/sshd_config\nsed -ri 's/session required pam_loginuid.so/#session required pam_loginuid.so/g' /etc/pam.d/sshd\n\nchkconfig sshd on\n\ncd ~;ssh-keygen -t rsa -P '' -f ~/.ssh/id_dsa;cd .ssh;cat id_dsa.pub >> authorized_keys\n```\n","slug":"Docker构建免密登录","published":1,"updated":"2018-08-10T19:24:44.124Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vk000k2tpbobtqdc41"},{"utitle":"HBase原理性能分析","date":"2018-08-15T07:24:37.471Z","toc":true,"_content":"\n[TOC]\n\n\n\n![](https://ws3.sinaimg.cn/large/0069RVTdgy1fuap8gxtquj31ca0ncdij.jpg)\n\n# HBase介绍\n\nHBase表很大：一个表可以有数十亿行，上百万列；\n\nHBase的表将会分成很多个分区，每个分区部分会存在不同的机器上 \n分区是为了便于查询，放在不同机器上，io也增大，假如一个机器的io的是100m，两个就为200m，读取速度就变快了==>**多台机器的io能得到充分利用**\n\n<!-- more -->\n\nHBase表无模式：每行都有一个可排序的主键好任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有不同的列；\n\n面向列：列独立检索；\n\n稀疏：空列并不占用存储空间，表可以设计的非常稀疏；\n\n数据类型单一：HBase中的数据都是字符串，没有类型\n\n\n\nHBase采用类LSM的架构体系，数据写入并没有直接写入数据文件，而是会先写入缓存（Memstore），在满足一定条件下缓存数据再会异步刷新到硬盘。为了防止数据写入缓存之后不会因为RegionServer进程发生异常导致数据丢失，在写入缓存之前会首先将数据顺序写入HLog中。如果不幸一旦发生RegionServer宕机或者其他异常，这种设计可以从HLog中进行日志回放进行数据补救，保证数据不丢失。HBase故障恢复的最大看点就在于如何通过HLog回放补救丢失数据。\n\n\n\n# HBase结构\n\nHBase进行存储的服务器\n\nHRegion是HBase当中的一个类，**一个表分区的类，按照行分区** \n一个HRegion只会在一个HBase上，一个HBase上可以有多个HRegion \nHBase表每个分区（按照行来分区）的数据被封装到一个类HRegion内 \n如：HBase存在user表，role表，共4个regionServer，HRegion1存储管理user表的一部分，HRegion2存储管理user表的一部分，HRegion3存储管理role表的一部分，HRegion4存储管理role表的一部分\n\n## HRegionserver\n\n管理用户对Table的增、删、改、查操作；\n记录region在哪台Hregion server上\n在Region Split后，负责新Region的分配；\n新机器加入时，管理HRegion Server的负载均衡，调整Region分布\n在HRegion Server宕机后，负责失效HRegion Server 上的Regions迁移。\n\n\n\n## HBase\n\nHRegion Server主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBase中最核心的模块。\n\nHRegion Server管理了很多table的分区，也就是region。\n\n## HRegion构成\n\nHRegion类中有**HLog，store**成员，分别代表硬盘和内存 \n\n### Store\n\n每个Region包含着多个Store对象，一个列簇对应一个store 。每个Store包含一个MemStore和若干StoreFile，StoreFile包含一个或多个HFile，StoreFile是对HFile的一种封装。MemStore存放在内存中，StoreFile存储在HDFS上。\n\n### HLog\n\nHLog最终是放在hdfs上。\n\n当我们客户端上传一个表名，一个列簇，一个值，这条命令的值会原封不动的将其写入到HLog里面， 这个是一个appendLog,只可以从底部追加，不允许修改，写到HLog之后，再将数据写入到内存（memstore）当中，HLog里面是存储的操作信息的数据 写在HLog中是因为，防止在写入到内存中的时候，宕机\n\n## Region的划分\n\nRegion按大小分割的，随着数据增多，Region不断增大，当增大到一个阀值（默认256m）的时候，Region就会分成两个新的Region\n\n\n\n## HRegion的存储\n\n### ROOT表和META表\n\nHBase的所有Region元数据被存储在.META.表中，随着Region的增多，.META.表中的数据也会增大，并分裂成多个新的Region。为了定位.META.表中各个Region的位置，把.META.表中所有Region的元数据保存在-ROOT-表中，最后由Zookeeper记录-ROOT-表的位置信息。所有客户端访问用户数据前，需要首先访问Zookeeper获得-ROOT-的位置，然后访问-ROOT-表获得.META.表的位置，最后根据.META.表中的信息确定用户数据存放的位置，如下图所示。\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fuag5ugi8yj30y60ds0tw.jpg)\n\n-ROOT-表永远不会被分割，它只有一个Region，这样可以保证最多只需要三次跳转就可以定位任意一个Region。为了加快访问速度，.META.表的所有Region全部保存在内存中。客户端会将查询过的位置信息缓存起来，且缓存不会主动失效。如果客户端根据缓存信息还访问不到数据，则询问相关.META.表的Region服务器，试图获取数据的位置，如果还是失败，则询问-ROOT-表相关的.META.表在哪里。最后，如果前面的信息全部失效，则通过ZooKeeper重新定位Region的信息。所以如果客户端上的缓存全部是失效，则需要进行6次网络来回，才能定位到正确的Region。\n\n\n\nroot表,mate表都不会很大 \n因为root表，只是记录位置，本身就不会太大， \n**meta表，和root表在过程中都会被加载到内存中** \n经过3次来回，总共六次，会得到数据表的位置 \n1，client向zookeeper获取root表的位置 2，zookeeper返回root表地址信息 \n3，client读取root表，获得table1的meta表的地址，4 root表所在机器返回meta表地址 \n5，client向mete表读取table地址，6 client向table插入数据 \n读取和写入都会经历上面的过程\n\n\n\n# HBase写数据流程\n\nclient向HRegionserver发送写请求。 \nHRegionserver将**操作信息**数据写到hlog（write ahead log）。为了数据的持久化和恢复。 HLog记录的是操作数据 \n\nHRegionserver将**实际数据**写到内存（memstore） \n反馈client写成功。\n\n\n\n## 数据flush\n\n当memstore数据达到阈值64（新版本默认是128M），将内存集合中的数据刷到硬盘，将内存中的数据删除，同时删除Hlog中的历史数据。 \n并将数据存储到hdfs中。以**数据块**的形式存储。 \n在hlog中做标记点。\n\n### flush的说明\n\n当内存文件memstore文件达到64m的时候，会将数据合并刷新写入到StroeFile文件里面，再将数据写入到HFile里面，HFile文件是一个hdfs文件，序列化到hdfs里面，再通过hdfs的api写入到hdfs集群里面 \n提交到hdfs集群后，HLog，memstore的数据将会被清除\n\n\n\n## 数据合并\n\n1、当（hdfs中）数据块达到4块，hmaster将数据块**加载到本地**(HRegionserver)，进行合并 \n注：这个数据块单块没有大小限制 \n2、当合并的数据超过256M，进行拆分，将拆分后的region分配给不同的hregionserver管理 \n注：如果不大于256M，将数据原封不动写回hdfs \n3、当hregionsever**宕机**后，将该hregionserver上的hlog拆分（按表拆分)，然后分配给不同的hregionserver加载，修改.META. \n注：由Hmaster来更改.META.文件，不会对HRegionserver的性能造成影响 \n4、注意：hlog会同步到hdfs \n注：合并的数据是对相同rowkey的分组合并，如user表中，对id='1'的操作内容合并，对id='2'的合并\n\n\n\n### 合并的好处\n\n**合并操作是由hmaster来工作** \n合并操作是针对一个表来说，user表合并user表，role表合并role表 \n1、清理了垃圾数据 \n2、将大的数据块拆分后，给多个机器管理，优化读取操作等速率\n\n# HBase的读流程\n\n通过zookeeper和-ROOT- .META.表定位HBase。\n数据从内存和硬盘合并后返回给client\n数据块会缓存\n\n\n\n\n\n# Client\n\nHBase Client使用HBase的RPC机制与HRegionserver和RegionServer进行通信\n管理类操作：Client与HRegionserver进行RPC；\n数据读写类操作：Client与HBase进行RPC。\n\n\n\n\n\n\n\n# 读取数据与写入数据流程的不同\n\n除了在表中读取数据，还要在内存磁盘上去搜寻还未存到hdfs里面的数据。 \n所以数据块太大的应该拆分，可以加快查询速度 \n经常查询的数据块还应该放在内存中（HRegionserver的内存中）\n\n\n\n# HBase的出现\n\nhdfs是分布式文件系统，只能保存整个文件，如果一行一行的保存数据，namenode的压力会很大 \n假如有一个文件下有100w小个文件，每个文件都是1k，在datanode中不会占用128m的分块大，但是每个文件元数据所占的大小是一样的，这样的话，namenode空间占满时，datanode中的数据实际上很少。 \nHBase就会很好的解决这个问题，一个文件一个文件写的时候，是先写入到HBase中，先写入到HBase集群中（HBase也分为主从，主为HMaster，从为HRegionserver） \n，HRegionserver的内存中，当数据量达到128m的时候，将数据写入到hdfs中，这样128m数据的元数据只有一条\n\n# HBase细节\n\nHBase实际上是一个缓存层，存储的数据量很少，存的一部分缓存的数据，HBase需要zookeeper来定位HBase查找数据的偏移量\n\n# HBase 主从之间的关系\n\nhmaster只是一个管理者，而且只管理，当HBase集群挂掉之后，数据偏移信息和表的信息，而不管理数据信息，所以有一种极端情况，当HBase集群启动之后，表创建完成，正常运行之后，将hmaster关闭也不会影响整个集群的运行。不像namenode挂了之后不能响应了\n\n# note：\n\nHBase写快读慢，读慢是相对于写来说的，但是跟mysql相比，也不是一个量级的 \n\n\n\n","source":"_posts/HBase性能分析.md","raw":"---\nutitle: HBase原理性能分析\ndate: 2018年08月06日 22时15分52秒\ntags: [HBase,原理]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n\n\n![](https://ws3.sinaimg.cn/large/0069RVTdgy1fuap8gxtquj31ca0ncdij.jpg)\n\n# HBase介绍\n\nHBase表很大：一个表可以有数十亿行，上百万列；\n\nHBase的表将会分成很多个分区，每个分区部分会存在不同的机器上 \n分区是为了便于查询，放在不同机器上，io也增大，假如一个机器的io的是100m，两个就为200m，读取速度就变快了==>**多台机器的io能得到充分利用**\n\n<!-- more -->\n\nHBase表无模式：每行都有一个可排序的主键好任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有不同的列；\n\n面向列：列独立检索；\n\n稀疏：空列并不占用存储空间，表可以设计的非常稀疏；\n\n数据类型单一：HBase中的数据都是字符串，没有类型\n\n\n\nHBase采用类LSM的架构体系，数据写入并没有直接写入数据文件，而是会先写入缓存（Memstore），在满足一定条件下缓存数据再会异步刷新到硬盘。为了防止数据写入缓存之后不会因为RegionServer进程发生异常导致数据丢失，在写入缓存之前会首先将数据顺序写入HLog中。如果不幸一旦发生RegionServer宕机或者其他异常，这种设计可以从HLog中进行日志回放进行数据补救，保证数据不丢失。HBase故障恢复的最大看点就在于如何通过HLog回放补救丢失数据。\n\n\n\n# HBase结构\n\nHBase进行存储的服务器\n\nHRegion是HBase当中的一个类，**一个表分区的类，按照行分区** \n一个HRegion只会在一个HBase上，一个HBase上可以有多个HRegion \nHBase表每个分区（按照行来分区）的数据被封装到一个类HRegion内 \n如：HBase存在user表，role表，共4个regionServer，HRegion1存储管理user表的一部分，HRegion2存储管理user表的一部分，HRegion3存储管理role表的一部分，HRegion4存储管理role表的一部分\n\n## HRegionserver\n\n管理用户对Table的增、删、改、查操作；\n记录region在哪台Hregion server上\n在Region Split后，负责新Region的分配；\n新机器加入时，管理HRegion Server的负载均衡，调整Region分布\n在HRegion Server宕机后，负责失效HRegion Server 上的Regions迁移。\n\n\n\n## HBase\n\nHRegion Server主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBase中最核心的模块。\n\nHRegion Server管理了很多table的分区，也就是region。\n\n## HRegion构成\n\nHRegion类中有**HLog，store**成员，分别代表硬盘和内存 \n\n### Store\n\n每个Region包含着多个Store对象，一个列簇对应一个store 。每个Store包含一个MemStore和若干StoreFile，StoreFile包含一个或多个HFile，StoreFile是对HFile的一种封装。MemStore存放在内存中，StoreFile存储在HDFS上。\n\n### HLog\n\nHLog最终是放在hdfs上。\n\n当我们客户端上传一个表名，一个列簇，一个值，这条命令的值会原封不动的将其写入到HLog里面， 这个是一个appendLog,只可以从底部追加，不允许修改，写到HLog之后，再将数据写入到内存（memstore）当中，HLog里面是存储的操作信息的数据 写在HLog中是因为，防止在写入到内存中的时候，宕机\n\n## Region的划分\n\nRegion按大小分割的，随着数据增多，Region不断增大，当增大到一个阀值（默认256m）的时候，Region就会分成两个新的Region\n\n\n\n## HRegion的存储\n\n### ROOT表和META表\n\nHBase的所有Region元数据被存储在.META.表中，随着Region的增多，.META.表中的数据也会增大，并分裂成多个新的Region。为了定位.META.表中各个Region的位置，把.META.表中所有Region的元数据保存在-ROOT-表中，最后由Zookeeper记录-ROOT-表的位置信息。所有客户端访问用户数据前，需要首先访问Zookeeper获得-ROOT-的位置，然后访问-ROOT-表获得.META.表的位置，最后根据.META.表中的信息确定用户数据存放的位置，如下图所示。\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fuag5ugi8yj30y60ds0tw.jpg)\n\n-ROOT-表永远不会被分割，它只有一个Region，这样可以保证最多只需要三次跳转就可以定位任意一个Region。为了加快访问速度，.META.表的所有Region全部保存在内存中。客户端会将查询过的位置信息缓存起来，且缓存不会主动失效。如果客户端根据缓存信息还访问不到数据，则询问相关.META.表的Region服务器，试图获取数据的位置，如果还是失败，则询问-ROOT-表相关的.META.表在哪里。最后，如果前面的信息全部失效，则通过ZooKeeper重新定位Region的信息。所以如果客户端上的缓存全部是失效，则需要进行6次网络来回，才能定位到正确的Region。\n\n\n\nroot表,mate表都不会很大 \n因为root表，只是记录位置，本身就不会太大， \n**meta表，和root表在过程中都会被加载到内存中** \n经过3次来回，总共六次，会得到数据表的位置 \n1，client向zookeeper获取root表的位置 2，zookeeper返回root表地址信息 \n3，client读取root表，获得table1的meta表的地址，4 root表所在机器返回meta表地址 \n5，client向mete表读取table地址，6 client向table插入数据 \n读取和写入都会经历上面的过程\n\n\n\n# HBase写数据流程\n\nclient向HRegionserver发送写请求。 \nHRegionserver将**操作信息**数据写到hlog（write ahead log）。为了数据的持久化和恢复。 HLog记录的是操作数据 \n\nHRegionserver将**实际数据**写到内存（memstore） \n反馈client写成功。\n\n\n\n## 数据flush\n\n当memstore数据达到阈值64（新版本默认是128M），将内存集合中的数据刷到硬盘，将内存中的数据删除，同时删除Hlog中的历史数据。 \n并将数据存储到hdfs中。以**数据块**的形式存储。 \n在hlog中做标记点。\n\n### flush的说明\n\n当内存文件memstore文件达到64m的时候，会将数据合并刷新写入到StroeFile文件里面，再将数据写入到HFile里面，HFile文件是一个hdfs文件，序列化到hdfs里面，再通过hdfs的api写入到hdfs集群里面 \n提交到hdfs集群后，HLog，memstore的数据将会被清除\n\n\n\n## 数据合并\n\n1、当（hdfs中）数据块达到4块，hmaster将数据块**加载到本地**(HRegionserver)，进行合并 \n注：这个数据块单块没有大小限制 \n2、当合并的数据超过256M，进行拆分，将拆分后的region分配给不同的hregionserver管理 \n注：如果不大于256M，将数据原封不动写回hdfs \n3、当hregionsever**宕机**后，将该hregionserver上的hlog拆分（按表拆分)，然后分配给不同的hregionserver加载，修改.META. \n注：由Hmaster来更改.META.文件，不会对HRegionserver的性能造成影响 \n4、注意：hlog会同步到hdfs \n注：合并的数据是对相同rowkey的分组合并，如user表中，对id='1'的操作内容合并，对id='2'的合并\n\n\n\n### 合并的好处\n\n**合并操作是由hmaster来工作** \n合并操作是针对一个表来说，user表合并user表，role表合并role表 \n1、清理了垃圾数据 \n2、将大的数据块拆分后，给多个机器管理，优化读取操作等速率\n\n# HBase的读流程\n\n通过zookeeper和-ROOT- .META.表定位HBase。\n数据从内存和硬盘合并后返回给client\n数据块会缓存\n\n\n\n\n\n# Client\n\nHBase Client使用HBase的RPC机制与HRegionserver和RegionServer进行通信\n管理类操作：Client与HRegionserver进行RPC；\n数据读写类操作：Client与HBase进行RPC。\n\n\n\n\n\n\n\n# 读取数据与写入数据流程的不同\n\n除了在表中读取数据，还要在内存磁盘上去搜寻还未存到hdfs里面的数据。 \n所以数据块太大的应该拆分，可以加快查询速度 \n经常查询的数据块还应该放在内存中（HRegionserver的内存中）\n\n\n\n# HBase的出现\n\nhdfs是分布式文件系统，只能保存整个文件，如果一行一行的保存数据，namenode的压力会很大 \n假如有一个文件下有100w小个文件，每个文件都是1k，在datanode中不会占用128m的分块大，但是每个文件元数据所占的大小是一样的，这样的话，namenode空间占满时，datanode中的数据实际上很少。 \nHBase就会很好的解决这个问题，一个文件一个文件写的时候，是先写入到HBase中，先写入到HBase集群中（HBase也分为主从，主为HMaster，从为HRegionserver） \n，HRegionserver的内存中，当数据量达到128m的时候，将数据写入到hdfs中，这样128m数据的元数据只有一条\n\n# HBase细节\n\nHBase实际上是一个缓存层，存储的数据量很少，存的一部分缓存的数据，HBase需要zookeeper来定位HBase查找数据的偏移量\n\n# HBase 主从之间的关系\n\nhmaster只是一个管理者，而且只管理，当HBase集群挂掉之后，数据偏移信息和表的信息，而不管理数据信息，所以有一种极端情况，当HBase集群启动之后，表创建完成，正常运行之后，将hmaster关闭也不会影响整个集群的运行。不像namenode挂了之后不能响应了\n\n# note：\n\nHBase写快读慢，读慢是相对于写来说的，但是跟mysql相比，也不是一个量级的 \n\n\n\n","slug":"HBase性能分析","published":1,"updated":"2018-08-15T14:46:17.237Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vm000m2tpbjfpcdq2w"},{"title":"HBase拷贝生产环境数据到本地Spark解析运行调试","date":"2018-08-10T10:19:15.453Z","toc":true,"_content":"\n[TOC]\n\n> 由于线上环境要经过跳板机跳转，并且打包测试，上传jar包步骤多，不然的话，要进行各种端口转发，且有权限控制，不易在本地idea编辑器上进行程序运行及调试\n>\n> 现在想法是，将线上测试环境的数据拷贝小部分到本地自己搭建的集群，进行程序的逻辑和初期调试\n>\n> 此贴就是记录一些操作\n\n这都是要基于本地有HBASE及其依赖组件的。\n\n主要思路是，拷贝线上查询的结果到文件hbaseout1.txt，将hbaseout1.txt文件sz导入本地\n\n再在本地集群上将数据插入到hbase\n\n<!-- more -->\n\n# 1、创建和线上同名通结构的表\n\n在线上执行 `describe 'beehive:a_up_rawdata'`\n\n得到\n\n![](https://ws4.sinaimg.cn/large/0069RVTdgy1fu4t1ryqfbj31kw074n4w.jpg)\n\n\n\n在本地执行\n\n```\nhbase shell\ncreate_namespace 'beehive'\n\ncreate 'beehive:a_up_rawdata',{NAME => 'cf', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', COMPRESSION => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '1'}\n```\n\n\n\n# 2、拷贝线上hbase数据\n\n\n\n## 查询结果导入文件\n\n在线上机器任意目录执行\n\n```bash\necho \"get 'beehive:a_up_rawdata','530111199211287371',{COLUMN=>'cf:273468436_data'}\"| hbase shell> hbaseout1.txt\n```\n\n\n\n解析: `get 'beehive:a_up_rawdata','530111199211287371',{COLUMN=>'cf:273468436_data'}`是执行的hbase的查询语句，将查询的结果存入到当前目录 **hbaseout1.txt**文件中\n\n## 查询文件下载到本地\n\n`sz hbaseout1.txt`\n\n## 修改文件内容\n\n可以查看hbaseout1.txt中可以看到会有表头\n\n![](https://ws2.sinaimg.cn/large/0069RVTdgy1fu4s99k250j31g208qgs4.jpg)\n\n需要将这部分表头数据删除，组成标准的导入文件\n\n## 修改文件编码\n\n![](https://ws1.sinaimg.cn/large/0069RVTdgy1fu4shn9bpnj31l203i0tv.jpg)\n\n在通过 hbase shell 查看中文值时, 是 unicode 编码格式，使得直接查看中文值不太方便。如果要查看需要把 unicode 编码进行 decode\n\n[参考]: https://blog.csdn.net/zychun1991/article/details/69938992\t\"hbase shell 中文 unicode 编码\"\n\n将查询结果导出来\n\n```bash\nprint ('需要转码内容'.decode('utf-8'))\n\n命令样例\npython 2.7 \n\tprint ('***\\xE4\\xBD\\xA010009 '.decode('utf-8'))\npython 3 \n\tprint '***\\xE4\\xBD\\xA010009 '.decode('utf-8')\n```\n\n可以有更友好的将内容设置为文件名，然后将转码后重新写入到一个文件，后续会更新\n\n\n\n转码过后，文字显示正确\n\n![](https://ws4.sinaimg.cn/large/0069RVTdgy1fu4sp3u3w6j31ak0f27fi.jpg)\n\n\n\n## 重新组合文件\n\n由于导入到hbase命令为 **`格式：hbase [类][分隔符] [行键，列族][表] [导入文件`]\n\n由于我这次导入的文件里面有“,”，所有将分隔符设置为“|”\n\n更改后的文件格式为\n\n![](https://ws2.sinaimg.cn/large/0069RVTdgy1fu4ssvedr6j319a030wel.jpg)\n\n将文件上传到hdfs\n\n```bash\nhadoop fs -put hadoop fs -put hbaseout1.txt /local/\n```\n\n## 将数据导入到本地hbase\n\n```Bash\nhbase org.apache.hadoop.hbase.mapreduce.ImportTsv  -Dimporttsv.separator=\"|\"  -Dimporttsv.columns=HBASE_ROW_KEY,cf:273468436_data beehive:a_up_rawdata /local/hbaseout2.txt\n```\n\n\n# 3、校验查看\n\n在hue上查看hbase内容，显示有数据\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu50q6kl65j31kg0kymxw.jpg)\n\n在hbase shell 查看\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fu515ozpw6j31kw08b7dl.jpg)\n\n","source":"_posts/HBase拷贝生产环境数据到本地运行调试.md","raw":"---\ntitle: HBase拷贝生产环境数据到本地Spark解析运行调试\ndate: 2018年08月06日 22时15分52秒\ntags: [HBase,操作]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n> 由于线上环境要经过跳板机跳转，并且打包测试，上传jar包步骤多，不然的话，要进行各种端口转发，且有权限控制，不易在本地idea编辑器上进行程序运行及调试\n>\n> 现在想法是，将线上测试环境的数据拷贝小部分到本地自己搭建的集群，进行程序的逻辑和初期调试\n>\n> 此贴就是记录一些操作\n\n这都是要基于本地有HBASE及其依赖组件的。\n\n主要思路是，拷贝线上查询的结果到文件hbaseout1.txt，将hbaseout1.txt文件sz导入本地\n\n再在本地集群上将数据插入到hbase\n\n<!-- more -->\n\n# 1、创建和线上同名通结构的表\n\n在线上执行 `describe 'beehive:a_up_rawdata'`\n\n得到\n\n![](https://ws4.sinaimg.cn/large/0069RVTdgy1fu4t1ryqfbj31kw074n4w.jpg)\n\n\n\n在本地执行\n\n```\nhbase shell\ncreate_namespace 'beehive'\n\ncreate 'beehive:a_up_rawdata',{NAME => 'cf', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', COMPRESSION => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '1'}\n```\n\n\n\n# 2、拷贝线上hbase数据\n\n\n\n## 查询结果导入文件\n\n在线上机器任意目录执行\n\n```bash\necho \"get 'beehive:a_up_rawdata','530111199211287371',{COLUMN=>'cf:273468436_data'}\"| hbase shell> hbaseout1.txt\n```\n\n\n\n解析: `get 'beehive:a_up_rawdata','530111199211287371',{COLUMN=>'cf:273468436_data'}`是执行的hbase的查询语句，将查询的结果存入到当前目录 **hbaseout1.txt**文件中\n\n## 查询文件下载到本地\n\n`sz hbaseout1.txt`\n\n## 修改文件内容\n\n可以查看hbaseout1.txt中可以看到会有表头\n\n![](https://ws2.sinaimg.cn/large/0069RVTdgy1fu4s99k250j31g208qgs4.jpg)\n\n需要将这部分表头数据删除，组成标准的导入文件\n\n## 修改文件编码\n\n![](https://ws1.sinaimg.cn/large/0069RVTdgy1fu4shn9bpnj31l203i0tv.jpg)\n\n在通过 hbase shell 查看中文值时, 是 unicode 编码格式，使得直接查看中文值不太方便。如果要查看需要把 unicode 编码进行 decode\n\n[参考]: https://blog.csdn.net/zychun1991/article/details/69938992\t\"hbase shell 中文 unicode 编码\"\n\n将查询结果导出来\n\n```bash\nprint ('需要转码内容'.decode('utf-8'))\n\n命令样例\npython 2.7 \n\tprint ('***\\xE4\\xBD\\xA010009 '.decode('utf-8'))\npython 3 \n\tprint '***\\xE4\\xBD\\xA010009 '.decode('utf-8')\n```\n\n可以有更友好的将内容设置为文件名，然后将转码后重新写入到一个文件，后续会更新\n\n\n\n转码过后，文字显示正确\n\n![](https://ws4.sinaimg.cn/large/0069RVTdgy1fu4sp3u3w6j31ak0f27fi.jpg)\n\n\n\n## 重新组合文件\n\n由于导入到hbase命令为 **`格式：hbase [类][分隔符] [行键，列族][表] [导入文件`]\n\n由于我这次导入的文件里面有“,”，所有将分隔符设置为“|”\n\n更改后的文件格式为\n\n![](https://ws2.sinaimg.cn/large/0069RVTdgy1fu4ssvedr6j319a030wel.jpg)\n\n将文件上传到hdfs\n\n```bash\nhadoop fs -put hadoop fs -put hbaseout1.txt /local/\n```\n\n## 将数据导入到本地hbase\n\n```Bash\nhbase org.apache.hadoop.hbase.mapreduce.ImportTsv  -Dimporttsv.separator=\"|\"  -Dimporttsv.columns=HBASE_ROW_KEY,cf:273468436_data beehive:a_up_rawdata /local/hbaseout2.txt\n```\n\n\n# 3、校验查看\n\n在hue上查看hbase内容，显示有数据\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu50q6kl65j31kg0kymxw.jpg)\n\n在hbase shell 查看\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fu515ozpw6j31kw08b7dl.jpg)\n\n","slug":"HBase拷贝生产环境数据到本地运行调试","published":1,"updated":"2018-08-15T14:29:52.538Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vn000r2tpb0lbd5tnc"},{"title":"HDFS元数据备份流程","date":"2018-08-15T15:56:42.315Z","toc":true,"_content":"\n[TOC]\n\n\n\n<!-- more -->","source":"_posts/HDFS元数据备份流程.md","raw":"---\ntitle: HDFS元数据备份流程\ndate: 2018年08月06日 22时15分52秒\ntags: [HDFS,原理,Hadoop]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n\n\n<!-- more -->","slug":"HDFS元数据备份流程","published":1,"updated":"2018-08-15T15:56:42.934Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vo000t2tpbjbd6bdyw"},{"title":"Hadoop零碎知识点","date":"2018-08-15T16:34:55.243Z","toc":true,"_content":"\n[TOC]\n\n## 查看元数据信息\n可以通过hdfs的一个工具来查看edits中的信息\n\n```bash\nbin/hdfs oev -i edits -o edits.xml\nbin/hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml\n```\n<!-- more -->\n\n","source":"_posts/Hadoop零碎知识点.md","raw":"---\ntitle: Hadoop零碎知识点\ndate: 2018年08月06日 22时15分52秒\ntags: [HDFS,原理,Hadoop]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n## 查看元数据信息\n可以通过hdfs的一个工具来查看edits中的信息\n\n```bash\nbin/hdfs oev -i edits -o edits.xml\nbin/hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml\n```\n<!-- more -->\n\n","slug":"Hadoop零碎知识点","published":1,"updated":"2018-09-05T02:58:28.337Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vp000y2tpb9113ca31"},{"title":"Hadoop-HA-Federation机制","date":"2018-08-15T15:21:34.817Z","toc":true,"_content":"\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/006tNbRwly1fubqgn0culj31eo0nm401.jpg)\n\n（1）hadoop-HA集群运作机制介绍\n\n所谓HA，即高可用（7*24小时不中断服务）\n\n实现高可用最关键的是消除单点故障\n\nhadoop-ha严格来说应该分成各个组件的HA机制——HDFS的HA、YARN的HA\n\n \n\n（2）HDFS的HA机制详解\n\n通过双namenode消除单点故障\n\n双namenode协调工作的要点：\n\n​    A、元数据管理方式需要改变：\n\n​    内存中各自保存一份元数据\n\n​    Edits日志只能有一份，只有Active状态的namenode节点可以做写操作\n\n​    两个namenode都可以读取edits\n\n​    共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现）\n\n​    B、需要一个状态管理功能模块\n\n​    实现了一个zkfailover，常驻在每一个namenode所在的节点\n\n​    每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识\n\n​    当需要进行状态切换时，由zkfailover来负责切换\n\n​    切换时需要防止brain split现象的发生\n\n\n\n\n\n\n\nHadoop-HA的主要思想是有两个NameNode，一个作为主NameNode，一个作为standby，两个NameNode使用同一个命名空间。通过zookeepr（JournalNode）来进行协调，实现NameNode的主备切换。\n\n<!-- more -->","source":"_posts/Hadoop-构成及HA-.md","raw":"---\ntitle: Hadoop-HA-Federation机制\ndate: 2018年08月06日 22时15分52秒\ntags: [Hadoop]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/006tNbRwly1fubqgn0culj31eo0nm401.jpg)\n\n（1）hadoop-HA集群运作机制介绍\n\n所谓HA，即高可用（7*24小时不中断服务）\n\n实现高可用最关键的是消除单点故障\n\nhadoop-ha严格来说应该分成各个组件的HA机制——HDFS的HA、YARN的HA\n\n \n\n（2）HDFS的HA机制详解\n\n通过双namenode消除单点故障\n\n双namenode协调工作的要点：\n\n​    A、元数据管理方式需要改变：\n\n​    内存中各自保存一份元数据\n\n​    Edits日志只能有一份，只有Active状态的namenode节点可以做写操作\n\n​    两个namenode都可以读取edits\n\n​    共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现）\n\n​    B、需要一个状态管理功能模块\n\n​    实现了一个zkfailover，常驻在每一个namenode所在的节点\n\n​    每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识\n\n​    当需要进行状态切换时，由zkfailover来负责切换\n\n​    切换时需要防止brain split现象的发生\n\n\n\n\n\n\n\nHadoop-HA的主要思想是有两个NameNode，一个作为主NameNode，一个作为standby，两个NameNode使用同一个命名空间。通过zookeepr（JournalNode）来进行协调，实现NameNode的主备切换。\n\n<!-- more -->","slug":"Hadoop-构成及HA-","published":1,"updated":"2018-08-16T10:52:49.096Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vq000z2tpbfew2692s"},{"title":"Hbase-shell操作","date":"2018-08-10T09:27:05.100Z","toc":true,"_content":"\n[TOC]\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fu5599pvv8j30y60b0dfw.jpg)\n\nhbase使用命令行操作，简单直接，方便快捷，掌握一点必备的基础命令。\n\nHBase启动命令行\n\n```bash\n$HBASE_HOME/bin/hbase shell\n```\n\n<!-- more -->\n\n## 创建表\n\n```bash\ncreate 'testtable',{NAME=>'cf',VERSIONS=>2},{NAME=>'cf2',VERSIONS=>2}\n```\n\n## 创建namespace\n\n```bash\ncreate_namespace 'beehive'\n```\n\n\n\n## 查看表结构\n\n```bash\ndisable 'testtable'\n```\n\n\n\n## 删除表\n\n```bash\ndrop 'testtable'\n```\n\n## 修改表\n\n```bash\ndisable 'testtable'\nalter 'testtable',{NAME=>'cf',TTL=>'10000000'},{NAME=>'cf2',TTL=>'10000000'}\nenable 'testtable'\n修改表必须先 disable 表\n```\n\n## 表数据的增删查改：\n\n### 添加数据：\n\n```Bash\nput 'testtable','rowkey1','cf:key1','val1'\n```\n\n\n\n### 查询数据:\n\n```bash\nget 'testtable','rowkey1','cf:key1'\nget 'testtable','rowkey1', {COLUMN=>'cf:key1'}\n```\n\n\n\n### 扫描表:\n\n```bash\nscan 'testtable',{COLUMNS=>cf:col1,LIMIT=>5} #可以添加STARTROW、TIMERANGE和FITLER等高级功能\n```\n\n### 查询表中的数据行数:\n\n语法：`count <table>, {INTERVAL => intervalNum, CACHE => cacheNum}`\n\n```Bash\ncount 'testtable',{INTERVAL => 100, CACHE => 500}\n```\n\n\n\n### 删除数据:\n\n```bash\ndelete 'testtable','rowkey1','cf:key1'\ntruncate 'testtable'\n```\n\n","source":"_posts/Hbase-shell操作.md","raw":"---\ntitle: Hbase-shell操作\ndate: 2018年08月06日 22时15分52秒\ntags:  [HBase,Shell]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fu5599pvv8j30y60b0dfw.jpg)\n\nhbase使用命令行操作，简单直接，方便快捷，掌握一点必备的基础命令。\n\nHBase启动命令行\n\n```bash\n$HBASE_HOME/bin/hbase shell\n```\n\n<!-- more -->\n\n## 创建表\n\n```bash\ncreate 'testtable',{NAME=>'cf',VERSIONS=>2},{NAME=>'cf2',VERSIONS=>2}\n```\n\n## 创建namespace\n\n```bash\ncreate_namespace 'beehive'\n```\n\n\n\n## 查看表结构\n\n```bash\ndisable 'testtable'\n```\n\n\n\n## 删除表\n\n```bash\ndrop 'testtable'\n```\n\n## 修改表\n\n```bash\ndisable 'testtable'\nalter 'testtable',{NAME=>'cf',TTL=>'10000000'},{NAME=>'cf2',TTL=>'10000000'}\nenable 'testtable'\n修改表必须先 disable 表\n```\n\n## 表数据的增删查改：\n\n### 添加数据：\n\n```Bash\nput 'testtable','rowkey1','cf:key1','val1'\n```\n\n\n\n### 查询数据:\n\n```bash\nget 'testtable','rowkey1','cf:key1'\nget 'testtable','rowkey1', {COLUMN=>'cf:key1'}\n```\n\n\n\n### 扫描表:\n\n```bash\nscan 'testtable',{COLUMNS=>cf:col1,LIMIT=>5} #可以添加STARTROW、TIMERANGE和FITLER等高级功能\n```\n\n### 查询表中的数据行数:\n\n语法：`count <table>, {INTERVAL => intervalNum, CACHE => cacheNum}`\n\n```Bash\ncount 'testtable',{INTERVAL => 100, CACHE => 500}\n```\n\n\n\n### 删除数据:\n\n```bash\ndelete 'testtable','rowkey1','cf:key1'\ntruncate 'testtable'\n```\n\n","slug":"Hbase-shell操作","published":1,"updated":"2018-08-10T18:00:25.636Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vs00132tpbsgpkqvzt"},{"title":"Hdfs结构性能分析及读写流程","date":"2018-08-15T15:55:15.892Z","toc":true,"_content":"\n[TOC]\n\n\n\n\n\n<!-- more -->\n\n# namenode和secondaryNameNode最好不要放在一台机器上\n\n宕机可能导致数据不能恢复 \n测试环境或者学习环境可以弄在一台机器上\n\n# hdfs中namenode和datanode的初始化\n\nhdfs会在配置文件中配置一个datanode的工作目录元数据 \n查看目录结构 tree hddata/ \n![img](file:///var/folders/9p/dbpbsyq158n7y12303j34hxm0000gp/T/WizNote/3172cdcb-77fe-4f72-8882-12fe40ddef6a/index_files/4ee70fb7-cfdc-4010-a387-986e268b5a1e.jpg) \ndatanode的工作目录是在datanode启动后初始化的 \n而hadoop namenode format 只会初始name的工作目录，和datanode没有关系\n\n# 把一个hdfs的一个节点加入到另一个集群\n\n必须要将hdfs datanode的工作目录删除，不然持有上一个集群的datanode的工作目录，会认为是一个误操作，为了防止丢失数据，不会让其连接上\n\n# 如果集群够大，上百台机器\n\n那么在hdfs上面，是需要配置机架感知\n\n# namenode管理元数据\n\nnamenode会将元数据放在内存里面，这样方便快速对数据的请求 \n但是放在内存中是不安全的，所有就序列化到fsimage里面 \n![img](file:///var/folders/9p/dbpbsyq158n7y12303j34hxm0000gp/T/WizNote/3172cdcb-77fe-4f72-8882-12fe40ddef6a/index_files/2d3df4da-f64b-4ccf-967b-9ade7c2ab46f.jpg) \n就像jvm中dump，将内存中所有数据dump出去 \n假如 \n![img](file:///var/folders/9p/dbpbsyq158n7y12303j34hxm0000gp/T/WizNote/3172cdcb-77fe-4f72-8882-12fe40ddef6a/index_files/8fa4e8c2-8b29-4809-8219-b383aa6d8664.jpg) \n所以存大文件划算，因为元数据消耗的内存都是一样的 \n![img](file:///var/folders/9p/dbpbsyq158n7y12303j34hxm0000gp/T/WizNote/3172cdcb-77fe-4f72-8882-12fe40ddef6a/index_files/2a0de522-13be-4777-b2f4-68377648bbb4.jpg) \n但是内存中的数据量太大，不可能经常序列化，所以需要定时序列化\n\n## 所以引入了secondaryNameNode\n\n更新元数据的时候，不可能去直接跟更改元数据fsimage文件，因为文件是线性结构，假如遇到更改中间内容会很不方便，所有就将操作信息记录在edits日志文件中，只是记录操作信息\n\n### edits文件定期转为元数据\n\n为了防止edits过多，导致在启动hdfs集群datanode的时候会很慢，因为需要将edits通过转化形成为元数据fsimage文件，所以应该定期将edits文件转换为fsimage元数据，然后将fsimage替换掉\n\n### secondaryNameNode的出现\n\n如果nameNode来做上面的edits转换为元数据的话，由于消耗的资源太大，就不能为其他比如从hdfs中读取数据服务提供资源，或者提供服务的效果不好 \n所以这个时候就把合并操作交给secondNameNode来做 \n这个过程叫做checkpoint \n![img](file:///var/folders/9p/dbpbsyq158n7y12303j34hxm0000gp/T/WizNote/3172cdcb-77fe-4f72-8882-12fe40ddef6a/index_files/d6a7699e-3158-4cfc-ab83-9fec25286004.jpg)\n\n## 为了防止namenode宕机导致了数据丢失\n\n可以在hdfs-site.xml文件中在多个机器上的目录来保存name的edits，fsimage文件\n\n `<property>``    <name>dfs.name.dir</name>``    <value>/home/bigdata/names1,/home/bigdata/names2</value>``</property>`\n\n配置的多个的话，会同时往这两个目录中写\n\n如果不配置这个默认的目录是core-site.xml文件中配置的hadoop的临时文件 \n ` \n <property> \n​       <name>hadoop.tmp.dir</name> \n​       <value>/home/bigdata/apps/hadoop-2.6.4/tmp</value> \n   </property> `\n\n# Hdfs的结构\n\n1.HDFS集群分为两大角色：NameNode、DataNode （secondary NameNode）\n2.NameNode负责管理整个文件系统的元数据\n记录文件在哪里\n3.DataNode 负责管理用户的文件数据块\n不负责切块，负责保管\n4.文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上\n5.每一个文件块可以有多个副本，并存放在不同的datanode上\n副本不会放在同一个机器上，因为副本就是防止宕机，\n6.Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量\n因为datanode如果宕机的话，name该机器上的对应的副本数据将会消失，这样需要将其在其他机器上进行恢复，恢复的话，就需要上面就需要数据和未宕机时的数据尽量保持一致，所以需要依赖于datanode定期汇报，不然差距的数据会很大\n7.HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行\n\n\n\n## Hdfs写操作\n\n 详细步骤解析\n\n1、根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在\n\n不存在则会返回path not exist异常\n\n2、namenode返回是否可以上传\n\n3、client请求第一个 block（0-128m）该传输到哪些datanode服务器上\n\n返回该block存放的位置，及其副本的信息存放的位置\n\n4、namenode返回3个datanode服务器ABC\n\n副本选择策略\n\n考虑空间和距离的因素，网络跳转的跳数，比如说机架的位置，\n\n第一台是看谁比较近（机架），因为传输比较快，副本则是是看谁比较远，防止机架出问题（如断电），干扰性更小\n\n而集群全线崩塌\n\n5、client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端\n\n这样是防止整个流程变慢，同时创建通道，先建立通道pipeline,通道\n\n6、client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存bytebuf），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答\n\n \n\n因为等一个block写满之后再传送，速度会很慢，所以是接收一个packet就会写入到管道流pipeline中。\n\n只要上传一个成功，则客户端视为上传成功，因为如果没上传成功，namenode会进行异步的复制副本的信息\n\n7、当一个block传输完成之后，client再次请求**namenode**上传第二个block的服务器。\n\n注：写的过程中，namenode记录下来了文件路径，文件有几个block也记录下来了，每个block分配到哪些机器上也记录下到了，及其每个block的副本信息，副本在那几个机器上。\n\n校验的时候不是一个packet（一批chunk，共64k）校验，而是以一个chunk来校验，一个chunk是512byte（字节）\n\n# Hdfs读操作\n\n1、跟namenode通信查询元数据，找到文件块所在的datanode服务器\n\n2、挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流\n\n3、datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）\n\n4、客户端以packet为单位接收，现在本地缓存，然后写入目标文件","source":"_posts/Hdfs结构性能分析及读写流程.md","raw":"---\ntitle: Hdfs结构性能分析及读写流程\ndate: 2018年08月06日 22时15分52秒\ntags: [HDFS,原理,Hadoop]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n\n\n\n\n<!-- more -->\n\n# namenode和secondaryNameNode最好不要放在一台机器上\n\n宕机可能导致数据不能恢复 \n测试环境或者学习环境可以弄在一台机器上\n\n# hdfs中namenode和datanode的初始化\n\nhdfs会在配置文件中配置一个datanode的工作目录元数据 \n查看目录结构 tree hddata/ \n![img](file:///var/folders/9p/dbpbsyq158n7y12303j34hxm0000gp/T/WizNote/3172cdcb-77fe-4f72-8882-12fe40ddef6a/index_files/4ee70fb7-cfdc-4010-a387-986e268b5a1e.jpg) \ndatanode的工作目录是在datanode启动后初始化的 \n而hadoop namenode format 只会初始name的工作目录，和datanode没有关系\n\n# 把一个hdfs的一个节点加入到另一个集群\n\n必须要将hdfs datanode的工作目录删除，不然持有上一个集群的datanode的工作目录，会认为是一个误操作，为了防止丢失数据，不会让其连接上\n\n# 如果集群够大，上百台机器\n\n那么在hdfs上面，是需要配置机架感知\n\n# namenode管理元数据\n\nnamenode会将元数据放在内存里面，这样方便快速对数据的请求 \n但是放在内存中是不安全的，所有就序列化到fsimage里面 \n![img](file:///var/folders/9p/dbpbsyq158n7y12303j34hxm0000gp/T/WizNote/3172cdcb-77fe-4f72-8882-12fe40ddef6a/index_files/2d3df4da-f64b-4ccf-967b-9ade7c2ab46f.jpg) \n就像jvm中dump，将内存中所有数据dump出去 \n假如 \n![img](file:///var/folders/9p/dbpbsyq158n7y12303j34hxm0000gp/T/WizNote/3172cdcb-77fe-4f72-8882-12fe40ddef6a/index_files/8fa4e8c2-8b29-4809-8219-b383aa6d8664.jpg) \n所以存大文件划算，因为元数据消耗的内存都是一样的 \n![img](file:///var/folders/9p/dbpbsyq158n7y12303j34hxm0000gp/T/WizNote/3172cdcb-77fe-4f72-8882-12fe40ddef6a/index_files/2a0de522-13be-4777-b2f4-68377648bbb4.jpg) \n但是内存中的数据量太大，不可能经常序列化，所以需要定时序列化\n\n## 所以引入了secondaryNameNode\n\n更新元数据的时候，不可能去直接跟更改元数据fsimage文件，因为文件是线性结构，假如遇到更改中间内容会很不方便，所有就将操作信息记录在edits日志文件中，只是记录操作信息\n\n### edits文件定期转为元数据\n\n为了防止edits过多，导致在启动hdfs集群datanode的时候会很慢，因为需要将edits通过转化形成为元数据fsimage文件，所以应该定期将edits文件转换为fsimage元数据，然后将fsimage替换掉\n\n### secondaryNameNode的出现\n\n如果nameNode来做上面的edits转换为元数据的话，由于消耗的资源太大，就不能为其他比如从hdfs中读取数据服务提供资源，或者提供服务的效果不好 \n所以这个时候就把合并操作交给secondNameNode来做 \n这个过程叫做checkpoint \n![img](file:///var/folders/9p/dbpbsyq158n7y12303j34hxm0000gp/T/WizNote/3172cdcb-77fe-4f72-8882-12fe40ddef6a/index_files/d6a7699e-3158-4cfc-ab83-9fec25286004.jpg)\n\n## 为了防止namenode宕机导致了数据丢失\n\n可以在hdfs-site.xml文件中在多个机器上的目录来保存name的edits，fsimage文件\n\n `<property>``    <name>dfs.name.dir</name>``    <value>/home/bigdata/names1,/home/bigdata/names2</value>``</property>`\n\n配置的多个的话，会同时往这两个目录中写\n\n如果不配置这个默认的目录是core-site.xml文件中配置的hadoop的临时文件 \n ` \n <property> \n​       <name>hadoop.tmp.dir</name> \n​       <value>/home/bigdata/apps/hadoop-2.6.4/tmp</value> \n   </property> `\n\n# Hdfs的结构\n\n1.HDFS集群分为两大角色：NameNode、DataNode （secondary NameNode）\n2.NameNode负责管理整个文件系统的元数据\n记录文件在哪里\n3.DataNode 负责管理用户的文件数据块\n不负责切块，负责保管\n4.文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上\n5.每一个文件块可以有多个副本，并存放在不同的datanode上\n副本不会放在同一个机器上，因为副本就是防止宕机，\n6.Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量\n因为datanode如果宕机的话，name该机器上的对应的副本数据将会消失，这样需要将其在其他机器上进行恢复，恢复的话，就需要上面就需要数据和未宕机时的数据尽量保持一致，所以需要依赖于datanode定期汇报，不然差距的数据会很大\n7.HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行\n\n\n\n## Hdfs写操作\n\n 详细步骤解析\n\n1、根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在\n\n不存在则会返回path not exist异常\n\n2、namenode返回是否可以上传\n\n3、client请求第一个 block（0-128m）该传输到哪些datanode服务器上\n\n返回该block存放的位置，及其副本的信息存放的位置\n\n4、namenode返回3个datanode服务器ABC\n\n副本选择策略\n\n考虑空间和距离的因素，网络跳转的跳数，比如说机架的位置，\n\n第一台是看谁比较近（机架），因为传输比较快，副本则是是看谁比较远，防止机架出问题（如断电），干扰性更小\n\n而集群全线崩塌\n\n5、client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端\n\n这样是防止整个流程变慢，同时创建通道，先建立通道pipeline,通道\n\n6、client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存bytebuf），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答\n\n \n\n因为等一个block写满之后再传送，速度会很慢，所以是接收一个packet就会写入到管道流pipeline中。\n\n只要上传一个成功，则客户端视为上传成功，因为如果没上传成功，namenode会进行异步的复制副本的信息\n\n7、当一个block传输完成之后，client再次请求**namenode**上传第二个block的服务器。\n\n注：写的过程中，namenode记录下来了文件路径，文件有几个block也记录下来了，每个block分配到哪些机器上也记录下到了，及其每个block的副本信息，副本在那几个机器上。\n\n校验的时候不是一个packet（一批chunk，共64k）校验，而是以一个chunk来校验，一个chunk是512byte（字节）\n\n# Hdfs读操作\n\n1、跟namenode通信查询元数据，找到文件块所在的datanode服务器\n\n2、挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流\n\n3、datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）\n\n4、客户端以packet为单位接收，现在本地缓存，然后写入目标文件","slug":"Hdfs结构性能分析及读写流程","published":1,"updated":"2018-08-16T15:48:44.374Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vt00162tpb76sn7zdy"},{"title":"Hive建表及sql相关","date":"2018-08-11T02:31:24.062Z","toc":true,"_content":"\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/0069RVTdgy1fu5k3varq8j31880g8js9.jpg)\n\n\n\nhive主要是做离线日志分析的，不是为了做单行的事务控制的数据\n新版hive也支持单行数据的读取，但是效率非常低，所以也没有什么updata语句\n\n<!-- more -->\n\n\nhdfs的数据是放在hdfs里面的，表的描述的结构元数据信息是放在mysql里面\nhdfs中数据的信息在以下类似目录\n**/user/hive/warehouse/thishive.db/book/country=japan**\n\n可以在hive的客户端直接敲用hdfs的命令查看到\n```\nhdfs dfs -ls /hive目录\n```\n\n## 本地模式\n\nset hive.exec.mode.local.auto=true;\n\n## 建表(默认是内部表)\n\n```sql\ncreate table inner_table(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by '\\t';\n```\n\n### 建分区表\n```sql\ncreate table outter_table(id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by '\\t';\n```\n\n### 建外部表\n\n```sql\ncreate external table td_ext(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by '\\t' location '/td_ext';\n```\n\nlocaltion是表示存放的位置\n\n## 复制表\n\n```\ncreate  table 表1 like 表2 ;\n```\n\n将表2的结构复制到表1\n将文件的数据导入到表中\n\n## 导入数据到表中\n\n   给表导入数据（若是分区表，则导入的时候需要加partition(#####)）\n\n```bash\nload data local inpath '/home/hadoop/mylog.log' into table 表名 partition(datestr='2013-09-18') ;\n```\n\n如果是导入本地文件，需要加参数local，如果是hdfs上的话，则不加\n\n## 导出hive表中数据\n\n```\ninsert overwrite local directory '/home/hadoop/student.txt'  select * from 表名;\n```\n\n> 不加local表示导出到hdfs\n\n## 外部表和内部表的区别\n\ndrop table 外部表； 只会将外部表的结构 元数据信息删除，而不会删除外表的数据\ndrop table 内部表；会将内部表的结构元数据信息及其数据信息全部删除\n\n# 保存select查询结果的几种方式：\n\n```\n1、将查询结果保存到一张新的hive表中\n\ncreate table t_tmp\nas\nselect * from t_p;\n\n2、将查询结果保存到一张已经存在的hive表中\n\ninsert into  table t_tmp \nselect * from t_p;\n\n3、将查询结果保存到指定的文件目录（可以是本地，也可以是hdfs）\n\n本地\ninsert overwrite local directory '/home/hadoop/student.txt'\n\nselect * from student;\n\n导入到mysql\ninsert overwrite directory '/aaa/test'\nselect * from t_p;\n\n\n\n```\n\n## 分区表\n\n普通表和分区表区别：有大量数据增加的需要建分区表\n分区的字段会自动加在表结构上\n\n这个是将导入到fruit的分区里面\n\n```bash\nload data local inpath '/home/bigdata/food.txt' overwrite into table book partition (type='fruit')；\n```\n\n\n在hdfs里面，分区表会存在多个不同的目录，但是在查询的时候，还是将多个分区表的信息融入到一个表中\n\n## 使用\n\n如果是使用overwrite命令，必须加stored as textfile；\n\n### 小操作\n\n以下资源来自网络（若有不合适，请联系我）\n\n### students.txt\n\n```\n95001,李勇,男,20,CS\n95002,刘晨,女,19,IS\n95003,王敏,女,22,MA\n95004,张立,男,19,IS\n95005,刘刚,男,18,MA\n95006,孙庆,男,23,CS\n95007,易思玲,女,19,MA\n95008,李娜,女,18,CS\n95009,梦圆圆,女,18,MA\n95010,孔小涛,男,19,CS\n95011,包小柏,男,18,MA\n95012,孙花,女,20,CS\n95013,冯伟,男,21,CS\n95014,王小丽,女,19,CS\n95015,王君,男,18,MA\n95016,钱国,男,21,MA\n95017,王风娟,女,18,IS\n95018,王一,女,19,IS\n95019,邢小丽,女,19,IS\n95020,赵钱,男,21,IS\n95021,周二,男,17,MA\n95022,郑明,男,20,MA\n```\n\n### sc.txt\n\n```\n95001,1,81\n95001,2,85\n95001,3,88\n95001,4,70\n95002,2,90\n95002,3,80\n95002,4,71\n95002,5,60\n95003,1,82\n95003,3,90\n95003,5,100\n95004,1,80\n95004,2,92\n95004,4,91\n95004,5,70\n95005,1,70\n95005,2,92\n95005,3,99\n95005,6,87\n95006,1,72\n95006,2,62\n95006,3,100\n95006,4,59\n95006,5,60\n95006,6,98\n95007,3,68\n95007,4,91\n95007,5,94\n95007,6,78\n95008,1,98\n95008,3,89\n95008,6,91\n95009,2,81\n95009,4,89\n95009,6,100\n95010,2,98\n95010,5,90\n95010,6,80\n95011,1,81\n95011,2,91\n95011,3,81\n95011,4,86\n95012,1,81\n95012,3,78\n95012,4,85\n95012,6,98\n95013,1,98\n95013,2,58\n95013,4,88\n95013,5,93\n95014,1,91\n95014,2,100\n95014,4,98\n95015,1,91\n95015,3,59\n95015,4,100\n95015,6,95\n95016,1,92\n95016,2,99\n95016,4,82\n95017,4,82\n95017,5,100\n95017,6,58\n95018,1,95\n95018,2,100\n95018,3,67\n95018,4,78\n95019,1,77\n95019,2,90\n95019,3,91\n95019,4,67\n95019,5,87\n95020,1,66\n95020,2,99\n95020,5,93\n95021,2,93\n95021,5,91\n95021,6,99\n95022,3,69\n95022,4,93\n95022,5,82\n95022,6,100\n```\n\n\n\n### course.txt\n\n```\n1,数据库\n2,数学\n3,信息系统\n4,操作系统\n5,数据结构\n6,数据处理\n```\n\n### 建表\n\n```sql\ncreate table student(Sno int,Sname string,Sex string,Sage int,Sdept string)row format delimited fields terminated by ','stored as textfile;\ncreate table course(Cno int,Cname string) row format delimited fields terminated by ',' stored as textfile;\ncreate table sc(Sno int,Cno int,Grade int)row format delimited fields terminated by ',' stored as textfile;\n\nload data local inpath '/home/bigdata/apps/hive/hivedata/students.txt' overwrite into table student;\nload data local inpath '/home/bigdata/apps/hive/hivedata/sc.txt' overwrite into table sc;\nload data local inpath '/home/bigdata/apps/hive/hivedata/course.txt' overwrite into table course;\n```\n### sql需求\n\n```sql\n查询全体学生的学号与姓名\n　　hive> select Sno,Sname from student;\n\n查询选修了课程的学生姓名\n　　hive> select distinct Sname from student inner join sc on student.Sno=Sc.Sno;\n\n----hive的group by 和集合函数\n\n查询学生的总人数\n　　hive> select count(distinct Sno)count from student;\n\n计算1号课程的学生平均成绩\n　　hive> select avg(distinct Grade) from sc where Cno=1;\n查询各科成绩平均分\n\t\thive> select Cno,avg(Grade) from sc group by Cno;  \n查询选修1号课程的学生最高分数\n　　select Grade from sc where Cno=1 sort by Grade desc limit 1; \n(注意比较:select * from sc where Cno=1 sort by Grade\n\t\t  select Grade from sc where Cno=1 order by Grade)     \n　　   \n　　\n求各个课程号及相应的选课人数 \n　　hive> select Cno,count(1) from sc group by Cno;\n\n\n查询选修了3门以上的课程的学生学号\n　　hive> select Sno from (select Sno,count(Cno) CountCno from sc group by Sno)a where a.CountCno>3;\n或　hive> select Sno from sc group by Sno having count(Cno)>3; \n\n----hive的Order By/Sort By/Distribute By\n　　Order By ，在strict 模式下（hive.mapred.mode=strict),order by 语句必须跟着limit语句，但是在nonstrict下就不是必须的，这样做的理由是必须有一个reduce对最终的结果进行排序，如果最后输出的行数过多，一个reduce需要花费很长的时间。\n\n查询学生信息，结果按学号全局有序\n　　hive> set hive.mapred.mode=strict;   <默认nonstrict>\nhive> select Sno from student order by Sno;\nFAILED: Error in semantic analysis: 1:33 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'Sno'\n　　Sort By，它通常发生在每一个redcue里，“order by” 和“sort by”的区别在于，前者能给保证输出都是有顺序的，而后者如果有多个reduce的时候只是保证了输出的部分有序。set mapred.reduce.tasks=<number>在sort by可以指定，在用sort by的时候，如果没有指定列，它会随机的分配到不同的reduce里去。distribute by 按照指定的字段对数据进行划分到不同的输出reduce中 \n　　此方法会根据性别划分到不同的reduce中 ，然后按年龄排序并输出到不同的文件中。\n\n查询学生信息，按性别分区，在分区内按年龄有序\n　　hive> set mapred.reduce.tasks=2;\n　　hive> insert overwrite local directory '/home/hadoop/out' \nselect * from student distribute by Sex sort by Sage;\n\n----Join查询,join只支持等值连接 \n查询每个学生及其选修课程的情况\n　　hive> select student.*,sc.* from student join sc on (student.Sno =sc.Sno);\n查询学生的得分情况。\n　　hive>select student.Sname,course.Cname,sc.Grade from student join sc on student.Sno=sc.Sno join course on sc.cno=course.cno;\n\n查询选修2号课程且成绩在90分以上的所有学生。\n　　hive> select student.Sname,sc.Grade from student join sc on student.Sno=sc.Sno \nwhere  sc.Cno=2 and sc.Grade>90;\n　　\n----LEFT，RIGHT 和 FULL OUTER JOIN ,inner join, left semi join\n查询所有学生的信息，如果在成绩表中有成绩，则输出成绩表中的课程号\n　　hive> select student.Sname,sc.Cno from student left outer join sc on student.Sno=sc.Sno;\n　　如果student的sno值对应的sc在中没有值，则会输出student.Sname null.如果用right out join会保留右边的值，左边的为null。\n　　Join 发生在WHERE 子句之前。如果你想限制 join 的输出，应该在 WHERE 子句中写过滤条件——或是在join 子句中写。\n　　\n----LEFT SEMI JOIN  Hive 当前没有实现 IN/EXISTS 子查询，可以用 LEFT SEMI JOIN 重写子查询语句\n\n重写以下子查询为LEFT SEMI JOIN\n  SELECT a.key, a.value\n  FROM a\n  WHERE a.key exist in\n   (SELECT b.key\n    FROM B);\n可以被重写为：\n   SELECT a.key, a.val\n   FROM a LEFT SEMI JOIN b on (a.key = b.key)\n\n查询与“刘晨”在同一个系学习的学生\n　　hive> select s1.Sname from student s1 left semi join student s2 on s1.Sdept=s2.Sdept and s2.Sname='刘晨';\n\n注意比较：\nselect * from student s1 left join student s2 on s1.Sdept=s2.Sdept and s2.Sname='刘晨';\nselect * from student s1 right join student s2 on s1.Sdept=s2.Sdept and s2.Sname='刘晨';\nselect * from student s1 inner join student s2 on s1.Sdept=s2.Sdept and s2.Sname='刘晨';\nselect * from student s1 left semi join student s2 on s1.Sdept=s2.Sdept and s2.Sname='刘晨';\nselect s1.Sname from student s1 right semi join student s2 on s1.Sdept=s2.Sdept and s2.Sname='刘晨';\n```\n\n","source":"_posts/Hive sql相关.md","raw":"---\ntitle: Hive建表及sql相关\ndate: 2018年08月06日 22时15分52秒\ntags: [Hive,使用]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/0069RVTdgy1fu5k3varq8j31880g8js9.jpg)\n\n\n\nhive主要是做离线日志分析的，不是为了做单行的事务控制的数据\n新版hive也支持单行数据的读取，但是效率非常低，所以也没有什么updata语句\n\n<!-- more -->\n\n\nhdfs的数据是放在hdfs里面的，表的描述的结构元数据信息是放在mysql里面\nhdfs中数据的信息在以下类似目录\n**/user/hive/warehouse/thishive.db/book/country=japan**\n\n可以在hive的客户端直接敲用hdfs的命令查看到\n```\nhdfs dfs -ls /hive目录\n```\n\n## 本地模式\n\nset hive.exec.mode.local.auto=true;\n\n## 建表(默认是内部表)\n\n```sql\ncreate table inner_table(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by '\\t';\n```\n\n### 建分区表\n```sql\ncreate table outter_table(id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by '\\t';\n```\n\n### 建外部表\n\n```sql\ncreate external table td_ext(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by '\\t' location '/td_ext';\n```\n\nlocaltion是表示存放的位置\n\n## 复制表\n\n```\ncreate  table 表1 like 表2 ;\n```\n\n将表2的结构复制到表1\n将文件的数据导入到表中\n\n## 导入数据到表中\n\n   给表导入数据（若是分区表，则导入的时候需要加partition(#####)）\n\n```bash\nload data local inpath '/home/hadoop/mylog.log' into table 表名 partition(datestr='2013-09-18') ;\n```\n\n如果是导入本地文件，需要加参数local，如果是hdfs上的话，则不加\n\n## 导出hive表中数据\n\n```\ninsert overwrite local directory '/home/hadoop/student.txt'  select * from 表名;\n```\n\n> 不加local表示导出到hdfs\n\n## 外部表和内部表的区别\n\ndrop table 外部表； 只会将外部表的结构 元数据信息删除，而不会删除外表的数据\ndrop table 内部表；会将内部表的结构元数据信息及其数据信息全部删除\n\n# 保存select查询结果的几种方式：\n\n```\n1、将查询结果保存到一张新的hive表中\n\ncreate table t_tmp\nas\nselect * from t_p;\n\n2、将查询结果保存到一张已经存在的hive表中\n\ninsert into  table t_tmp \nselect * from t_p;\n\n3、将查询结果保存到指定的文件目录（可以是本地，也可以是hdfs）\n\n本地\ninsert overwrite local directory '/home/hadoop/student.txt'\n\nselect * from student;\n\n导入到mysql\ninsert overwrite directory '/aaa/test'\nselect * from t_p;\n\n\n\n```\n\n## 分区表\n\n普通表和分区表区别：有大量数据增加的需要建分区表\n分区的字段会自动加在表结构上\n\n这个是将导入到fruit的分区里面\n\n```bash\nload data local inpath '/home/bigdata/food.txt' overwrite into table book partition (type='fruit')；\n```\n\n\n在hdfs里面，分区表会存在多个不同的目录，但是在查询的时候，还是将多个分区表的信息融入到一个表中\n\n## 使用\n\n如果是使用overwrite命令，必须加stored as textfile；\n\n### 小操作\n\n以下资源来自网络（若有不合适，请联系我）\n\n### students.txt\n\n```\n95001,李勇,男,20,CS\n95002,刘晨,女,19,IS\n95003,王敏,女,22,MA\n95004,张立,男,19,IS\n95005,刘刚,男,18,MA\n95006,孙庆,男,23,CS\n95007,易思玲,女,19,MA\n95008,李娜,女,18,CS\n95009,梦圆圆,女,18,MA\n95010,孔小涛,男,19,CS\n95011,包小柏,男,18,MA\n95012,孙花,女,20,CS\n95013,冯伟,男,21,CS\n95014,王小丽,女,19,CS\n95015,王君,男,18,MA\n95016,钱国,男,21,MA\n95017,王风娟,女,18,IS\n95018,王一,女,19,IS\n95019,邢小丽,女,19,IS\n95020,赵钱,男,21,IS\n95021,周二,男,17,MA\n95022,郑明,男,20,MA\n```\n\n### sc.txt\n\n```\n95001,1,81\n95001,2,85\n95001,3,88\n95001,4,70\n95002,2,90\n95002,3,80\n95002,4,71\n95002,5,60\n95003,1,82\n95003,3,90\n95003,5,100\n95004,1,80\n95004,2,92\n95004,4,91\n95004,5,70\n95005,1,70\n95005,2,92\n95005,3,99\n95005,6,87\n95006,1,72\n95006,2,62\n95006,3,100\n95006,4,59\n95006,5,60\n95006,6,98\n95007,3,68\n95007,4,91\n95007,5,94\n95007,6,78\n95008,1,98\n95008,3,89\n95008,6,91\n95009,2,81\n95009,4,89\n95009,6,100\n95010,2,98\n95010,5,90\n95010,6,80\n95011,1,81\n95011,2,91\n95011,3,81\n95011,4,86\n95012,1,81\n95012,3,78\n95012,4,85\n95012,6,98\n95013,1,98\n95013,2,58\n95013,4,88\n95013,5,93\n95014,1,91\n95014,2,100\n95014,4,98\n95015,1,91\n95015,3,59\n95015,4,100\n95015,6,95\n95016,1,92\n95016,2,99\n95016,4,82\n95017,4,82\n95017,5,100\n95017,6,58\n95018,1,95\n95018,2,100\n95018,3,67\n95018,4,78\n95019,1,77\n95019,2,90\n95019,3,91\n95019,4,67\n95019,5,87\n95020,1,66\n95020,2,99\n95020,5,93\n95021,2,93\n95021,5,91\n95021,6,99\n95022,3,69\n95022,4,93\n95022,5,82\n95022,6,100\n```\n\n\n\n### course.txt\n\n```\n1,数据库\n2,数学\n3,信息系统\n4,操作系统\n5,数据结构\n6,数据处理\n```\n\n### 建表\n\n```sql\ncreate table student(Sno int,Sname string,Sex string,Sage int,Sdept string)row format delimited fields terminated by ','stored as textfile;\ncreate table course(Cno int,Cname string) row format delimited fields terminated by ',' stored as textfile;\ncreate table sc(Sno int,Cno int,Grade int)row format delimited fields terminated by ',' stored as textfile;\n\nload data local inpath '/home/bigdata/apps/hive/hivedata/students.txt' overwrite into table student;\nload data local inpath '/home/bigdata/apps/hive/hivedata/sc.txt' overwrite into table sc;\nload data local inpath '/home/bigdata/apps/hive/hivedata/course.txt' overwrite into table course;\n```\n### sql需求\n\n```sql\n查询全体学生的学号与姓名\n　　hive> select Sno,Sname from student;\n\n查询选修了课程的学生姓名\n　　hive> select distinct Sname from student inner join sc on student.Sno=Sc.Sno;\n\n----hive的group by 和集合函数\n\n查询学生的总人数\n　　hive> select count(distinct Sno)count from student;\n\n计算1号课程的学生平均成绩\n　　hive> select avg(distinct Grade) from sc where Cno=1;\n查询各科成绩平均分\n\t\thive> select Cno,avg(Grade) from sc group by Cno;  \n查询选修1号课程的学生最高分数\n　　select Grade from sc where Cno=1 sort by Grade desc limit 1; \n(注意比较:select * from sc where Cno=1 sort by Grade\n\t\t  select Grade from sc where Cno=1 order by Grade)     \n　　   \n　　\n求各个课程号及相应的选课人数 \n　　hive> select Cno,count(1) from sc group by Cno;\n\n\n查询选修了3门以上的课程的学生学号\n　　hive> select Sno from (select Sno,count(Cno) CountCno from sc group by Sno)a where a.CountCno>3;\n或　hive> select Sno from sc group by Sno having count(Cno)>3; \n\n----hive的Order By/Sort By/Distribute By\n　　Order By ，在strict 模式下（hive.mapred.mode=strict),order by 语句必须跟着limit语句，但是在nonstrict下就不是必须的，这样做的理由是必须有一个reduce对最终的结果进行排序，如果最后输出的行数过多，一个reduce需要花费很长的时间。\n\n查询学生信息，结果按学号全局有序\n　　hive> set hive.mapred.mode=strict;   <默认nonstrict>\nhive> select Sno from student order by Sno;\nFAILED: Error in semantic analysis: 1:33 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'Sno'\n　　Sort By，它通常发生在每一个redcue里，“order by” 和“sort by”的区别在于，前者能给保证输出都是有顺序的，而后者如果有多个reduce的时候只是保证了输出的部分有序。set mapred.reduce.tasks=<number>在sort by可以指定，在用sort by的时候，如果没有指定列，它会随机的分配到不同的reduce里去。distribute by 按照指定的字段对数据进行划分到不同的输出reduce中 \n　　此方法会根据性别划分到不同的reduce中 ，然后按年龄排序并输出到不同的文件中。\n\n查询学生信息，按性别分区，在分区内按年龄有序\n　　hive> set mapred.reduce.tasks=2;\n　　hive> insert overwrite local directory '/home/hadoop/out' \nselect * from student distribute by Sex sort by Sage;\n\n----Join查询,join只支持等值连接 \n查询每个学生及其选修课程的情况\n　　hive> select student.*,sc.* from student join sc on (student.Sno =sc.Sno);\n查询学生的得分情况。\n　　hive>select student.Sname,course.Cname,sc.Grade from student join sc on student.Sno=sc.Sno join course on sc.cno=course.cno;\n\n查询选修2号课程且成绩在90分以上的所有学生。\n　　hive> select student.Sname,sc.Grade from student join sc on student.Sno=sc.Sno \nwhere  sc.Cno=2 and sc.Grade>90;\n　　\n----LEFT，RIGHT 和 FULL OUTER JOIN ,inner join, left semi join\n查询所有学生的信息，如果在成绩表中有成绩，则输出成绩表中的课程号\n　　hive> select student.Sname,sc.Cno from student left outer join sc on student.Sno=sc.Sno;\n　　如果student的sno值对应的sc在中没有值，则会输出student.Sname null.如果用right out join会保留右边的值，左边的为null。\n　　Join 发生在WHERE 子句之前。如果你想限制 join 的输出，应该在 WHERE 子句中写过滤条件——或是在join 子句中写。\n　　\n----LEFT SEMI JOIN  Hive 当前没有实现 IN/EXISTS 子查询，可以用 LEFT SEMI JOIN 重写子查询语句\n\n重写以下子查询为LEFT SEMI JOIN\n  SELECT a.key, a.value\n  FROM a\n  WHERE a.key exist in\n   (SELECT b.key\n    FROM B);\n可以被重写为：\n   SELECT a.key, a.val\n   FROM a LEFT SEMI JOIN b on (a.key = b.key)\n\n查询与“刘晨”在同一个系学习的学生\n　　hive> select s1.Sname from student s1 left semi join student s2 on s1.Sdept=s2.Sdept and s2.Sname='刘晨';\n\n注意比较：\nselect * from student s1 left join student s2 on s1.Sdept=s2.Sdept and s2.Sname='刘晨';\nselect * from student s1 right join student s2 on s1.Sdept=s2.Sdept and s2.Sname='刘晨';\nselect * from student s1 inner join student s2 on s1.Sdept=s2.Sdept and s2.Sname='刘晨';\nselect * from student s1 left semi join student s2 on s1.Sdept=s2.Sdept and s2.Sname='刘晨';\nselect s1.Sname from student s1 right semi join student s2 on s1.Sdept=s2.Sdept and s2.Sname='刘晨';\n```\n\n","slug":"Hive sql相关","published":1,"updated":"2018-08-30T04:00:08.449Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vu001a2tpb8mbb5zaz"},{"title":"Hive分桶表,分区表简单分析","date":"2018-08-11T03:21:32.532Z","toc":true,"_content":"\n[TOC]\n\n![](https://ws2.sinaimg.cn/large/0069RVTdgy1fu81tzpekzj305z06j3yc.jpg)\n\n对于每一个表或者是分区，Hive 可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive 是针对某一列进行分桶。Hive 采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶中。分桶的好处是可以获得更高的查询处理效率。使取样更高效。\n\n分桶依赖于yarn的所以分桶的时候需要启动yarn\n\n<!-- more -->\n\n# 分桶表创建\n\n\\#设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数\n\n```sql\nset hive.enforce.bucketing = true;\nset mapreduce.job.reduces=4;\n```\n\n创建表\n\n```sql\ncreate table person_buck(id int,name string,sex string,age int)\nclustered by(id) \nsorted by(id DESC)\ninto 4 buckets\nrow format delimited\nfields terminated by ',';\n```\n\n\n\n```\n开会往创建的分桶表插入数据(插入数据需要是已分桶, 且排序的)\n可以使用distribute by(id) sort by(id asc)  或是排序和分桶的字段相同的时候使用Cluster by(字段)\n注意使用cluster by  就等同于分桶+排序(sort)\n```\n\n\n\n```sql\ninsert into table person_buck select id,name,sex,age from student distribute by(id) sort by(id asc);\n```\n\n\n\n\n\n\n\n## 分桶模式的参数\n\n设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数\n\nset hive.enforce.bucketing = true;\n\n不设置reduce的数量会使用默认的数量，默认的数量会和分桶的数量不一致，则不能分出正确分桶\n\nset mapreduce.job.reduces=4;\n\n本地模式\n\nset hive.exec.mode.local.auto=true\n\n动态分区\n\n--设置为true表示开启动态分区功能（默认为false）\n\nset hive.exec.dynamic.partition=true;\n\n--设置为nonstrict,表示允许所有分区都是动态的（默认为strict）\n\nset hive.exec.dynamic.partition.mode=nonstrict;\n\n\n\n## Update与分桶表关系\n\nHive对使用Update功能的表有特定的语法要求, 语法要求如下:\n(1)要执行Update的表中, 建表时必须带有buckets(分桶)属性\n(2)要执行Update的表中, 需要指定格式,其余格式目前赞不支持, 如:parquet格式, 目前只支持ORCFileformat和AcidOutputFormat\n(3)要执行Update的表中, 建表时必须指定参数('transactional' = true);\n举例:\n\n```\ncreate table student (id bigint,name string) clustered by (name) into 2 buckets stored as orc TBLPROPERTIES('transactional'='true');\n```\n\n\n## 更新语句:\n\n```\nupdate student set id='444' where name='tom';\n```\n\n\n\n# 分桶表测试\n\n这个例子就是将分区的字段进行hash散列将数据分桶到分桶数个文件中去\n\n导入一个文件到分桶表里面\n\n##  创建表\n\n```sql\ncreate table t_buk(id int,name string) clustered by(id)  sorted by(id DESC) into 4 buckets row format delimited``fields terminated by ',';\n```\n\n## 创建数据\n\n**cd /usr/hive/hivedata/**\n\n**vim buk.txt** \n\n```\n1,数据库\n2,数学\n3,信息系统\n4,操作系统\n5,数据结构\n6,数据no\n7,数据other\n8,数据time\n9,数据操作\n10,数据挖掘\n11,数据挖机\n12,数据信号\n```\n\n\n\n## 读取本地文件\n\n```\nload data local inpath '/usr/hive/hivedata/buk.txt' into table t_buk;\n```\n\nload方式这样导入数据到一个分桶表里面，是不会作出分桶的操作的，不会分成桶数个文件，还是一个文件在hdfs系统中\n\n# 注意\n\n1. 要想导入到数据到分桶表里面，必须是一个是已经是分桶的数据，比如已经形成了分桶数据个文件，才可以导入到分桶表里面，导数据的时候是不会将原来的数据形式变成分桶的数据形式\n\n# hive分桶表的使用场景\n\n所以一般是在一个表中查询了数据然后在塞入到一个分区表里面，查询是走mapReduce程序，然后将数据按分桶表照分桶的策略写入到分桶表中\n\n形如\n\n```sql\ninsert into t_buk select * from other … …;\n```\n\n后面的\n\n清除数据\n\n```sql\ntruncate table t_buk;\n```\n\n创建一个表来读取数据\n\n```sql\ncreate table t_p(id int,name string)\nrow format delimited\nfields terminated by ',';\n\nload data local inpath '/usr/hive/hivedata/buk.txt' into table t_p;\n```\n\n insert into table t_buk select id,name from t_p; \n\n insert overwirte 也可以\n\n## 结果如下\n\n```\nNumber of reduce tasks is set to 0 since there's no reduce operator\nINFO  : number of splits:1\nINFO  : Submitting tokens for job: job_1502537431423_0011\nINFO  : The url to track the job: http://bigdata1:8088/proxy/application_1502537431423_0011/\nINFO  : Starting Job = job_1502537431423_0011, Tracking URL = http://bigdata1:8088/proxy/application_1502537431423_0011/\nINFO  : Kill Command = /home/bigdata/apps/hadoop/bin/hadoop job  -kill job_1502537431423_0011\nINFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\nINFO  : 2017-08-15 21:12:15,669 Stage-1 map = 0%,  reduce = 0%\nINFO  : 2017-08-15 21:12:32,386 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.27 sec\nINFO  : MapReduce Total cumulative CPU time: 1 seconds 270 msec\nINFO  : Ended Job = job_1502537431423_0011\nINFO  : Stage-4 is selected by condition resolver.\nINFO  : Stage-3 is filtered out by condition resolver.\nINFO  : Stage-5 is filtered out by condition resolver.\nINFO  : Moving data to: hdfs://bigdata1:9000/user/hive/warehouse/t_buk/.hive-staging_hive_2017-08-15_21-12-02_490_4088487413275551800-3/-ext-10000 from hdfs://bigdata1:9000/user/hive/warehouse/t_buk/.hive-staging_hive_2017-08-15_21-12-02_490_4088487413275551800-3/-ext-10002\nINFO  : Loading data to table default.t_buk from hdfs://bigdata1:9000/user/hive/warehouse/t_buk/.hive-staging_hive_2017-08-15_21-12-02_490_4088487413275551800-3/-ext-10000\nINFO  : Table default.t_buk stats: [numFiles=1, numRows=12, totalSize=167, rawDataSize=155]\n```\n\n查看hdfs管理页面 50070\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu7ufetxnpj30yw06fmx9.jpg)\n\n 还是只有一文件，表示分桶不成功，没设reduce数量，使用默认的数量1，和我们期望分桶数量不一致\n\n# 设置分桶参数\n\n因为没有启动模式的开关，如下\n\n设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数\n\n```\nset hive.enforce.bucketing = true;\nset mapreduce.job.reduces=4;\nset hive.exec.mode.local.auto=true;\nset hive.exec.dynamic.partition=true;\nset hive.exec.dynamic.partition.mode=nonstrict;\n```\n\n## 重新创建表\n\n\n\n可以通过set hive.enforce.bucketing查看是否设置成功\n\n先查看sort by (id)；\n\n根据4个reduce来局部有序，每个reduce有序，但是从哪儿截断每个reduce并不确定\n\n```\nselect id,name from t_p sort by (id);\nINFO  : Number of reduce tasks not specified. Defaulting to jobconf value of: 4\nINFO  : In order to change the average load for a reducer (in bytes):\nINFO  :   set hive.exec.reducers.bytes.per.reducer=<number>\nINFO  : In order to limit the maximum number of reducers:\nINFO  :   set hive.exec.reducers.max=<number>\nINFO  : In order to set a constant number of reducers:\nINFO  :   set mapreduce.job.reduces=<number>\nINFO  : number of splits:1\nINFO  : Submitting tokens for job: job_1502537431423_0012\nINFO  : The url to track the job: http://bigdata1:8088/proxy/application_1502537431423_0012/\nINFO  : Starting Job = job_1502537431423_0012, Tracking URL = http://bigdata1:8088/proxy/application_1502537431423_0012/\nINFO  : Kill Command = /home/bigdata/apps/hadoop/bin/hadoop job  -kill job_1502537431423_0012\nINFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 4\nINFO  : 2017-08-15 21:26:14,284 Stage-1 map = 0%,  reduce = 0%\nINFO  : 2017-08-15 21:26:24,633 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.29 sec\nINFO  : 2017-08-15 21:26:38,745 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 6.99 sec\nINFO  : 2017-08-15 21:26:43,887 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 6.99 sec\nINFO  : 2017-08-15 21:26:46,970 Stage-1 map = 100%,  reduce = 75%, Cumulative CPU 9.06 sec\nINFO  : 2017-08-15 21:26:50,081 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.8 sec\nINFO  : MapReduce Total cumulative CPU time: 10 seconds 800 msec\nINFO  : Ended Job = job_1502537431423_0012\n+-----+----------+--+\n| id  |   name   |\n+-----+----------+--+\n| 4   | 操作系统     |\n| 8   | 数据time   |\n| 12  | 数据信号     |\n| 2   | 数学       |\n| 6   | 数据no     |\n| 1   | 数据库      |\n| 3   | 信息系统     |\n| 5   | 数据结构     |\n| 10  | 数据挖掘     |\n| 11  | 数据挖机     |\n| 7   | 数据other  |\n| 9   | 数据操作     |\n+-----+----------+--+\n```\n\n\n\n再试一次select 插入（将t_buk truncate也可，也可使用overwrite关键字）\n\n `insert overwrite table t_buk select id,name from t_p cluster by (id);`\n\n再查看hdfs ui页面50070\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu7uk05fz5j30x70a8mxm.jpg)\n\n## 再分别查看这几个文件\n\n```\n$HADOOP_HOME/bin/hadoop fs -cat /user/hive/warehouse/t_buk/000000_0\n$HADOOP_HOME/bin/hadoop fs -cat /user/hive/warehouse/t_buk/000001_0  \n$HADOOP_HOME/bin/hadoop fs -cat /user/hive/warehouse/t_buk/000002_0  \n$HADOOP_HOME/bin/hadoop fs -cat /user/hive/warehouse/t_buk/000003_0\n```\n\n## 得到结果\n\n```\n[bigdata@master hivedata]$  hadoop fs -cat /user/hive/warehouse/t_buk/000000_0  \n4,操作系统\n8,数据time\n12,数据信号\n[bigdata@master hivedata]$  hadoop fs -cat /user/hive/warehouse/t_buk/000001_0  \n1,数据库\n5,数据结构\n9,数据操作\n[bigdata@master hivedata]$  hadoop fs -cat /user/hive/warehouse/t_buk/000002_0  \n2,数学\n6,数据no\n10,数据挖掘\n[bigdata@master hivedata]$  hadoop fs -cat /user/hive/warehouse/t_buk/000003_0\n3,信息系统\n7,数据other\n11,数据挖机\n```\n\n# 分桶表疑问\n\n## 为什么每个桶里面的数据条数不一样\n\n```\nhash散列的时候数据可能将数据有的分的多，有的分的少\n```\n\ncluster by （id） 根据id分桶，桶内根据id排序，相当于 distribute by 和 sort by的集合，只是指定的字段都是同一个\n用两个组合更加强大，分桶字段排序字段可以设置为不同\n\n# 分桶表的意义：\n\n提高join操作的效率案例\n\n> 如果a表和b表已经是分桶表，而且分桶的字段都是是id字段\n> 做这个join操作是，还需要做笛卡尔积吗？ 这样不需要，因为同一id哈希后的数据是一致的，这就是分桶表存在的意义\n\n# 注意\n\n1. 在分桶表中使用order by 是非常不建议的，这样会设置成一个reduce，强行将数据写入，一个reduce的内存会爆炸\n2. 使用cluster by  就等同于分桶+排序(sort) \n\n insert overwrite table student_buck  select * from student cluster by(Sno) sort by(Sage);  报错,cluster 和 sort 不能共存\n\n","source":"_posts/Hive分桶表相关.md","raw":"---\ntitle: Hive分桶表,分区表简单分析\ndate: 2018年08月06日 22时15分52秒\ntags: [Hive]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n![](https://ws2.sinaimg.cn/large/0069RVTdgy1fu81tzpekzj305z06j3yc.jpg)\n\n对于每一个表或者是分区，Hive 可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive 是针对某一列进行分桶。Hive 采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶中。分桶的好处是可以获得更高的查询处理效率。使取样更高效。\n\n分桶依赖于yarn的所以分桶的时候需要启动yarn\n\n<!-- more -->\n\n# 分桶表创建\n\n\\#设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数\n\n```sql\nset hive.enforce.bucketing = true;\nset mapreduce.job.reduces=4;\n```\n\n创建表\n\n```sql\ncreate table person_buck(id int,name string,sex string,age int)\nclustered by(id) \nsorted by(id DESC)\ninto 4 buckets\nrow format delimited\nfields terminated by ',';\n```\n\n\n\n```\n开会往创建的分桶表插入数据(插入数据需要是已分桶, 且排序的)\n可以使用distribute by(id) sort by(id asc)  或是排序和分桶的字段相同的时候使用Cluster by(字段)\n注意使用cluster by  就等同于分桶+排序(sort)\n```\n\n\n\n```sql\ninsert into table person_buck select id,name,sex,age from student distribute by(id) sort by(id asc);\n```\n\n\n\n\n\n\n\n## 分桶模式的参数\n\n设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数\n\nset hive.enforce.bucketing = true;\n\n不设置reduce的数量会使用默认的数量，默认的数量会和分桶的数量不一致，则不能分出正确分桶\n\nset mapreduce.job.reduces=4;\n\n本地模式\n\nset hive.exec.mode.local.auto=true\n\n动态分区\n\n--设置为true表示开启动态分区功能（默认为false）\n\nset hive.exec.dynamic.partition=true;\n\n--设置为nonstrict,表示允许所有分区都是动态的（默认为strict）\n\nset hive.exec.dynamic.partition.mode=nonstrict;\n\n\n\n## Update与分桶表关系\n\nHive对使用Update功能的表有特定的语法要求, 语法要求如下:\n(1)要执行Update的表中, 建表时必须带有buckets(分桶)属性\n(2)要执行Update的表中, 需要指定格式,其余格式目前赞不支持, 如:parquet格式, 目前只支持ORCFileformat和AcidOutputFormat\n(3)要执行Update的表中, 建表时必须指定参数('transactional' = true);\n举例:\n\n```\ncreate table student (id bigint,name string) clustered by (name) into 2 buckets stored as orc TBLPROPERTIES('transactional'='true');\n```\n\n\n## 更新语句:\n\n```\nupdate student set id='444' where name='tom';\n```\n\n\n\n# 分桶表测试\n\n这个例子就是将分区的字段进行hash散列将数据分桶到分桶数个文件中去\n\n导入一个文件到分桶表里面\n\n##  创建表\n\n```sql\ncreate table t_buk(id int,name string) clustered by(id)  sorted by(id DESC) into 4 buckets row format delimited``fields terminated by ',';\n```\n\n## 创建数据\n\n**cd /usr/hive/hivedata/**\n\n**vim buk.txt** \n\n```\n1,数据库\n2,数学\n3,信息系统\n4,操作系统\n5,数据结构\n6,数据no\n7,数据other\n8,数据time\n9,数据操作\n10,数据挖掘\n11,数据挖机\n12,数据信号\n```\n\n\n\n## 读取本地文件\n\n```\nload data local inpath '/usr/hive/hivedata/buk.txt' into table t_buk;\n```\n\nload方式这样导入数据到一个分桶表里面，是不会作出分桶的操作的，不会分成桶数个文件，还是一个文件在hdfs系统中\n\n# 注意\n\n1. 要想导入到数据到分桶表里面，必须是一个是已经是分桶的数据，比如已经形成了分桶数据个文件，才可以导入到分桶表里面，导数据的时候是不会将原来的数据形式变成分桶的数据形式\n\n# hive分桶表的使用场景\n\n所以一般是在一个表中查询了数据然后在塞入到一个分区表里面，查询是走mapReduce程序，然后将数据按分桶表照分桶的策略写入到分桶表中\n\n形如\n\n```sql\ninsert into t_buk select * from other … …;\n```\n\n后面的\n\n清除数据\n\n```sql\ntruncate table t_buk;\n```\n\n创建一个表来读取数据\n\n```sql\ncreate table t_p(id int,name string)\nrow format delimited\nfields terminated by ',';\n\nload data local inpath '/usr/hive/hivedata/buk.txt' into table t_p;\n```\n\n insert into table t_buk select id,name from t_p; \n\n insert overwirte 也可以\n\n## 结果如下\n\n```\nNumber of reduce tasks is set to 0 since there's no reduce operator\nINFO  : number of splits:1\nINFO  : Submitting tokens for job: job_1502537431423_0011\nINFO  : The url to track the job: http://bigdata1:8088/proxy/application_1502537431423_0011/\nINFO  : Starting Job = job_1502537431423_0011, Tracking URL = http://bigdata1:8088/proxy/application_1502537431423_0011/\nINFO  : Kill Command = /home/bigdata/apps/hadoop/bin/hadoop job  -kill job_1502537431423_0011\nINFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\nINFO  : 2017-08-15 21:12:15,669 Stage-1 map = 0%,  reduce = 0%\nINFO  : 2017-08-15 21:12:32,386 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.27 sec\nINFO  : MapReduce Total cumulative CPU time: 1 seconds 270 msec\nINFO  : Ended Job = job_1502537431423_0011\nINFO  : Stage-4 is selected by condition resolver.\nINFO  : Stage-3 is filtered out by condition resolver.\nINFO  : Stage-5 is filtered out by condition resolver.\nINFO  : Moving data to: hdfs://bigdata1:9000/user/hive/warehouse/t_buk/.hive-staging_hive_2017-08-15_21-12-02_490_4088487413275551800-3/-ext-10000 from hdfs://bigdata1:9000/user/hive/warehouse/t_buk/.hive-staging_hive_2017-08-15_21-12-02_490_4088487413275551800-3/-ext-10002\nINFO  : Loading data to table default.t_buk from hdfs://bigdata1:9000/user/hive/warehouse/t_buk/.hive-staging_hive_2017-08-15_21-12-02_490_4088487413275551800-3/-ext-10000\nINFO  : Table default.t_buk stats: [numFiles=1, numRows=12, totalSize=167, rawDataSize=155]\n```\n\n查看hdfs管理页面 50070\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu7ufetxnpj30yw06fmx9.jpg)\n\n 还是只有一文件，表示分桶不成功，没设reduce数量，使用默认的数量1，和我们期望分桶数量不一致\n\n# 设置分桶参数\n\n因为没有启动模式的开关，如下\n\n设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数\n\n```\nset hive.enforce.bucketing = true;\nset mapreduce.job.reduces=4;\nset hive.exec.mode.local.auto=true;\nset hive.exec.dynamic.partition=true;\nset hive.exec.dynamic.partition.mode=nonstrict;\n```\n\n## 重新创建表\n\n\n\n可以通过set hive.enforce.bucketing查看是否设置成功\n\n先查看sort by (id)；\n\n根据4个reduce来局部有序，每个reduce有序，但是从哪儿截断每个reduce并不确定\n\n```\nselect id,name from t_p sort by (id);\nINFO  : Number of reduce tasks not specified. Defaulting to jobconf value of: 4\nINFO  : In order to change the average load for a reducer (in bytes):\nINFO  :   set hive.exec.reducers.bytes.per.reducer=<number>\nINFO  : In order to limit the maximum number of reducers:\nINFO  :   set hive.exec.reducers.max=<number>\nINFO  : In order to set a constant number of reducers:\nINFO  :   set mapreduce.job.reduces=<number>\nINFO  : number of splits:1\nINFO  : Submitting tokens for job: job_1502537431423_0012\nINFO  : The url to track the job: http://bigdata1:8088/proxy/application_1502537431423_0012/\nINFO  : Starting Job = job_1502537431423_0012, Tracking URL = http://bigdata1:8088/proxy/application_1502537431423_0012/\nINFO  : Kill Command = /home/bigdata/apps/hadoop/bin/hadoop job  -kill job_1502537431423_0012\nINFO  : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 4\nINFO  : 2017-08-15 21:26:14,284 Stage-1 map = 0%,  reduce = 0%\nINFO  : 2017-08-15 21:26:24,633 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.29 sec\nINFO  : 2017-08-15 21:26:38,745 Stage-1 map = 100%,  reduce = 50%, Cumulative CPU 6.99 sec\nINFO  : 2017-08-15 21:26:43,887 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 6.99 sec\nINFO  : 2017-08-15 21:26:46,970 Stage-1 map = 100%,  reduce = 75%, Cumulative CPU 9.06 sec\nINFO  : 2017-08-15 21:26:50,081 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.8 sec\nINFO  : MapReduce Total cumulative CPU time: 10 seconds 800 msec\nINFO  : Ended Job = job_1502537431423_0012\n+-----+----------+--+\n| id  |   name   |\n+-----+----------+--+\n| 4   | 操作系统     |\n| 8   | 数据time   |\n| 12  | 数据信号     |\n| 2   | 数学       |\n| 6   | 数据no     |\n| 1   | 数据库      |\n| 3   | 信息系统     |\n| 5   | 数据结构     |\n| 10  | 数据挖掘     |\n| 11  | 数据挖机     |\n| 7   | 数据other  |\n| 9   | 数据操作     |\n+-----+----------+--+\n```\n\n\n\n再试一次select 插入（将t_buk truncate也可，也可使用overwrite关键字）\n\n `insert overwrite table t_buk select id,name from t_p cluster by (id);`\n\n再查看hdfs ui页面50070\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu7uk05fz5j30x70a8mxm.jpg)\n\n## 再分别查看这几个文件\n\n```\n$HADOOP_HOME/bin/hadoop fs -cat /user/hive/warehouse/t_buk/000000_0\n$HADOOP_HOME/bin/hadoop fs -cat /user/hive/warehouse/t_buk/000001_0  \n$HADOOP_HOME/bin/hadoop fs -cat /user/hive/warehouse/t_buk/000002_0  \n$HADOOP_HOME/bin/hadoop fs -cat /user/hive/warehouse/t_buk/000003_0\n```\n\n## 得到结果\n\n```\n[bigdata@master hivedata]$  hadoop fs -cat /user/hive/warehouse/t_buk/000000_0  \n4,操作系统\n8,数据time\n12,数据信号\n[bigdata@master hivedata]$  hadoop fs -cat /user/hive/warehouse/t_buk/000001_0  \n1,数据库\n5,数据结构\n9,数据操作\n[bigdata@master hivedata]$  hadoop fs -cat /user/hive/warehouse/t_buk/000002_0  \n2,数学\n6,数据no\n10,数据挖掘\n[bigdata@master hivedata]$  hadoop fs -cat /user/hive/warehouse/t_buk/000003_0\n3,信息系统\n7,数据other\n11,数据挖机\n```\n\n# 分桶表疑问\n\n## 为什么每个桶里面的数据条数不一样\n\n```\nhash散列的时候数据可能将数据有的分的多，有的分的少\n```\n\ncluster by （id） 根据id分桶，桶内根据id排序，相当于 distribute by 和 sort by的集合，只是指定的字段都是同一个\n用两个组合更加强大，分桶字段排序字段可以设置为不同\n\n# 分桶表的意义：\n\n提高join操作的效率案例\n\n> 如果a表和b表已经是分桶表，而且分桶的字段都是是id字段\n> 做这个join操作是，还需要做笛卡尔积吗？ 这样不需要，因为同一id哈希后的数据是一致的，这就是分桶表存在的意义\n\n# 注意\n\n1. 在分桶表中使用order by 是非常不建议的，这样会设置成一个reduce，强行将数据写入，一个reduce的内存会爆炸\n2. 使用cluster by  就等同于分桶+排序(sort) \n\n insert overwrite table student_buck  select * from student cluster by(Sno) sort by(Sage);  报错,cluster 和 sort 不能共存\n\n","slug":"Hive分桶表相关","published":1,"updated":"2018-08-13T09:00:52.971Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vv001e2tpb79933r5e"},{"title":"Hive搭建及启动","date":"2018-07-19T18:51:21.978Z","toc":true,"_content":"\n[TOC]\n# 配置HOME\n\n下载hive包，并解压\n\n```\nhttp://archive.apache.org/dist/\n```\n\n`ln -s hive-2.1.1  /usr/hive`\n\n\n**vi ~/.bashrc**\n\n```bash\nexport PATH=$JAVA_HOME/bin:$PATH\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\n$HIVE_HOME/bin/hive\nexport HIVE_HOME=/usr/hive\nPATH=$HIVE_HOME/bin:$PATH\n#hive依赖于hadoop(可以不运行在同一主机，但是需要hadoop的配置)\n$HADOOP_HOME=/usr/hadoop\n```\n\n**source ~/.bashrc**\n\n<!--more -->\n\n# 安装mysql\n\n> ## Hive元数据介绍\n>\n> Hive 将元数据存储在 RDBMS 中，一般常用 MySQL 和 Derby。默认情况下，Hive 元数据保存在内嵌的 Derby 数据库中，只能允许一个会话连接，只适合简单的测试。实际生产环境中不适用， 为了支持多用户会话，则需要一个独立的元数据库，使用 MySQL 作为元数据库，Hive 内部对 MySQL 提供了很好的支持，配置一个独立的元数据库\n\n```bash\nyum install -y mysql-server\nchkconfig --add mysqld\nchkconfig mysqld on\nchkconfig --list mysqld\nservice mysqld start\n\nmysql -u root -p\nEnter password:           //默认密码为空，输入后回车即可\nset password for root@localhost=password('root'); 　　密码设置为root\nset password for root@=password('root');\n默认情况下Mysql只允许本地登录，所以只需配置root@localhost就好\n设置所有ip访问密码为root\nset password for root@%=password('root'); 　　　　　　密码设置为root （其实这一步可以不配）\n设置master访问密码为root\nset password for root@master=password('root'); 　　密码设置为root （其实这一步可以不配）\n查询密码\nselect user,host,password from mysql.user;  　　查看密码是否设置成功\n设置所有ip可以通过root访问\nGRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;\nGRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' IDENTIFIED BY 'hive' WITH GRANT OPTION;\n\nmysql -uroot -proot\ncreate user 'hive' identified by 'hive';\ncreate user 'hive'@'%' identified by 'hive';\n\ncreate database hive;\n```\n\n\n\n\n# 配置Hive\n\n`mkdir iotmp`\n\n`cp hive-default.xml.template hive-site.xml`\n\n`vim hive-site.xml`\n```xml\n<configuration>\n<property>\n    <name>javax.jdo.option.ConnectionDriverName</name>\n    <value>com.mysql.jdbc.Driver</value>\n    <description>Driver class name for a JDBC metastore</description>\n</property>\n\n<property>\n    <name>javax.jdo.option.ConnectionURL</name>\n    <value>jdbc:mysql://localhost:3306/hive?characterEncoding=UTF-8</value>\n    <description>JDBC connect string for a JDBC metastore</description>\n</property>\n\n<property>\n    <name>javax.jdo.option.ConnectionUserName</name>\n    <value>root</value>\n    <description>Username to use against metastore database</description>\n</property>\n\n<property>\n    <name>javax.jdo.option.ConnectionPassword</name>\n    <value>root</value>\n    <description>password to use against metastore database</description>\n</property>\n<property>\n    <name>hive.querylog.location</name>\n    <value>/usr/hive/iotmp</value>\n    <description>Location of Hive run time structured log file</description>\n</property>\n\n<property>\n    <name>hive.exec.local.scratchdir</name>\n    <value>/usr/hive/iotmp</value>\n    <description>Local scratch space for Hive jobs</description>\n</property>\n\n<property>\n    <name>hive.downloaded.resources.dir</name>\n    <value>/usr/hive/iotmp</value>\n    <description>Temporary local directory for added resources in the remote file system.</description>\n</property>\n<property>\n\t<name>hive.metastore.uris</name>\n\t<value>thrift://master:9083</value>\n</property>\n</configuration>\n```\nHive 的元数据可以存储在本地的 MySQL 中，但是大多数情况会是一个 mysql 集群，而且不在本地。所以在 hive 中需要开启远程 metastore。由于我是本地的 mysql，我就不配置下列属性了，但是如果是远程的 metastore，配置下面的属性。\n```xml\n<property>\n      <name>hive.metastore.uris</name>\n      <value></value>\n  <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>\n</property>\n<property>\n      <name>hive.server2.transport.mode</name>\n      <value>http</value>\n      <description>Server transport mode. \"binary\" or \"http\".</description>\n</property>\n\n链接：https://www.jianshu.com/p/87b76a686216\n```\n\n# Hive命令\n\n## 启动hive\n\n```bash\n$HIVE_HOME/bin/hive\n如果调试，可以加上参数\n$HIVE_HOME/bin/hivehive -hiveconf hive.root.logger=DEBUG,console\n```\n\n## 启动hiveserver2\n\n```bash\n$HIVE_HOME/bin/hive --service hiveserver2\n\nor nohup $HIVE_HOME/bin/hiveserver2 1>/var/log/hiveserver.log 2>/var/log/hiveserver.err &\n```\n\n> hiveserver端口号默认是10000\n\n**hiveserver2是否启动**\n`netstat -nl|grep 10000`\n\n## beeline工具测试使用jdbc方式连接\n\n可以在部署了hive任意节点上用beeline去连接\n\n```bash\n$HIVE_HOME/bin/beeline -u jdbc:hive2://hive:10000\nor\n$HIVE_HOME/bin/beeline -u jdbc:hive2://hive:10000 -n bigdata \n最后一个参数是用户\nor \n$HIVE_HOME/bin/beeline 回车，进入beeline的命令界面 \n输入命令连接hiveserver2 \nbeeline> !connect jdbc:hive2://hive:10000 \n```\n\n使用beeline通过jdbc连接上之后就可以像client一样操作。\n\nhiveserver2会同时启动一个webui，端口号默认为10002，可以通过http://localhost:10002/访问\n界面中可以看到Session/Query/Software等信息。(此网页只可查看，不可以操作hive数据仓库)\n\n> 参考https://blog.csdn.net/lblblblblzdx/article/details/79760959\n>\n> 参考https://www.cnblogs.com/netuml/p/7841387.html\n\n\n\nhive元数据库（就是hive库）中有几个表（有好几十个，但只需要记住其中几个就可以了），有哪些字段 \n元数据库最重要的作用是保存一些信息，hive的描述信息，比如说hive有几个database，有几个表，这些表对应的hdfs的地址在哪儿，表有几个字段，建了几个分区，创建了几个自定义函数\n\ncolumns_v2表记录是主外键关系表， \nfuncs表，是存的创建了什么函数 \npartition表是记录的创建了什么分区 \n\n# 报错\n\n> 报错 Hive 2.3.3 MetaException(message:Version information not found in metastore.)\n\n```\nschematool -initSchema -dbType mysql\n```\n\n> 参考https://stackoverflow.com/questions/50230515/hive-2-3-3-metaexceptionmessageversion-information-not-found-in-metastore\n>\n> http://sishuok.com/forum/blogPost/list/6221.html\n>\n> https://blog.csdn.net/nokia_hp/article/details/79054079\n\n\n\n","source":"_posts/Hive搭建及启动.md","raw":"---\ntitle: Hive搭建及启动\ndate: 2018年08月06日 22时15分52秒\ntags: [Hive,Docker]\ncategories: 安装部署\ntoc: true\n---\n\n[TOC]\n# 配置HOME\n\n下载hive包，并解压\n\n```\nhttp://archive.apache.org/dist/\n```\n\n`ln -s hive-2.1.1  /usr/hive`\n\n\n**vi ~/.bashrc**\n\n```bash\nexport PATH=$JAVA_HOME/bin:$PATH\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\n$HIVE_HOME/bin/hive\nexport HIVE_HOME=/usr/hive\nPATH=$HIVE_HOME/bin:$PATH\n#hive依赖于hadoop(可以不运行在同一主机，但是需要hadoop的配置)\n$HADOOP_HOME=/usr/hadoop\n```\n\n**source ~/.bashrc**\n\n<!--more -->\n\n# 安装mysql\n\n> ## Hive元数据介绍\n>\n> Hive 将元数据存储在 RDBMS 中，一般常用 MySQL 和 Derby。默认情况下，Hive 元数据保存在内嵌的 Derby 数据库中，只能允许一个会话连接，只适合简单的测试。实际生产环境中不适用， 为了支持多用户会话，则需要一个独立的元数据库，使用 MySQL 作为元数据库，Hive 内部对 MySQL 提供了很好的支持，配置一个独立的元数据库\n\n```bash\nyum install -y mysql-server\nchkconfig --add mysqld\nchkconfig mysqld on\nchkconfig --list mysqld\nservice mysqld start\n\nmysql -u root -p\nEnter password:           //默认密码为空，输入后回车即可\nset password for root@localhost=password('root'); 　　密码设置为root\nset password for root@=password('root');\n默认情况下Mysql只允许本地登录，所以只需配置root@localhost就好\n设置所有ip访问密码为root\nset password for root@%=password('root'); 　　　　　　密码设置为root （其实这一步可以不配）\n设置master访问密码为root\nset password for root@master=password('root'); 　　密码设置为root （其实这一步可以不配）\n查询密码\nselect user,host,password from mysql.user;  　　查看密码是否设置成功\n设置所有ip可以通过root访问\nGRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;\nGRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' IDENTIFIED BY 'hive' WITH GRANT OPTION;\n\nmysql -uroot -proot\ncreate user 'hive' identified by 'hive';\ncreate user 'hive'@'%' identified by 'hive';\n\ncreate database hive;\n```\n\n\n\n\n# 配置Hive\n\n`mkdir iotmp`\n\n`cp hive-default.xml.template hive-site.xml`\n\n`vim hive-site.xml`\n```xml\n<configuration>\n<property>\n    <name>javax.jdo.option.ConnectionDriverName</name>\n    <value>com.mysql.jdbc.Driver</value>\n    <description>Driver class name for a JDBC metastore</description>\n</property>\n\n<property>\n    <name>javax.jdo.option.ConnectionURL</name>\n    <value>jdbc:mysql://localhost:3306/hive?characterEncoding=UTF-8</value>\n    <description>JDBC connect string for a JDBC metastore</description>\n</property>\n\n<property>\n    <name>javax.jdo.option.ConnectionUserName</name>\n    <value>root</value>\n    <description>Username to use against metastore database</description>\n</property>\n\n<property>\n    <name>javax.jdo.option.ConnectionPassword</name>\n    <value>root</value>\n    <description>password to use against metastore database</description>\n</property>\n<property>\n    <name>hive.querylog.location</name>\n    <value>/usr/hive/iotmp</value>\n    <description>Location of Hive run time structured log file</description>\n</property>\n\n<property>\n    <name>hive.exec.local.scratchdir</name>\n    <value>/usr/hive/iotmp</value>\n    <description>Local scratch space for Hive jobs</description>\n</property>\n\n<property>\n    <name>hive.downloaded.resources.dir</name>\n    <value>/usr/hive/iotmp</value>\n    <description>Temporary local directory for added resources in the remote file system.</description>\n</property>\n<property>\n\t<name>hive.metastore.uris</name>\n\t<value>thrift://master:9083</value>\n</property>\n</configuration>\n```\nHive 的元数据可以存储在本地的 MySQL 中，但是大多数情况会是一个 mysql 集群，而且不在本地。所以在 hive 中需要开启远程 metastore。由于我是本地的 mysql，我就不配置下列属性了，但是如果是远程的 metastore，配置下面的属性。\n```xml\n<property>\n      <name>hive.metastore.uris</name>\n      <value></value>\n  <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>\n</property>\n<property>\n      <name>hive.server2.transport.mode</name>\n      <value>http</value>\n      <description>Server transport mode. \"binary\" or \"http\".</description>\n</property>\n\n链接：https://www.jianshu.com/p/87b76a686216\n```\n\n# Hive命令\n\n## 启动hive\n\n```bash\n$HIVE_HOME/bin/hive\n如果调试，可以加上参数\n$HIVE_HOME/bin/hivehive -hiveconf hive.root.logger=DEBUG,console\n```\n\n## 启动hiveserver2\n\n```bash\n$HIVE_HOME/bin/hive --service hiveserver2\n\nor nohup $HIVE_HOME/bin/hiveserver2 1>/var/log/hiveserver.log 2>/var/log/hiveserver.err &\n```\n\n> hiveserver端口号默认是10000\n\n**hiveserver2是否启动**\n`netstat -nl|grep 10000`\n\n## beeline工具测试使用jdbc方式连接\n\n可以在部署了hive任意节点上用beeline去连接\n\n```bash\n$HIVE_HOME/bin/beeline -u jdbc:hive2://hive:10000\nor\n$HIVE_HOME/bin/beeline -u jdbc:hive2://hive:10000 -n bigdata \n最后一个参数是用户\nor \n$HIVE_HOME/bin/beeline 回车，进入beeline的命令界面 \n输入命令连接hiveserver2 \nbeeline> !connect jdbc:hive2://hive:10000 \n```\n\n使用beeline通过jdbc连接上之后就可以像client一样操作。\n\nhiveserver2会同时启动一个webui，端口号默认为10002，可以通过http://localhost:10002/访问\n界面中可以看到Session/Query/Software等信息。(此网页只可查看，不可以操作hive数据仓库)\n\n> 参考https://blog.csdn.net/lblblblblzdx/article/details/79760959\n>\n> 参考https://www.cnblogs.com/netuml/p/7841387.html\n\n\n\nhive元数据库（就是hive库）中有几个表（有好几十个，但只需要记住其中几个就可以了），有哪些字段 \n元数据库最重要的作用是保存一些信息，hive的描述信息，比如说hive有几个database，有几个表，这些表对应的hdfs的地址在哪儿，表有几个字段，建了几个分区，创建了几个自定义函数\n\ncolumns_v2表记录是主外键关系表， \nfuncs表，是存的创建了什么函数 \npartition表是记录的创建了什么分区 \n\n# 报错\n\n> 报错 Hive 2.3.3 MetaException(message:Version information not found in metastore.)\n\n```\nschematool -initSchema -dbType mysql\n```\n\n> 参考https://stackoverflow.com/questions/50230515/hive-2-3-3-metaexceptionmessageversion-information-not-found-in-metastore\n>\n> http://sishuok.com/forum/blogPost/list/6221.html\n>\n> https://blog.csdn.net/nokia_hp/article/details/79054079\n\n\n\n","slug":"Hive搭建及启动","published":1,"updated":"2018-08-15T18:14:43.446Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vw001h2tpbnnqybm72"},{"title":"Hive累计报表","date":"2018-08-13T03:49:13.857Z","toc":true,"_content":"\n[TOC]\n\n在 hive 做统计的时候，总是涉及到做累计的报表处理，下面案列就是来做相应处理\n\n# 准备\n\n## 创建数据文件\n\n（在hadoop所在的机器）\n\n**vim /usr/hadoop/hivedata/t_sales.dat**\n\n```\n舒肤佳,2018-06,5\n舒肤佳,2018-06,15\n美姿,2018-06,5\n舒肤佳,2018-06,8\n美姿,2018-06,25\n舒肤佳,2018-06,5\n舒肤佳,2018-07,4\n舒肤佳,2018-07,6\n美姿,2018-07,10\n美姿,2018-07,5\n```\n\n上传到hdfs\n\n```shell\nhadoop fs -put /usr/hadoop/hivedata/t_sales.dat /local/hivedata/t_sales.dat\n```\n\n<!-- more -->\n\n## 创建表及读取数据\n\n```sql\ncreate table t_sales(brandname string,month string,sales int)\nrow format delimited fields terminated by ',';\nload data inpath '/local/hivedata/t_sales.dat' into table t_sales;\n```\n\n如果是上传本地文件（如果在hive所在主机上） 则在load  data 后加 local，如 `load data local inpath '/usr/hadoop/hivedata/t_sales.dat' into table t_sales;`\n\n\n\n# 1、先求每个品牌的月总金额\n\n```\nselect brandname,month,sum(sales) as all_sales from t_sales group by brandname,month\n```\n\n![](https://ws1.sinaimg.cn/large/0069RVTdgy1fu87l2nswej31500smq3k.jpg)\n\n\n\n# 2、将月总金额自连接\n\n```sql\nselect * from  (select brandname,month,sum(sales) as sal from t_sales group by brandname,month) A \n    inner join \n   (select brandname,month,sum(sales) as sal from t_sales group by brandname,month) B\n    on\nA.brandname=B.brandname\nwhere \nB.month <= A.month;\n```\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu88kcmiosj31ik0zgq4y.jpg)\n\n\n\n\n\n# 3、从上一步的结果中进行分组查询\n\n分组的字段是 a.brandname a.month\n\n求月累计值： 将 b.month <= a.month 的所有 b.sals求和即可\n\n```\nselect A.brandname,A.month,max(A.sales) as sales,sum(B.sales) as accumulate\nfrom \n(select brandname,month,sum(sales) as sales from t_sales group by brandname,month) A \ninner join \n(select brandname,month,sum(sales) as sales from t_sales group by brandname,month) B\non\nA.brandname=B.brandname\nwhere B.month <= A.month\ngroup by A.brandname,A.month\norder by A.brandname,A.month;\n```\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fu88tmmjfqj31ag0vamyy.jpg)","source":"_posts/Hive累计报表.md","raw":"---\ntitle: Hive累计报表\ndate: 2018年08月06日 22时15分52秒\ntags: [Hive,报表]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n在 hive 做统计的时候，总是涉及到做累计的报表处理，下面案列就是来做相应处理\n\n# 准备\n\n## 创建数据文件\n\n（在hadoop所在的机器）\n\n**vim /usr/hadoop/hivedata/t_sales.dat**\n\n```\n舒肤佳,2018-06,5\n舒肤佳,2018-06,15\n美姿,2018-06,5\n舒肤佳,2018-06,8\n美姿,2018-06,25\n舒肤佳,2018-06,5\n舒肤佳,2018-07,4\n舒肤佳,2018-07,6\n美姿,2018-07,10\n美姿,2018-07,5\n```\n\n上传到hdfs\n\n```shell\nhadoop fs -put /usr/hadoop/hivedata/t_sales.dat /local/hivedata/t_sales.dat\n```\n\n<!-- more -->\n\n## 创建表及读取数据\n\n```sql\ncreate table t_sales(brandname string,month string,sales int)\nrow format delimited fields terminated by ',';\nload data inpath '/local/hivedata/t_sales.dat' into table t_sales;\n```\n\n如果是上传本地文件（如果在hive所在主机上） 则在load  data 后加 local，如 `load data local inpath '/usr/hadoop/hivedata/t_sales.dat' into table t_sales;`\n\n\n\n# 1、先求每个品牌的月总金额\n\n```\nselect brandname,month,sum(sales) as all_sales from t_sales group by brandname,month\n```\n\n![](https://ws1.sinaimg.cn/large/0069RVTdgy1fu87l2nswej31500smq3k.jpg)\n\n\n\n# 2、将月总金额自连接\n\n```sql\nselect * from  (select brandname,month,sum(sales) as sal from t_sales group by brandname,month) A \n    inner join \n   (select brandname,month,sum(sales) as sal from t_sales group by brandname,month) B\n    on\nA.brandname=B.brandname\nwhere \nB.month <= A.month;\n```\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu88kcmiosj31ik0zgq4y.jpg)\n\n\n\n\n\n# 3、从上一步的结果中进行分组查询\n\n分组的字段是 a.brandname a.month\n\n求月累计值： 将 b.month <= a.month 的所有 b.sals求和即可\n\n```\nselect A.brandname,A.month,max(A.sales) as sales,sum(B.sales) as accumulate\nfrom \n(select brandname,month,sum(sales) as sales from t_sales group by brandname,month) A \ninner join \n(select brandname,month,sum(sales) as sales from t_sales group by brandname,month) B\non\nA.brandname=B.brandname\nwhere B.month <= A.month\ngroup by A.brandname,A.month\norder by A.brandname,A.month;\n```\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fu88tmmjfqj31ag0vamyy.jpg)","slug":"Hive累计报表","published":1,"updated":"2018-08-14T08:48:15.072Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vx001k2tpb20goysu0"},{"title":"Hive自定义函数UDF相关","date":"2018-08-11T03:16:01.806Z","toc":true,"_content":"\n[TOC]\n\n# UDF开发及使用\n\n1. 打成jar包上传到服务器，将jar包添加到hive的classpath\n\n```sql\n或者 hive>add JAR /home/hadoop/udf.jar;\n```\n\n1. 创建临时函数与开发好的java class关联\n\n```\nHive>create temporary function runTime as 'me.yao.bigdata.udf.RunTime';\n```\n\n即可在hql中使用自定义的函数time() \n\nSelect time(name),age from t_test;\n\n> add jar只在一次会话中生效\n\n<!-- more -->\n\n\n\n# transform案例:可以不需要上传jar包\n\n## 1、加载数据\n\n先加载rating.json([链接](https://pan.baidu.com/s/1kJnjv-en3R3HoFB8vhM9AA))文件到hive的一个原始表 rat_json\n\n```sql\ncreate table rat_json(line string) row format delimited;\nload data local inpath '/home/bigdata/apps/hive/hivedata/rating.json' into table rat_json;\n```\n\n## 2、解析字段\n\n需要解析json数据成四个字段，插入一张新的表 t_rating\n\n```sql\ncreate table t_rating as\nselect get_json_object(line,'$.movie') as movieid,get_json_object(line,'$.rate')as rate,get_json_object(line,'$.timeStamp')as timestring,get_json_object(line,'$.uid')as uid from rat_json;\n\n\n```\n\n或者\n\n```sql\ninsert overwrite table t_rating\nselect get_json_object(line,'$.movie') as movieid,get_json_object(line,'$.rate')as rate,get_json_object(line,'$.timeStamp')as timestring,get_json_object(line,'$.uid')as uid from rat_json;\n```\n\n## 3、转换weekday\n\n使用transform+python的方式去转换unixtime为weekday\n\n先编辑一个python脚本文件\n\n**cd /usr/hive/hivedata/**\n\n**vim weekday_mapper.py**\n\n```python\n#!/bin/python\n\nimport sys\n\nimport datetime\n\nfor line in sys.stdin:\n\n  line = line.strip()\n\n  movieid, rating, unixtime,userid = line.split('\\t')\n\n  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()\n\n  print '\\t'.join([movieid, rating, str(weekday),userid])\n```\n\n## 将文件加入classpath\n\n保存文件\n\n然后，将文件加入hive的classpath：\n\n```sql\nhive>add FILE /usr/hive/hivedata/weekday_mapper.py;\n```\n\n## 再创建表\n\n```sql\ncreate TABLE u_data_new as\nSELECT\n  TRANSFORM (movieid, rate, timestring,uid)\n  USING 'python weekday_mapper.py'\n  AS (movieid, rate, weekday,uid)\nFROM t_rating;\n```\n\n## 查询表\n\n```sql\nselect distinct(weekday) from u_data_new limit 10;\n```\n\n","source":"_posts/Hive自定义函数流程.md","raw":"---\ntitle: Hive自定义函数UDF相关\ndate: 2018年08月06日 22时15分52秒\ntags: [Hive]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n# UDF开发及使用\n\n1. 打成jar包上传到服务器，将jar包添加到hive的classpath\n\n```sql\n或者 hive>add JAR /home/hadoop/udf.jar;\n```\n\n1. 创建临时函数与开发好的java class关联\n\n```\nHive>create temporary function runTime as 'me.yao.bigdata.udf.RunTime';\n```\n\n即可在hql中使用自定义的函数time() \n\nSelect time(name),age from t_test;\n\n> add jar只在一次会话中生效\n\n<!-- more -->\n\n\n\n# transform案例:可以不需要上传jar包\n\n## 1、加载数据\n\n先加载rating.json([链接](https://pan.baidu.com/s/1kJnjv-en3R3HoFB8vhM9AA))文件到hive的一个原始表 rat_json\n\n```sql\ncreate table rat_json(line string) row format delimited;\nload data local inpath '/home/bigdata/apps/hive/hivedata/rating.json' into table rat_json;\n```\n\n## 2、解析字段\n\n需要解析json数据成四个字段，插入一张新的表 t_rating\n\n```sql\ncreate table t_rating as\nselect get_json_object(line,'$.movie') as movieid,get_json_object(line,'$.rate')as rate,get_json_object(line,'$.timeStamp')as timestring,get_json_object(line,'$.uid')as uid from rat_json;\n\n\n```\n\n或者\n\n```sql\ninsert overwrite table t_rating\nselect get_json_object(line,'$.movie') as movieid,get_json_object(line,'$.rate')as rate,get_json_object(line,'$.timeStamp')as timestring,get_json_object(line,'$.uid')as uid from rat_json;\n```\n\n## 3、转换weekday\n\n使用transform+python的方式去转换unixtime为weekday\n\n先编辑一个python脚本文件\n\n**cd /usr/hive/hivedata/**\n\n**vim weekday_mapper.py**\n\n```python\n#!/bin/python\n\nimport sys\n\nimport datetime\n\nfor line in sys.stdin:\n\n  line = line.strip()\n\n  movieid, rating, unixtime,userid = line.split('\\t')\n\n  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()\n\n  print '\\t'.join([movieid, rating, str(weekday),userid])\n```\n\n## 将文件加入classpath\n\n保存文件\n\n然后，将文件加入hive的classpath：\n\n```sql\nhive>add FILE /usr/hive/hivedata/weekday_mapper.py;\n```\n\n## 再创建表\n\n```sql\ncreate TABLE u_data_new as\nSELECT\n  TRANSFORM (movieid, rate, timestring,uid)\n  USING 'python weekday_mapper.py'\n  AS (movieid, rate, weekday,uid)\nFROM t_rating;\n```\n\n## 查询表\n\n```sql\nselect distinct(weekday) from u_data_new limit 10;\n```\n\n","slug":"Hive自定义函数流程","published":1,"updated":"2018-08-13T03:19:44.441Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43vz001n2tpbewytjvjc"},{"title":"Json与Scala类型的相互转换处理","date":"2018-08-10T17:19:05.443Z","toc":true,"_content":"\n\n\n\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu5817otq1j31kw0mojzv.jpg)\n\n在开发过程中时常会有对json数据的一些处理，现做一些记录\n\n<!-- more -->\n\n```scala\nimport com.alibaba.fastjson.{JSON, JSONArray, JSONObject}\nimport com.fasterxml.jackson.databind.ObjectMapper\nimport com.fasterxml.jackson.module.scala.DefaultScalaModule\nimport net.minidev.json.parser.JSONParser\nimport scala.collection.JavaConversions.mapAsScalaMap\nimport scala.collection.mutable\nimport java.util\n\n/**\n  * json utils\n  */\nobject JsonUtils {\n\n\n  val mapper: ObjectMapper = new ObjectMapper()\n\n  def toJsonString(T: Object): String = {\n    mapper.registerModule(DefaultScalaModule)\n    mapper.writeValueAsString(T)\n  }\n\n  def getArrayFromJson(jsonStr: String) = {\n    JSON.parseArray(jsonStr)\n  }\n  def getObjectFromJson(jsonStr: String): JSONObject = {\n    JSON.parseObject(jsonStr)\n  }\n  /**\n    * 配合getObjectFromJson 使用把 JSONObject 变为 map\n    * @param jsonObj\n    * @return\n    */\n  def jsonObj2Map(jsonObj:JSONObject): mutable.Map[String, String] = {\n    var map = mutable.Map[String, String]()\n    val itr: util.Iterator[String] = jsonObj.keySet().iterator()\n    while (itr.hasNext) {\n      val key = itr.next()\n      map += ((key, jsonObj.getString(key)))\n    }\n    map\n  }\n  /**\n    * json 字符串转成 Map\n    * #############有些情况下转换会有问题###############\n    * @param json\n    * @return\n    */\n  def json2Map(json: String): mutable.HashMap[String,String] ={\n    val map : mutable.HashMap[String,String]= mutable.HashMap()\n    val jsonParser =new JSONParser()\n    //将string转化为jsonObject\n    val jsonObj: JSONObject = jsonParser.parse(json).asInstanceOf[JSONObject]\n\n    //获取所有键\n    val jsonKey = jsonObj.keySet()\n\n    val iter = jsonKey.iterator()\n\n    while (iter.hasNext){\n      val field = iter.next()\n      val value = jsonObj.get(field).toString\n\n      if(value.startsWith(\"{\")&&value.endsWith(\"}\")){\n        val value = mapAsScalaMap(jsonObj.get(field).asInstanceOf[util.HashMap[String, String]])\n        map.put(field,value.toString())\n      }else{\n        map.put(field,value)\n      }\n    }\n    map\n  }\n  /**\n    * map 转换成 json 字符串\n    * @param map\n    * @return\n    */\n  def map2Json(map : mutable.Map[String,String]): String = {\n    import net.minidev.json.{JSONObject}\n    import scala.collection.JavaConversions.mutableMapAsJavaMap\n    val jsonString = JSONObject.toJSONString(map)\n    jsonString\n  }\n}\n```\n\n# 测试实例\n\n\n\n```scala\ndef main(args: Array[String]) {\n\n      val json = \"[{\\\"batchid\\\":305322456,\\\"amount\\\":20.0,\\\"count\\\":20},{\\\"batchid\\\":305322488,\\\"amount\\\":\\\"10.0\\\",\\\"count\\\":\\\"10\\\"}]\"\n      val array: JSONArray = JsonUtils.getArrayFromJson(json)\n      println(array)\n      array.toArray().foreach(json=>{\n        println(json)\n        val jobj = json.asInstanceOf[JSONObject]\n        println(jobj.get(\"batchid\"))\n      })\n\n      val jsonStr = \"{\\\"batchid\\\":119,\\\"amount\\\":200.0,\\\"count\\\":200}\"\n      val jsonObj: JSONObject = JsonUtils.getObjectFromJson(jsonStr)\n      println(jsonObj)\n\n      val jsonObj2: JSONObject = JsonUtils.getObjectFromJson(\"{'name':'Wang','age':18,'tag1':[{'tn1':'100','tn2':'101','ts':'ts01'},{'tn1':'100','tn2':'101','ts':'ts02'},{'tn1':'100','tn2':'101','ts':'ts03'}]}\")\n      println(jsonObj2)\n}\n```\n\n","source":"_posts/Json与Scala类型的一些互相转换处理.md","raw":"---\ntitle: Json与Scala类型的相互转换处理\ndate: 2018年08月06日 22时15分52秒\ntags: [Json,Scala]\ncategories: 大数据\ntoc: true\n---\n\n\n\n\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu5817otq1j31kw0mojzv.jpg)\n\n在开发过程中时常会有对json数据的一些处理，现做一些记录\n\n<!-- more -->\n\n```scala\nimport com.alibaba.fastjson.{JSON, JSONArray, JSONObject}\nimport com.fasterxml.jackson.databind.ObjectMapper\nimport com.fasterxml.jackson.module.scala.DefaultScalaModule\nimport net.minidev.json.parser.JSONParser\nimport scala.collection.JavaConversions.mapAsScalaMap\nimport scala.collection.mutable\nimport java.util\n\n/**\n  * json utils\n  */\nobject JsonUtils {\n\n\n  val mapper: ObjectMapper = new ObjectMapper()\n\n  def toJsonString(T: Object): String = {\n    mapper.registerModule(DefaultScalaModule)\n    mapper.writeValueAsString(T)\n  }\n\n  def getArrayFromJson(jsonStr: String) = {\n    JSON.parseArray(jsonStr)\n  }\n  def getObjectFromJson(jsonStr: String): JSONObject = {\n    JSON.parseObject(jsonStr)\n  }\n  /**\n    * 配合getObjectFromJson 使用把 JSONObject 变为 map\n    * @param jsonObj\n    * @return\n    */\n  def jsonObj2Map(jsonObj:JSONObject): mutable.Map[String, String] = {\n    var map = mutable.Map[String, String]()\n    val itr: util.Iterator[String] = jsonObj.keySet().iterator()\n    while (itr.hasNext) {\n      val key = itr.next()\n      map += ((key, jsonObj.getString(key)))\n    }\n    map\n  }\n  /**\n    * json 字符串转成 Map\n    * #############有些情况下转换会有问题###############\n    * @param json\n    * @return\n    */\n  def json2Map(json: String): mutable.HashMap[String,String] ={\n    val map : mutable.HashMap[String,String]= mutable.HashMap()\n    val jsonParser =new JSONParser()\n    //将string转化为jsonObject\n    val jsonObj: JSONObject = jsonParser.parse(json).asInstanceOf[JSONObject]\n\n    //获取所有键\n    val jsonKey = jsonObj.keySet()\n\n    val iter = jsonKey.iterator()\n\n    while (iter.hasNext){\n      val field = iter.next()\n      val value = jsonObj.get(field).toString\n\n      if(value.startsWith(\"{\")&&value.endsWith(\"}\")){\n        val value = mapAsScalaMap(jsonObj.get(field).asInstanceOf[util.HashMap[String, String]])\n        map.put(field,value.toString())\n      }else{\n        map.put(field,value)\n      }\n    }\n    map\n  }\n  /**\n    * map 转换成 json 字符串\n    * @param map\n    * @return\n    */\n  def map2Json(map : mutable.Map[String,String]): String = {\n    import net.minidev.json.{JSONObject}\n    import scala.collection.JavaConversions.mutableMapAsJavaMap\n    val jsonString = JSONObject.toJSONString(map)\n    jsonString\n  }\n}\n```\n\n# 测试实例\n\n\n\n```scala\ndef main(args: Array[String]) {\n\n      val json = \"[{\\\"batchid\\\":305322456,\\\"amount\\\":20.0,\\\"count\\\":20},{\\\"batchid\\\":305322488,\\\"amount\\\":\\\"10.0\\\",\\\"count\\\":\\\"10\\\"}]\"\n      val array: JSONArray = JsonUtils.getArrayFromJson(json)\n      println(array)\n      array.toArray().foreach(json=>{\n        println(json)\n        val jobj = json.asInstanceOf[JSONObject]\n        println(jobj.get(\"batchid\"))\n      })\n\n      val jsonStr = \"{\\\"batchid\\\":119,\\\"amount\\\":200.0,\\\"count\\\":200}\"\n      val jsonObj: JSONObject = JsonUtils.getObjectFromJson(jsonStr)\n      println(jsonObj)\n\n      val jsonObj2: JSONObject = JsonUtils.getObjectFromJson(\"{'name':'Wang','age':18,'tag1':[{'tn1':'100','tn2':'101','ts':'ts01'},{'tn1':'100','tn2':'101','ts':'ts02'},{'tn1':'100','tn2':'101','ts':'ts03'}]}\")\n      println(jsonObj2)\n}\n```\n\n","slug":"Json与Scala类型的一些互相转换处理","published":1,"updated":"2018-08-10T19:39:06.343Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43w0001q2tpb4s5k4pxk"},{"title":"Kafka初步总结","date":"2018-08-13T06:50:40.913Z","toc":true,"_content":"\n[TOC]\n\n>  Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。\n>  Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。\n>  无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性\n>\n\n### JMS的基础\n\nJMS是什么：JMS是Java提供的一套技术规范\n\nJMS干什么用：用来异构系统 集成通信，缓解系统瓶颈，提高系统的伸缩性增强系统用户体验，使得系统模块化和组件化变得可行并更加灵活\n\n通过什么方式：生产消费者模式（生产者、服务器、消费者）\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu895melcpj30zd0ckdgd.jpg)\n\njdk，kafka，activemq……\n\n<!-- more -->\n\n### JMS消息传输模型\n\n点对点模式**（一对一，消费者主动拉取数据，消息收到后消息清除）**\n\n点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被**一个且只有一个接收者接收处理**，即使有多个消息监听者也是如此。\n\n发布/订阅模式**（一对多，数据生产后，推送给所有订阅者）**\n\n发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，**即时当前订阅者不可用，处于离线状态**。\n\n![](https://ws2.sinaimg.cn/large/0069RVTdgy1fu894keo4oj310n0hd0tv.jpg)\n\nqueue.put（object）  数据生产\n\nqueue.take(object)    数据消费\n\n**kafka是采用的类jms模式，与类jms模式区别是**： \n\njms两种模式：1，推送的话可以多个，2拉取的话只能一个消费者，因为消费完，消息数据就会不存在了 kafka解决了这种弊端，拉取模式下也可以多个消费者，因为消息可以持久化到硬盘，就算消费了也是存在的。Kafka中ack机制是为了保证消息完整被处理 \n\n\n\n# kafka是什么\n\n    类JMS消息队列，结合JMS中的两种模式，可以有多个消费者主动拉取数据，在JMS中只有点对点模式才有消费者主动拉取数据。\n    kafka是一个生产-消费模型。\n    Producer：生产者，只负责数据生产，生产者的代码可以集成到任务系统中。 \n              数据的分发策略由producer决定，默认是defaultPartition  Utils.abs(key.hashCode) % numPartitions\n    Broker：当前服务器上的Kafka进程,俗称拉皮条。只管数据存储，不管是谁生产，不管是谁消费。\n            在集群中每个broker都有一个唯一brokerid，不得重复。\n    Topic:目标发送的目的地，这是一个逻辑上的概念，落到磁盘上是一个partition的目录。partition的目录中有多个segment组合(index,log)\n            一个Topic对应多个partition[0,1,2,3]，一个partition对应多个segment组合。一个segment有默认的大小是1G。\n            每个partition可以设置多个副本(replication-factor 1),会从所有的副本中选取一个leader出来。所有读写操作都是通过leader来进行的。\n            特别强调，和mysql中主从有区别，mysql做主从是为了读写分离，在kafka中读写操作都是leader。\n    ConsumerGroup：数据消费者组，ConsumerGroup可以有多个，每个ConsumerGroup消费的数据都是一样的。\n                   可以把多个consumer线程划分为一个组，组里面所有成员共同消费一个topic的数据，组员之间不能重复消费。\n                   \n\n## Kafka核心组件\nTopic ：消息根据Topic进行归类\nProducer：发送消息者\nConsumer：消息接受者\nbroker：每个kafka实例(server)\nZookeeper：依赖集群保存meta信息。\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fu89ajoxc5j30mf0fnt9d.jpg)\n\n\n\n## 为什么需要消息队列\n\n消息系统的核心作用就是三点：解耦，异步和并行\n\n以用户注册的案列来说明消息系统的作用\n\n\n\n## 消息队列和rpc调用区别\n\n​    消息队列并不关心是哪个消费者消费了数据，发布成功后就不必管消息队列的内容是否被消费，但是rpc调用的话，必须要给调用的系统返回一个状态码\n\n以下为针对Kafka的一些总结\n\n\n# kafka生产数据时的分组策略\n    默认是defaultPartition  Utils.abs(key.hashCode) % numPartitions\n    上文中的key是producer在发送数据时传入的，produer.send(KeyedMessage(topic,myPartitionKey,messageContent))\n\n# kafka如何保证数据的完全生产\n    ack机制：broker表示发来的数据已确认接收无误，表示数据已经保存到磁盘。\n    0：不等待broker返回确认消息\n    1：等待topic中某个partition leader保存成功的状态反馈\n    -1：等待topic中某个partition 所有副本都保存成功的状态反馈\n    \n# broker如何保存数据\n    在理论环境下，broker按照顺序读写的机制，可以每秒保存600M的数据。主要通过pagecache机制，尽可能的利用当前物理机器上的空闲内存来做缓存。\n    当前topic所属的broker，必定有一个该topic的partition，partition是一个磁盘目录。partition的目录中有多个segment组合(index,log)\n\n# partition如何分布在不同的broker上\n\n\n```java\n int i = 0\n    list{kafka01,kafka02,kafka03}\n    \n    for(int i=0;i<5;i++){\n        brIndex = i%broker;\n        hostName = list.get(brIndex)\n    }\n```\n\n\n    \n# consumerGroup的组员和partition之间如何做负载均衡\n    最好是一一对应，一个partition对应一个consumer。\n    如果consumer的数量过多，必然有空闲的consumer。\n    \n    算法：\n        假如topic1,具有如下partitions: P0,P1,P2,P3\n        加入group中,有如下consumer: C1,C2\n        首先根据partition索引号对partitions排序: P0,P1,P2,P3\n        根据consumer.id排序: C0,C1\n        计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)\n        然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)]\n\n# 如何保证kafka消费者消费数据是全局有序的\n    如果要全局有序的，必须保证生产有序，存储有序，消费有序。\n    由于生产可以做集群，存储可以分片，消费可以设置为一个consumerGroup，要保证全局有序，就需要保证每个环节都有序。\n    只有一个可能，就是一个生产者，一个partition，一个消费者。这种场景和大数据应用场景相悖。\n    1.生产者是集群模式--》全局序号管理器\n    2.broker断只设置一个partion-》kafka的高并发下的负载均衡\n    3.消费者如果是一个组，如何保障消息有序？消费来一个线程（自定义一个数据结构来排序）\n    \n\n\n\n不被完整处理，会造成结果？ \n是否开启ack-fail机制需要根据**业务场景来** 在大数据操作**点击流数据**基本上是不开启的，点击流日志中一条pv，uv数据丢失不会造成什么影响。","source":"_posts/Kafka小知识点.md","raw":"---\ntitle: Kafka初步总结\ndate: 2018年08月06日 22时15分52秒\ntags: [Kafka]\ncategories: 总结\ntoc: true\n---\n\n[TOC]\n\n>  Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。\n>  Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。\n>  无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性\n>\n\n### JMS的基础\n\nJMS是什么：JMS是Java提供的一套技术规范\n\nJMS干什么用：用来异构系统 集成通信，缓解系统瓶颈，提高系统的伸缩性增强系统用户体验，使得系统模块化和组件化变得可行并更加灵活\n\n通过什么方式：生产消费者模式（生产者、服务器、消费者）\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu895melcpj30zd0ckdgd.jpg)\n\njdk，kafka，activemq……\n\n<!-- more -->\n\n### JMS消息传输模型\n\n点对点模式**（一对一，消费者主动拉取数据，消息收到后消息清除）**\n\n点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被**一个且只有一个接收者接收处理**，即使有多个消息监听者也是如此。\n\n发布/订阅模式**（一对多，数据生产后，推送给所有订阅者）**\n\n发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，**即时当前订阅者不可用，处于离线状态**。\n\n![](https://ws2.sinaimg.cn/large/0069RVTdgy1fu894keo4oj310n0hd0tv.jpg)\n\nqueue.put（object）  数据生产\n\nqueue.take(object)    数据消费\n\n**kafka是采用的类jms模式，与类jms模式区别是**： \n\njms两种模式：1，推送的话可以多个，2拉取的话只能一个消费者，因为消费完，消息数据就会不存在了 kafka解决了这种弊端，拉取模式下也可以多个消费者，因为消息可以持久化到硬盘，就算消费了也是存在的。Kafka中ack机制是为了保证消息完整被处理 \n\n\n\n# kafka是什么\n\n    类JMS消息队列，结合JMS中的两种模式，可以有多个消费者主动拉取数据，在JMS中只有点对点模式才有消费者主动拉取数据。\n    kafka是一个生产-消费模型。\n    Producer：生产者，只负责数据生产，生产者的代码可以集成到任务系统中。 \n              数据的分发策略由producer决定，默认是defaultPartition  Utils.abs(key.hashCode) % numPartitions\n    Broker：当前服务器上的Kafka进程,俗称拉皮条。只管数据存储，不管是谁生产，不管是谁消费。\n            在集群中每个broker都有一个唯一brokerid，不得重复。\n    Topic:目标发送的目的地，这是一个逻辑上的概念，落到磁盘上是一个partition的目录。partition的目录中有多个segment组合(index,log)\n            一个Topic对应多个partition[0,1,2,3]，一个partition对应多个segment组合。一个segment有默认的大小是1G。\n            每个partition可以设置多个副本(replication-factor 1),会从所有的副本中选取一个leader出来。所有读写操作都是通过leader来进行的。\n            特别强调，和mysql中主从有区别，mysql做主从是为了读写分离，在kafka中读写操作都是leader。\n    ConsumerGroup：数据消费者组，ConsumerGroup可以有多个，每个ConsumerGroup消费的数据都是一样的。\n                   可以把多个consumer线程划分为一个组，组里面所有成员共同消费一个topic的数据，组员之间不能重复消费。\n                   \n\n## Kafka核心组件\nTopic ：消息根据Topic进行归类\nProducer：发送消息者\nConsumer：消息接受者\nbroker：每个kafka实例(server)\nZookeeper：依赖集群保存meta信息。\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fu89ajoxc5j30mf0fnt9d.jpg)\n\n\n\n## 为什么需要消息队列\n\n消息系统的核心作用就是三点：解耦，异步和并行\n\n以用户注册的案列来说明消息系统的作用\n\n\n\n## 消息队列和rpc调用区别\n\n​    消息队列并不关心是哪个消费者消费了数据，发布成功后就不必管消息队列的内容是否被消费，但是rpc调用的话，必须要给调用的系统返回一个状态码\n\n以下为针对Kafka的一些总结\n\n\n# kafka生产数据时的分组策略\n    默认是defaultPartition  Utils.abs(key.hashCode) % numPartitions\n    上文中的key是producer在发送数据时传入的，produer.send(KeyedMessage(topic,myPartitionKey,messageContent))\n\n# kafka如何保证数据的完全生产\n    ack机制：broker表示发来的数据已确认接收无误，表示数据已经保存到磁盘。\n    0：不等待broker返回确认消息\n    1：等待topic中某个partition leader保存成功的状态反馈\n    -1：等待topic中某个partition 所有副本都保存成功的状态反馈\n    \n# broker如何保存数据\n    在理论环境下，broker按照顺序读写的机制，可以每秒保存600M的数据。主要通过pagecache机制，尽可能的利用当前物理机器上的空闲内存来做缓存。\n    当前topic所属的broker，必定有一个该topic的partition，partition是一个磁盘目录。partition的目录中有多个segment组合(index,log)\n\n# partition如何分布在不同的broker上\n\n\n```java\n int i = 0\n    list{kafka01,kafka02,kafka03}\n    \n    for(int i=0;i<5;i++){\n        brIndex = i%broker;\n        hostName = list.get(brIndex)\n    }\n```\n\n\n    \n# consumerGroup的组员和partition之间如何做负载均衡\n    最好是一一对应，一个partition对应一个consumer。\n    如果consumer的数量过多，必然有空闲的consumer。\n    \n    算法：\n        假如topic1,具有如下partitions: P0,P1,P2,P3\n        加入group中,有如下consumer: C1,C2\n        首先根据partition索引号对partitions排序: P0,P1,P2,P3\n        根据consumer.id排序: C0,C1\n        计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)\n        然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)]\n\n# 如何保证kafka消费者消费数据是全局有序的\n    如果要全局有序的，必须保证生产有序，存储有序，消费有序。\n    由于生产可以做集群，存储可以分片，消费可以设置为一个consumerGroup，要保证全局有序，就需要保证每个环节都有序。\n    只有一个可能，就是一个生产者，一个partition，一个消费者。这种场景和大数据应用场景相悖。\n    1.生产者是集群模式--》全局序号管理器\n    2.broker断只设置一个partion-》kafka的高并发下的负载均衡\n    3.消费者如果是一个组，如何保障消息有序？消费来一个线程（自定义一个数据结构来排序）\n    \n\n\n\n不被完整处理，会造成结果？ \n是否开启ack-fail机制需要根据**业务场景来** 在大数据操作**点击流数据**基本上是不开启的，点击流日志中一条pv，uv数据丢失不会造成什么影响。","slug":"Kafka小知识点","published":1,"updated":"2018-08-15T03:54:57.767Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43w1001t2tpbavcx4dbs"},{"title":"Kafka深入解析","date":"2018-08-14T07:43:27.968Z","toc":true,"_content":"\n[TOC]\n\n# Kafka结构\n\n- **Producer** ：消息生产者，就是向kafka broker发消息的客户端。\n\n\n- \n  **Consumer** ：消息消费者，向kafka broker取消息的客户端\n\n- **Topic** ：可以理解为一个队列\n\n  <!-- more -->\n\n- **Consumer Group （CG**）： （消费组）这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。\n\n  一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，*<u>但每个partion只会把消息发给该CG中的一个consumer</u>*。如果需要实现广播，<u>只要每个consumer有一个独立的CG就可以了</u>（即是需要消费消息的消费者，属于不同的消费组）。要实现单播只要所有的consumer在同一个CG（即是需要消费消息的消费者，属于同一个消费组）。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。\n\n- **Broker** ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。\n\n  broker的作用就是帮你把消息从发送端传送到接收端\n\n- **Partition**：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。\n\n- **Offset**：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka\n\n\n\n# Consumer与topic关系\n\n本质上kafka只支持Topic；\n\n- 每个group中可以有多个consumer，每个consumer属于一个consumer group；\n\n  通常情况下，一个group中会包含多个consumer，这样不仅可以提高topic中消息的并发消费能力，而且还能提高\"故障容错\"性，如果group中的某个consumer失效那么其消费的partitions将会有其他consumer自动接管。\n\n- 对于Topic中的一条特定的消息，只会被订阅此Topic的每个group中的其中一个consumer消费，此消息不会发送给一个group的多个consumer；\n\n- 那么一个group中所有的consumer将会交错的消费整个Topic，每个group中consumer消息消费互相独立，我们可以认为一个group是一个\"订阅\"者。\n\n- 在kafka中,一个partition中的消息只会被group中的一个consumer消费(同一时刻)；\n\n  一个Topic中的每个partions，只会被一个\"订阅者\"中的一个consumer消费，不过一个consumer可以同时消费多个partitions中的消息。\n\n- kafka的设计原理决定,对于一个topic，同一个group中不能有多于partitions个数的consumer同时消费，否则将意味着某些consumer将无法得到消息。\n\n- kafka只能保证一个partition中的消息被某个consumer消费时是顺序的；事实上，从Topic角度来说,当有多个partitions时,消息仍不是全局有序的。\n\n\n\n# Kafka消息的分发\n\nProducer客户端负责消息的分发\n\n- kafka集群中的任何一个broker都可以向producer提供metadata信息,这些metadata中包含\"集群中存活的servers列表\"/\"partitions leader列表\"等信息；\n- 当producer获取到metadata信息之后, producer将会和Topic下所有partition leader保持socket连接；\n- 消息由producer直接通过socket发送到broker，中间不会经过任何\"路由层\"，事实上，消息被路由到哪个partition上由producer客户端决定；\n\n比如可以采用\"random\"\"key-hash\"\"轮询\"等,如果一个topic中有多个partitions,那么在producer端实现\"消息均衡分发\"是必要的。\n\n\n\n在producer端的配置文件中,开发者可以指定partition路由的方式。\n\n\n**Producer消息发送的应答机制**\n设置发送数据是否需要服务端的反馈,有三个值0,1,-1\n  0: producer不会等待broker发送ack \n  1: 当leader接收到消息之后发送ack \n  -1: 当所有的follower都同步消息成功后发送ack\n\n**request.required.acks=0**\n\n# Consumer的负载均衡\n\n当一个group中,有consumer加入或者离开时,会触发partitions均衡.均衡的最终目的,是提升topic的并发消费能力，步骤如下：\n\n1. 假如topic1,具有如下partitions: P0,P1,P2,P3\n2. 加入group中,有如下consumer: C1,C2\n3. 首先根据partition索引号对partitions排序: P0,P1,P2,P3\n4. 根据consumer.id排序: C0,C1\n5. 计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)\n6. 然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)]\n\n![](https://ws3.sinaimg.cn/large/0069RVTdly1fu9a7zzwdrj31540lvjtv.jpg)\n\n\n\n# Kafka文件存储机制\n\n### Kafka文件存储基本结构\n\n在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。\n\n![](https://ws4.sinaimg.cn/large/0069RVTdly1fu9abi7beaj312e04agng.jpg)\n\n每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。**但每个段****segmentfile****消息数量不一定相等**，这种特性方便oldsegment file快速被删除。默认保留7天的数据。\n\n![](https://ws2.sinaimg.cn/large/0069RVTdly1fu9ajs8d4dj30rs0dgwj3.jpg)\n\n每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。（什么时候创建，什么时候删除）\n\n![](https://ws1.sinaimg.cn/large/0069RVTdly1fu9ajsxr9bj30tt0ektai.jpg)\n\n### Kafka Partition Segment\n\nSegment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀\".index\"和“.log”分别表示为segment索引文件、数据文件。\n\n![](https://ws4.sinaimg.cn/large/0069RVTdly1fu9ai5p7amj30nv0f0gre.jpg)\n\nl Segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。\n\nl 索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。\n\n![](https://ws2.sinaimg.cn/large/0069RVTdly1fu9ai52pxmj30sx0hetdm.jpg)\n\n3，497：当前log文件中的第几条信息，存放在磁盘上的那个地方\n\n \n\n上述图中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。\n\n其中以索引文件中元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。\n\n \n\nl segment data file由许多message组成， qq物理结构如下：\n\n| **关键字******         | **解释说明******                             |\n| ------------------- | ---------------------------------------- |\n| 8  byte offset      | 在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message |\n| 4 byte message size | message大小                                |\n| 4 byte CRC32        | 用crc32校验message                          |\n| 1 byte “magic\"      | 表示本次发布Kafka服务程序协议版本号                     |\n| 1 byte “attributes\" | 表示为独立版本、或标识压缩类型、或编码类型。                   |\n| 4 byte key length   | 表示key的长度,当key为-1时，K byte key字段不填         |\n| K byte key          | 可选                                       |\n| value bytes payload | 表示实际消息数据。                                |\n\n kafka查找message，先查找segment file \n\n00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0\n\n00000000000000368769.index的消息量起始偏移量为368770= 368769 + 1\n\n00000000000000737337.index的起始偏移量为737338=737337+ 1\n\n其他后续文件依次类推。\n\n以起始偏移量命名并排序这些文件，只要根据offset**二分查找**文件列表，就可以快速定位到具体文件。当offset=368776时定位到00000000000000368769.index和对应log文件。\n\n再通过segment file查找message      \n\n当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址\n\n然后再通过00000000000000368769.log顺序查找直到offset=368776为止。","source":"_posts/Kafka深入解析.md","raw":"---\ntitle: Kafka深入解析\ndate: 2018年08月06日 22时15分52秒\ntags: [Kafka,原理]\ncategories: 组件\ntoc: true\n---\n\n[TOC]\n\n# Kafka结构\n\n- **Producer** ：消息生产者，就是向kafka broker发消息的客户端。\n\n\n- \n  **Consumer** ：消息消费者，向kafka broker取消息的客户端\n\n- **Topic** ：可以理解为一个队列\n\n  <!-- more -->\n\n- **Consumer Group （CG**）： （消费组）这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。\n\n  一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，*<u>但每个partion只会把消息发给该CG中的一个consumer</u>*。如果需要实现广播，<u>只要每个consumer有一个独立的CG就可以了</u>（即是需要消费消息的消费者，属于不同的消费组）。要实现单播只要所有的consumer在同一个CG（即是需要消费消息的消费者，属于同一个消费组）。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。\n\n- **Broker** ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。\n\n  broker的作用就是帮你把消息从发送端传送到接收端\n\n- **Partition**：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。\n\n- **Offset**：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka\n\n\n\n# Consumer与topic关系\n\n本质上kafka只支持Topic；\n\n- 每个group中可以有多个consumer，每个consumer属于一个consumer group；\n\n  通常情况下，一个group中会包含多个consumer，这样不仅可以提高topic中消息的并发消费能力，而且还能提高\"故障容错\"性，如果group中的某个consumer失效那么其消费的partitions将会有其他consumer自动接管。\n\n- 对于Topic中的一条特定的消息，只会被订阅此Topic的每个group中的其中一个consumer消费，此消息不会发送给一个group的多个consumer；\n\n- 那么一个group中所有的consumer将会交错的消费整个Topic，每个group中consumer消息消费互相独立，我们可以认为一个group是一个\"订阅\"者。\n\n- 在kafka中,一个partition中的消息只会被group中的一个consumer消费(同一时刻)；\n\n  一个Topic中的每个partions，只会被一个\"订阅者\"中的一个consumer消费，不过一个consumer可以同时消费多个partitions中的消息。\n\n- kafka的设计原理决定,对于一个topic，同一个group中不能有多于partitions个数的consumer同时消费，否则将意味着某些consumer将无法得到消息。\n\n- kafka只能保证一个partition中的消息被某个consumer消费时是顺序的；事实上，从Topic角度来说,当有多个partitions时,消息仍不是全局有序的。\n\n\n\n# Kafka消息的分发\n\nProducer客户端负责消息的分发\n\n- kafka集群中的任何一个broker都可以向producer提供metadata信息,这些metadata中包含\"集群中存活的servers列表\"/\"partitions leader列表\"等信息；\n- 当producer获取到metadata信息之后, producer将会和Topic下所有partition leader保持socket连接；\n- 消息由producer直接通过socket发送到broker，中间不会经过任何\"路由层\"，事实上，消息被路由到哪个partition上由producer客户端决定；\n\n比如可以采用\"random\"\"key-hash\"\"轮询\"等,如果一个topic中有多个partitions,那么在producer端实现\"消息均衡分发\"是必要的。\n\n\n\n在producer端的配置文件中,开发者可以指定partition路由的方式。\n\n\n**Producer消息发送的应答机制**\n设置发送数据是否需要服务端的反馈,有三个值0,1,-1\n  0: producer不会等待broker发送ack \n  1: 当leader接收到消息之后发送ack \n  -1: 当所有的follower都同步消息成功后发送ack\n\n**request.required.acks=0**\n\n# Consumer的负载均衡\n\n当一个group中,有consumer加入或者离开时,会触发partitions均衡.均衡的最终目的,是提升topic的并发消费能力，步骤如下：\n\n1. 假如topic1,具有如下partitions: P0,P1,P2,P3\n2. 加入group中,有如下consumer: C1,C2\n3. 首先根据partition索引号对partitions排序: P0,P1,P2,P3\n4. 根据consumer.id排序: C0,C1\n5. 计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)\n6. 然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)]\n\n![](https://ws3.sinaimg.cn/large/0069RVTdly1fu9a7zzwdrj31540lvjtv.jpg)\n\n\n\n# Kafka文件存储机制\n\n### Kafka文件存储基本结构\n\n在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。\n\n![](https://ws4.sinaimg.cn/large/0069RVTdly1fu9abi7beaj312e04agng.jpg)\n\n每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。**但每个段****segmentfile****消息数量不一定相等**，这种特性方便oldsegment file快速被删除。默认保留7天的数据。\n\n![](https://ws2.sinaimg.cn/large/0069RVTdly1fu9ajs8d4dj30rs0dgwj3.jpg)\n\n每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。（什么时候创建，什么时候删除）\n\n![](https://ws1.sinaimg.cn/large/0069RVTdly1fu9ajsxr9bj30tt0ektai.jpg)\n\n### Kafka Partition Segment\n\nSegment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀\".index\"和“.log”分别表示为segment索引文件、数据文件。\n\n![](https://ws4.sinaimg.cn/large/0069RVTdly1fu9ai5p7amj30nv0f0gre.jpg)\n\nl Segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。\n\nl 索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。\n\n![](https://ws2.sinaimg.cn/large/0069RVTdly1fu9ai52pxmj30sx0hetdm.jpg)\n\n3，497：当前log文件中的第几条信息，存放在磁盘上的那个地方\n\n \n\n上述图中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。\n\n其中以索引文件中元数据3,497为例，依次在数据文件中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。\n\n \n\nl segment data file由许多message组成， qq物理结构如下：\n\n| **关键字******         | **解释说明******                             |\n| ------------------- | ---------------------------------------- |\n| 8  byte offset      | 在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message |\n| 4 byte message size | message大小                                |\n| 4 byte CRC32        | 用crc32校验message                          |\n| 1 byte “magic\"      | 表示本次发布Kafka服务程序协议版本号                     |\n| 1 byte “attributes\" | 表示为独立版本、或标识压缩类型、或编码类型。                   |\n| 4 byte key length   | 表示key的长度,当key为-1时，K byte key字段不填         |\n| K byte key          | 可选                                       |\n| value bytes payload | 表示实际消息数据。                                |\n\n kafka查找message，先查找segment file \n\n00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0\n\n00000000000000368769.index的消息量起始偏移量为368770= 368769 + 1\n\n00000000000000737337.index的起始偏移量为737338=737337+ 1\n\n其他后续文件依次类推。\n\n以起始偏移量命名并排序这些文件，只要根据offset**二分查找**文件列表，就可以快速定位到具体文件。当offset=368776时定位到00000000000000368769.index和对应log文件。\n\n再通过segment file查找message      \n\n当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址\n\n然后再通过00000000000000368769.log顺序查找直到offset=368776为止。","slug":"Kafka深入解析","published":1,"updated":"2018-08-14T08:30:28.045Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43w2001x2tpbpko8bhpd"},{"title":"Kafka读写数据","date":"2018-08-14T11:47:22.892Z","toc":true,"_content":"\n[TOC]\n\n首先kafka依赖于操作系统的pageCache机制，尽可能的把空闲的内存作为一个磁盘，只有发生缺页的才会放在磁盘中\n\n<!-- more -->\n\n那么落在磁盘上的话，还有就是采用的是sendfile机制：主要思想就是在内核中进行拷贝，再写socket流，传统io是先将数据读到内核在写到用户区，在写到内核，再是socket流，这样就算磁盘的话，也是相当快的","source":"_posts/Kafka读取数据性能.md","raw":"---\ntitle: Kafka读写数据\ndate: 2018年08月06日 22时15分52秒\ntags: [Kafka,原理]\ncategories: 组件\ntoc: true\n---\n\n[TOC]\n\n首先kafka依赖于操作系统的pageCache机制，尽可能的把空闲的内存作为一个磁盘，只有发生缺页的才会放在磁盘中\n\n<!-- more -->\n\n那么落在磁盘上的话，还有就是采用的是sendfile机制：主要思想就是在内核中进行拷贝，再写socket流，传统io是先将数据读到内核在写到用户区，在写到内核，再是socket流，这样就算磁盘的话，也是相当快的","slug":"Kafka读取数据性能","published":1,"updated":"2018-08-15T07:23:24.298Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43w300212tpbem79webm"},{"title":"Kafka集群配置及配置文件","date":"2018-08-13T06:54:35.320Z","toc":true,"_content":"\n[TOC]\n\n\n\n\n\n# Kafka集群部署\n\nkafka默认推荐的是2.11开头的，如果系统中没有其他软件依赖于Scala的话，就使用2.11版本的 \nscala是依赖于zookeeper的，所以需要给zookeeper配置地址\n\n<!-- more -->\n\n\n\n# 1、下载安装包\n\n<http://kafka.apache.org/downloads.html> \n在linux中使用wget命令下载安装包 \nwget <http://mirrors.hust.edu.cn/apache/kafka/0.8.2.2/kafka_2.11-0.8.2.2.tgz>\n\n# 2、解压安装包\n\n```\ntar -zxvf kafka_2.11-0.8.2.2.tgz -C /home/bigdata/apps/kafka/\ncd /home/bigdata/apps/kafka/\nln -s kafka_2.11-0.8.2.2 kafka\n\n```\n\n# 3、修改配置文件\n\n配置文件有4个点\n\nhostname应该保持一致\n\n```\nbroker.id =0\n#每一个broker在集群中的唯一表示，要求是正数。当该服务器的IP地址发生改变时，broker.id没有变化，则不会影响consumers的消息情况\n  log.dirs=/home/bigdata/logs/kafka-logs\n#kafka数据的存放地址，多个地址的话用逗号分割 /data/kafka-logs-1，/data/kafka-logs-2\n  port =9092\n#broker server服务端口\n  message.max.bytes =6525000\n#表示消息体的最大大小，单位是字节\n  num.network.threads =3\n#broker处理消息的最大线程数，一般情况下不需要去修改 配置了三台服务器，所以选择三个\n  #num.io.threads =8\n#broker处理磁盘IO的线程数，数值应该大于你的硬盘数\n  #background.threads =4\n#一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改\n  queued.max.requests =500\n#等待IO线程处理的请求队列最大数，若是等待IO的请求超过这个数值，那么会停止接受外部消息，应该是一种自我保护机制。\n#broker的主机地址，若是设置了，那么会绑定到这个地址上，若是没有，会绑定到所有的接口上，并将其中之一发送到ZK，一般不设置\n  socket.send.buffer.bytes=102400\n#socket的发送缓冲区，socket的调优参数SO_SNDBUFF\n  socket.receive.buffer.bytes =102400\n#socket的接受缓冲区，socket的调优参数SO_RCVBUFF\n  socket.request.max.bytes =104857600\n#socket请求的最大数值，防止serverOOM，message.max.bytes必然要小于socket.request.max.bytes，会被topic创建时的指定参数覆盖\n   #log.segment.bytes =1024*1024*1024\n#topic的分区是以一堆segment文件存储的，这个控制每个segment的大小，会被topic创建时的指定参数覆盖\n  log.roll.hours =168\n  zookeeper.connect = bigdata1:2181,bigdata2:2181,bigdata3:2181\n#zookeeper集群的地址，可以是多个，多个之间用逗号分割 hostname1:port1,hostname2:port2,hostname3:port3\n  zookeeper.session.timeout.ms=6000\n#ZooKeeper的最大超时时间，就是心跳的间隔，若是没有反映，那么认为已经死了，不易过大\n  zookeeper.connection.timeout.ms =6000\n#ZooKeeper的连接超时时间\n  zookeeper.sync.time.ms =2000\n#host.name=bigdata1\n #broker的主机地址，若是设置了，那么会绑定到这个地址上，若是没有，会绑定到所有的接口上，并将其中之一发送到ZK，一般不设置,hostname为主机\n \n# 这个是轻量的配置文件\n#broker的全局唯一编号，不能重复\nbroker.id=0\n#用来监听连接的端口，producer或consumer将在此端口建立连接\nport=9092\n#处理网络请求的线程数量，集群中有几个节点就设置几个\nnum.network.threads=3\n#用来处理磁盘io的线程数量\nnum.io.threads=8\n#发送套接字的缓冲区大小\nsocket.send.buffer.bytes=102400\n#接受套接字的缓冲区大小\nsocket.receive.buffer.bytes=102400\n#请求套接字的缓冲区大小\nsocket.request.max.bytes=104857600\n#kafka运行日志存放的路径\nlog.dirs=/home/hadoop/logs/kafka\n#topic在当前broker上的分片个数\nnum.partitions=2\n#用来恢复和清理data下数据的线程数量\nnum.recovery.threads.per.data.dir=1\n#segment文件保留的最长时间。超时将会被删除\nlog.retention.hours=168\n#滚动删除生成心得segment文件的最大时间\nlog.roll.hour=168\n\n```\n\n `------------------`\n\n```\nip（重要），如果不改，则客户端会抛出：producer connection to localhost:9092 unsuccessful错误，\nadvertised.host.name=192.168.11.11\n\n```\n\n\\```\n\n```\ncp  /home/bigdata/apps/kafka/config/server.properties /home/bigdata/apps/kafka/config/server.properties.bak\nvi  /home/bigdata/apps/kafka/config/server.properties\n\n```\n\n# 4、分发安装包\n\n```\nscp -r /home/bigdata/apps/kafka/ bigdata2:/home/bigdata/apps/\n\n```\n\n然后分别在各机器上创建软连\n\n```\ncd /home/bigdata/apps/kafka\nln -s kafka_2.11-0.8.2.2 kafka\n\n```\n\n# 5、再次修改配置文件（重要）\n\n```\n依次修改各服务器上配置文件的的broker.id，分别是0,1,2不得重复。\n```\n\n# 需要配置kafka的环境变量\n\n# 6、启动集群\n\n**依次在各节点上启动kafka** \n后台启动 `nohup最后加一个&` \n\n```bash\nKAFKA_HOME/bin/kafka-server-start.sh  KAFKA_HOME/config/server.properties &\n```\n\n\n\n# 配置文件及注释\n\n```\nbroker.id=0\n#当前机器在集群中的唯一标识，和zookeeper的myid性质一样\nport=9092\n#当前kafka对外提供服务的端口默认是9092\nhost.name=192.168.11.11\nadvertised.host.name=192.168.11.11\n#这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。\nnum.network.threads=3\n#这个是borker进行网络处理的线程数\nnum.io.threads=8\n#这个是borker进行I/O处理的线程数\nlog.dirs=/home/bigdata/apps/kafka/kafkalogs/\n#消息存放的目录，这个目录可以配置为“，”逗号分割的表达式，上面的num.io.threads要大于这个目录的个数这个目录，如果配置多个目录，新创建的topic他把消息持久化的地方是，当前以逗号分割的目录中，那个分区数最少就放那一个\nsocket.send.buffer.bytes=102400\n#发送缓冲区buffer大小，数据不是一下子就发送的，先回存储到缓冲区了到达一定的大小后在发送，能提高性能\nsocket.receive.buffer.bytes=102400\n#kafka接收缓冲区大小，当数据到达一定大小后在序列化到磁盘\nsocket.request.max.bytes=104857600\n#这个参数是向kafka请求消息或者向kafka发送消息的请请求的最大数，这个值不能超过java的堆栈大小\nnum.partitions=1\n#默认的分区数，一个topic默认1个分区数\nlog.retention.hours=168\n#默认消息的最大持久化时间，168小时，7天\n\n\nmessage.max.bytes=5242880\n#消息保存的最大值5M\ndefault.replication.factor=2\n#kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务\nreplica.fetch.max.bytes=5242880\n#取消息的最大直接数\nlog.segment.bytes=1073741824\n#这个参数是：因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新起一个文件\nlog.retention.check.interval.ms=300000\n#每隔300000毫秒去检查上面配置的log失效时间（log.retention.hours=168 ），到目录查看是否有过期的消息如果有，删除\nlog.cleaner.enable=false\n#是否启用log压缩，一般不用启用，启用的话可以提高性能\nzookeeper.connect=192.168.11.11:2181,192.168.11.12:2181,192.168.11.13:2181\n#设置zookeeper的连接端口\n```\n\n# 常用\n\n```\nbroker.id =0\n#每一个broker在集群中的唯一表示，要求是正数。当该服务器的IP地址发生改变时，broker.id没有变化，则不会影响consumers的消息情况\nlog.dirs=/data/kafka-logs\n#kafka数据的存放地址，多个地址的话用逗号分割 /data/kafka-logs-1，/data/kafka-logs-2\nport =9092\n#broker server服务端口\nmessage.max.bytes =6525000\n#表示消息体的最大大小，单位是字节\nnum.network.threads =3\n#broker处理消息的最大线程数，一般情况下不需要去修改 配置了三台服务器，所以选择三个\nnum.io.threads =8\n#broker处理磁盘IO的线程数，数值应该大于你的硬盘数\nbackground.threads =4\n#一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改\nqueued.max.requests =500\n#等待IO线程处理的请求队列最大数，若是等待IO的请求超过这个数值，那么会停止接受外部消息，应该是一种自我保护机制。\nsocket.send.buffer.bytes=100*1024\n#socket的发送缓冲区，socket的调优参数SO_SNDBUFF\nsocket.receive.buffer.bytes =100*1024\n#socket的接受缓冲区，socket的调优参数SO_RCVBUFF\nsocket.request.max.bytes =100*1024*1024\n#socket请求的最大数值，防止serverOOM，message.max.bytes必然要小于socket.request.max.bytes，会被topic创建时的指定参数覆盖\nlog.segment.bytes =1024*1024*1024\n#topic的分区是以一堆segment文件存储的，这个控制每个segment的大小，会被topic创建时的指定参数覆盖\nlog.roll.hours =24*7\n\n```\n\n# 详解\n\n```\nbroker.id =0\n#每一个broker在集群中的唯一表示，要求是正数。当该服务器的IP地址发生改变时，broker.id没有变化，则不会影响consumers的消息情况\nlog.dirs=/data/kafka-logs\n#kafka数据的存放地址，多个地址的话用逗号分割 /data/kafka-logs-1，/data/kafka-logs-2\nport =9092\n#broker server服务端口\nmessage.max.bytes =6525000\n#表示消息体的最大大小，单位是字节\nnum.network.threads =4\n#broker处理消息的最大线程数，一般情况下不需要去修改\nnum.io.threads =8\n#broker处理磁盘IO的线程数，数值应该大于你的硬盘数\nbackground.threads =4\n#一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改\nqueued.max.requests =500\n#等待IO线程处理的请求队列最大数，若是等待IO的请求超过这个数值，那么会停止接受外部消息，应该是一种自我保护机制。\nhost.name\n#broker的主机地址，若是设置了，那么会绑定到这个地址上，若是没有，会绑定到所有的接口上，并将其中之一发送到ZK，一般不设置\nsocket.send.buffer.bytes=100*1024\n#socket的发送缓冲区，socket的调优参数SO_SNDBUFF\nsocket.receive.buffer.bytes =100*1024\n#socket的接受缓冲区，socket的调优参数SO_RCVBUFF\nsocket.request.max.bytes =100*1024*1024\n#socket请求的最大数值，防止serverOOM，message.max.bytes必然要小于socket.request.max.bytes，会被topic创建时的指定参数覆盖\nlog.segment.bytes =1024*1024*1024\n#topic的分区是以一堆segment文件存储的，这个控制每个segment的大小，会被topic创建时的指定参数覆盖\nlog.roll.hours =24*7\n#这个参数会在日志segment没有达到log.segment.bytes设置的大小，也会强制新建一个segment会被 topic创建时的指定参数覆盖\nlog.cleanup.policy = delete\n#日志清理策略选择有：delete和compact主要针对过期数据的处理，或是日志文件达到限制的额度，会被 topic创建时的指定参数覆盖\nlog.retention.minutes=60*24 # 一天后删除\n#数据存储的最大时间超过这个时间会根据log.cleanup.policy设置的策略处理数据，也就是消费端能够多久去消费数据\n#log.retention.bytes和log.retention.minutes任意一个达到要求，都会执行删除，会被topic创建时的指定参数覆盖\nlog.retention.bytes=-1\n#topic每个分区的最大文件大小，一个topic的大小限制 = 分区数*log.retention.bytes。-1没有大小限log.retention.bytes和log.retention.minutes任意一个达到要求，都会执行删除，会被topic创建时的指定参数覆盖\nlog.retention.check.interval.ms=5minutes\n#文件大小检查的周期时间，是否处罚 log.cleanup.policy中设置的策略\nlog.cleaner.enable=false\n#是否开启日志压缩\nlog.cleaner.threads = 2\n#日志压缩运行的线程数\nlog.cleaner.io.max.bytes.per.second=None\n#日志压缩时候处理的最大大小\nlog.cleaner.dedupe.buffer.size=500*1024*1024\n#日志压缩去重时候的缓存空间，在空间允许的情况下，越大越好\nlog.cleaner.io.buffer.size=512*1024\n#日志清理时候用到的IO块大小一般不需要修改\nlog.cleaner.io.buffer.load.factor =0.9\n#日志清理中hash表的扩大因子一般不需要修改\nlog.cleaner.backoff.ms =15000\n#检查是否处罚日志清理的间隔\nlog.cleaner.min.cleanable.ratio=0.5\n#日志清理的频率控制，越大意味着更高效的清理，同时会存在一些空间上的浪费，会被topic创建时的指定参数覆盖\nlog.cleaner.delete.retention.ms =1day\n#对于压缩的日志保留的最长时间，也是客户端消费消息的最长时间，同log.retention.minutes的区别在于一个控制未压缩数据，一个控制压缩后的数据。会被topic创建时的指定参数覆盖\nlog.index.size.max.bytes =10*1024*1024\n#对于segment日志的索引文件大小限制，会被topic创建时的指定参数覆盖\nlog.index.interval.bytes =4096\n#当执行一个fetch操作后，需要一定的空间来扫描最近的offset大小，设置越大，代表扫描速度越快，但是也更好内存，一般情况下不需要搭理这个参数\nlog.flush.interval.messages=None\n#log文件”sync”到磁盘之前累积的消息条数,因为磁盘IO操作是一个慢操作,但又是一个”数据可靠性\"的必要手段,所以此参数的设置,需要在\"数据可靠性\"与\"性能\"之间做必要的权衡.如果此值过大,将会导致每次\"fsync\"的时间较长(IO阻塞),如果此值过小,将会导致\"fsync\"的次数较多,这也意味着整体的client请求有一定的延迟.物理server故障,将会导致没有fsync的消息丢失.\nlog.flush.scheduler.interval.ms =3000\n#检查是否需要固化到硬盘的时间间隔\nlog.flush.interval.ms = None\n#仅仅通过interval来控制消息的磁盘写入时机,是不足的.此参数用于控制\"fsync\"的时间间隔,如果消息量始终没有达到阀值,但是离上一次磁盘同步的时间间隔达到阀值,也将触发.\nlog.delete.delay.ms =60000\n#文件在索引中清除后保留的时间一般不需要去修改\nlog.flush.offset.checkpoint.interval.ms =60000\n#控制上次固化硬盘的时间点，以便于数据恢复一般不需要去修改\nauto.create.topics.enable =true\n#是否允许自动创建topic，若是false，就需要通过命令创建topic\ndefault.replication.factor =1\n#是否允许自动创建topic，若是false，就需要通过命令创建topic\nnum.partitions =1\n#每个topic的分区个数，若是在topic创建时候没有指定的话会被topic创建时的指定参数覆盖\n\n\n##这是轻量级的配置文件\nbroker.id=0\n#当前机器在集群中的唯一标识，和zookeeper的myid性质一样\nport=9092\n#当前kafka对外提供服务的端口默认是9092\nhost.name=192.168.11.11\n#这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。\nnum.network.threads=3\n#这个是borker进行网络处理的线程数\nnum.io.threads=8\n#这个是borker进行I/O处理的线程数\nlog.dirs=/home/hadoop/logs/kafka-logs \n#消息存放的目录，这个目录可以配置为“，”逗号分割的表达式，上面的num.io.threads要大于这个目录的个数这个目录，如果配置多个目录，新创建的topic他把消息持久化的地方是，当前以逗号分割的目录中，那个分区数最少就放那一个\nsocket.send.buffer.bytes=102400\n#发送缓冲区buffer大小，数据不是一下子就发送的，先回存储到缓冲区了到达一定的大小后在发送，能提高性能\nsocket.receive.buffer.bytes=102400\n#kafka接收缓冲区大小，当数据到达一定大小后在序列化到磁盘\nsocket.request.max.bytes=104857600\n#这个参数是向kafka请求消息或者向kafka发送消息的请请求的最大数，这个值不能超过java的堆栈大小\nnum.partitions=1\n#默认的分区数，一个topic默认1个分区数\nlog.retention.hours=168\n#默认消息的最大持久化时间，168小时，7天\nmessage.max.byte=5242880\n#消息保存的最大值5M\ndefault.replication.factor=2\n#kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务\nreplica.fetch.max.bytes=5242880\n#取消息的最大直接数\nlog.segment.bytes=1073741824\n#这个参数是：因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新起一个文件\nlog.retention.check.interval.ms=300000\n#每隔300000毫秒去检查上面配置的log失效时间（log.retention.hours=168 ），到目录查看是否有过期的消息如果有，删除\nlog.cleaner.enable=false\n#是否启用log压缩，一般不用启用，启用的话可以提高性能\nzookeeper.connect=192.168.11.11:2181,192.168.11.12:2181,192.168.11.13:2181\n#设置zookeeper的连接端口\n\n```\n\n# 以下是kafka中Leader,replicas配置参数\n\n```\ncontroller.socket.timeout.ms =30000\n#partition leader与replicas之间通讯时,socket的超时时间\ncontroller.message.queue.size=10\n#partition leader与replicas数据同步时,消息的队列尺寸\nreplica.lag.time.max.ms =10000\n#replicas响应partition leader的最长等待时间，若是超过这个时间，就将replicas列入ISR(in-sync replicas)，并认为它是死的，不会再加入管理中\nreplica.lag.max.messages =4000\n#如果follower落后与leader太多,将会认为此follower[或者说partition relicas]已经失效\n###通常,在follower与leader通讯时,因为网络延迟或者链接断开,总会导致replicas中消息同步滞后\n##如果消息之后太多,leader将认为此follower网络延迟较大或者消息吞吐能力有限,将会把此replicas迁移\n##到其他follower中.\n##在broker数量较少,或者网络不足的环境中,建议提高此值.\nreplica.socket.timeout.ms=30*1000\n#follower与leader之间的socket超时时间\nreplica.socket.receive.buffer.bytes=64*1024\n#leader复制时候的socket缓存大小\nreplica.fetch.max.bytes =1024*1024\n#replicas每次获取数据的最大大小\nreplica.fetch.wait.max.ms =500\n#replicas同leader之间通信的最大等待时间，失败了会重试\nreplica.fetch.min.bytes =1\n#fetch的最小数据尺寸,如果leader中尚未同步的数据不足此值,将会阻塞,直到满足条件\nnum.replica.fetchers=1\n#leader进行复制的线程数，增大这个数值会增加follower的IO\nreplica.high.watermark.checkpoint.interval.ms =5000\n#每个replica检查是否将最高水位进行固化的频率\ncontrolled.shutdown.enable =false\n#是否允许控制器关闭broker ,若是设置为true,会关闭所有在这个broker上的leader，并转移到其他broker\ncontrolled.shutdown.max.retries =3\n#控制器关闭的尝试次数\ncontrolled.shutdown.retry.backoff.ms =5000\n#每次关闭尝试的时间间隔\nleader.imbalance.per.broker.percentage =10\n#leader的不平衡比例，若是超过这个数值，会对分区进行重新的平衡\nleader.imbalance.check.interval.seconds =300\n#检查leader是否不平衡的时间间隔\noffset.metadata.max.bytes\n#客户端保留offset信息的最大空间大小\n\n```\n\n# kafka中zookeeper参数配置\n\n```\nzookeeper.connect = bigdata1:2181,bigdata2:2181,bigdata3:2181\n#zookeeper集群的地址，可以是多个，多个之间用逗号分割 hostname1:port1,hostname2:port2,hostname3:port3\nzookeeper.session.timeout.ms=6000\n#ZooKeeper的最大超时时间，就是心跳的间隔，若是没有反映，那么认为已经死了，不易过大\nzookeeper.connection.timeout.ms =6000\n#ZooKeeper的连接超时时间\nzookeeper.sync.time.ms =2000\n```","source":"_posts/Kafka集群配置及配置文件.md","raw":"---\ntitle: Kafka集群配置及配置文件\ndate: 2018年08月06日 22时15分52秒\ntags: [Kafka,大数据]\ncategories: 安装部署\ntoc: true\n---\n\n[TOC]\n\n\n\n\n\n# Kafka集群部署\n\nkafka默认推荐的是2.11开头的，如果系统中没有其他软件依赖于Scala的话，就使用2.11版本的 \nscala是依赖于zookeeper的，所以需要给zookeeper配置地址\n\n<!-- more -->\n\n\n\n# 1、下载安装包\n\n<http://kafka.apache.org/downloads.html> \n在linux中使用wget命令下载安装包 \nwget <http://mirrors.hust.edu.cn/apache/kafka/0.8.2.2/kafka_2.11-0.8.2.2.tgz>\n\n# 2、解压安装包\n\n```\ntar -zxvf kafka_2.11-0.8.2.2.tgz -C /home/bigdata/apps/kafka/\ncd /home/bigdata/apps/kafka/\nln -s kafka_2.11-0.8.2.2 kafka\n\n```\n\n# 3、修改配置文件\n\n配置文件有4个点\n\nhostname应该保持一致\n\n```\nbroker.id =0\n#每一个broker在集群中的唯一表示，要求是正数。当该服务器的IP地址发生改变时，broker.id没有变化，则不会影响consumers的消息情况\n  log.dirs=/home/bigdata/logs/kafka-logs\n#kafka数据的存放地址，多个地址的话用逗号分割 /data/kafka-logs-1，/data/kafka-logs-2\n  port =9092\n#broker server服务端口\n  message.max.bytes =6525000\n#表示消息体的最大大小，单位是字节\n  num.network.threads =3\n#broker处理消息的最大线程数，一般情况下不需要去修改 配置了三台服务器，所以选择三个\n  #num.io.threads =8\n#broker处理磁盘IO的线程数，数值应该大于你的硬盘数\n  #background.threads =4\n#一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改\n  queued.max.requests =500\n#等待IO线程处理的请求队列最大数，若是等待IO的请求超过这个数值，那么会停止接受外部消息，应该是一种自我保护机制。\n#broker的主机地址，若是设置了，那么会绑定到这个地址上，若是没有，会绑定到所有的接口上，并将其中之一发送到ZK，一般不设置\n  socket.send.buffer.bytes=102400\n#socket的发送缓冲区，socket的调优参数SO_SNDBUFF\n  socket.receive.buffer.bytes =102400\n#socket的接受缓冲区，socket的调优参数SO_RCVBUFF\n  socket.request.max.bytes =104857600\n#socket请求的最大数值，防止serverOOM，message.max.bytes必然要小于socket.request.max.bytes，会被topic创建时的指定参数覆盖\n   #log.segment.bytes =1024*1024*1024\n#topic的分区是以一堆segment文件存储的，这个控制每个segment的大小，会被topic创建时的指定参数覆盖\n  log.roll.hours =168\n  zookeeper.connect = bigdata1:2181,bigdata2:2181,bigdata3:2181\n#zookeeper集群的地址，可以是多个，多个之间用逗号分割 hostname1:port1,hostname2:port2,hostname3:port3\n  zookeeper.session.timeout.ms=6000\n#ZooKeeper的最大超时时间，就是心跳的间隔，若是没有反映，那么认为已经死了，不易过大\n  zookeeper.connection.timeout.ms =6000\n#ZooKeeper的连接超时时间\n  zookeeper.sync.time.ms =2000\n#host.name=bigdata1\n #broker的主机地址，若是设置了，那么会绑定到这个地址上，若是没有，会绑定到所有的接口上，并将其中之一发送到ZK，一般不设置,hostname为主机\n \n# 这个是轻量的配置文件\n#broker的全局唯一编号，不能重复\nbroker.id=0\n#用来监听连接的端口，producer或consumer将在此端口建立连接\nport=9092\n#处理网络请求的线程数量，集群中有几个节点就设置几个\nnum.network.threads=3\n#用来处理磁盘io的线程数量\nnum.io.threads=8\n#发送套接字的缓冲区大小\nsocket.send.buffer.bytes=102400\n#接受套接字的缓冲区大小\nsocket.receive.buffer.bytes=102400\n#请求套接字的缓冲区大小\nsocket.request.max.bytes=104857600\n#kafka运行日志存放的路径\nlog.dirs=/home/hadoop/logs/kafka\n#topic在当前broker上的分片个数\nnum.partitions=2\n#用来恢复和清理data下数据的线程数量\nnum.recovery.threads.per.data.dir=1\n#segment文件保留的最长时间。超时将会被删除\nlog.retention.hours=168\n#滚动删除生成心得segment文件的最大时间\nlog.roll.hour=168\n\n```\n\n `------------------`\n\n```\nip（重要），如果不改，则客户端会抛出：producer connection to localhost:9092 unsuccessful错误，\nadvertised.host.name=192.168.11.11\n\n```\n\n\\```\n\n```\ncp  /home/bigdata/apps/kafka/config/server.properties /home/bigdata/apps/kafka/config/server.properties.bak\nvi  /home/bigdata/apps/kafka/config/server.properties\n\n```\n\n# 4、分发安装包\n\n```\nscp -r /home/bigdata/apps/kafka/ bigdata2:/home/bigdata/apps/\n\n```\n\n然后分别在各机器上创建软连\n\n```\ncd /home/bigdata/apps/kafka\nln -s kafka_2.11-0.8.2.2 kafka\n\n```\n\n# 5、再次修改配置文件（重要）\n\n```\n依次修改各服务器上配置文件的的broker.id，分别是0,1,2不得重复。\n```\n\n# 需要配置kafka的环境变量\n\n# 6、启动集群\n\n**依次在各节点上启动kafka** \n后台启动 `nohup最后加一个&` \n\n```bash\nKAFKA_HOME/bin/kafka-server-start.sh  KAFKA_HOME/config/server.properties &\n```\n\n\n\n# 配置文件及注释\n\n```\nbroker.id=0\n#当前机器在集群中的唯一标识，和zookeeper的myid性质一样\nport=9092\n#当前kafka对外提供服务的端口默认是9092\nhost.name=192.168.11.11\nadvertised.host.name=192.168.11.11\n#这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。\nnum.network.threads=3\n#这个是borker进行网络处理的线程数\nnum.io.threads=8\n#这个是borker进行I/O处理的线程数\nlog.dirs=/home/bigdata/apps/kafka/kafkalogs/\n#消息存放的目录，这个目录可以配置为“，”逗号分割的表达式，上面的num.io.threads要大于这个目录的个数这个目录，如果配置多个目录，新创建的topic他把消息持久化的地方是，当前以逗号分割的目录中，那个分区数最少就放那一个\nsocket.send.buffer.bytes=102400\n#发送缓冲区buffer大小，数据不是一下子就发送的，先回存储到缓冲区了到达一定的大小后在发送，能提高性能\nsocket.receive.buffer.bytes=102400\n#kafka接收缓冲区大小，当数据到达一定大小后在序列化到磁盘\nsocket.request.max.bytes=104857600\n#这个参数是向kafka请求消息或者向kafka发送消息的请请求的最大数，这个值不能超过java的堆栈大小\nnum.partitions=1\n#默认的分区数，一个topic默认1个分区数\nlog.retention.hours=168\n#默认消息的最大持久化时间，168小时，7天\n\n\nmessage.max.bytes=5242880\n#消息保存的最大值5M\ndefault.replication.factor=2\n#kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务\nreplica.fetch.max.bytes=5242880\n#取消息的最大直接数\nlog.segment.bytes=1073741824\n#这个参数是：因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新起一个文件\nlog.retention.check.interval.ms=300000\n#每隔300000毫秒去检查上面配置的log失效时间（log.retention.hours=168 ），到目录查看是否有过期的消息如果有，删除\nlog.cleaner.enable=false\n#是否启用log压缩，一般不用启用，启用的话可以提高性能\nzookeeper.connect=192.168.11.11:2181,192.168.11.12:2181,192.168.11.13:2181\n#设置zookeeper的连接端口\n```\n\n# 常用\n\n```\nbroker.id =0\n#每一个broker在集群中的唯一表示，要求是正数。当该服务器的IP地址发生改变时，broker.id没有变化，则不会影响consumers的消息情况\nlog.dirs=/data/kafka-logs\n#kafka数据的存放地址，多个地址的话用逗号分割 /data/kafka-logs-1，/data/kafka-logs-2\nport =9092\n#broker server服务端口\nmessage.max.bytes =6525000\n#表示消息体的最大大小，单位是字节\nnum.network.threads =3\n#broker处理消息的最大线程数，一般情况下不需要去修改 配置了三台服务器，所以选择三个\nnum.io.threads =8\n#broker处理磁盘IO的线程数，数值应该大于你的硬盘数\nbackground.threads =4\n#一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改\nqueued.max.requests =500\n#等待IO线程处理的请求队列最大数，若是等待IO的请求超过这个数值，那么会停止接受外部消息，应该是一种自我保护机制。\nsocket.send.buffer.bytes=100*1024\n#socket的发送缓冲区，socket的调优参数SO_SNDBUFF\nsocket.receive.buffer.bytes =100*1024\n#socket的接受缓冲区，socket的调优参数SO_RCVBUFF\nsocket.request.max.bytes =100*1024*1024\n#socket请求的最大数值，防止serverOOM，message.max.bytes必然要小于socket.request.max.bytes，会被topic创建时的指定参数覆盖\nlog.segment.bytes =1024*1024*1024\n#topic的分区是以一堆segment文件存储的，这个控制每个segment的大小，会被topic创建时的指定参数覆盖\nlog.roll.hours =24*7\n\n```\n\n# 详解\n\n```\nbroker.id =0\n#每一个broker在集群中的唯一表示，要求是正数。当该服务器的IP地址发生改变时，broker.id没有变化，则不会影响consumers的消息情况\nlog.dirs=/data/kafka-logs\n#kafka数据的存放地址，多个地址的话用逗号分割 /data/kafka-logs-1，/data/kafka-logs-2\nport =9092\n#broker server服务端口\nmessage.max.bytes =6525000\n#表示消息体的最大大小，单位是字节\nnum.network.threads =4\n#broker处理消息的最大线程数，一般情况下不需要去修改\nnum.io.threads =8\n#broker处理磁盘IO的线程数，数值应该大于你的硬盘数\nbackground.threads =4\n#一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改\nqueued.max.requests =500\n#等待IO线程处理的请求队列最大数，若是等待IO的请求超过这个数值，那么会停止接受外部消息，应该是一种自我保护机制。\nhost.name\n#broker的主机地址，若是设置了，那么会绑定到这个地址上，若是没有，会绑定到所有的接口上，并将其中之一发送到ZK，一般不设置\nsocket.send.buffer.bytes=100*1024\n#socket的发送缓冲区，socket的调优参数SO_SNDBUFF\nsocket.receive.buffer.bytes =100*1024\n#socket的接受缓冲区，socket的调优参数SO_RCVBUFF\nsocket.request.max.bytes =100*1024*1024\n#socket请求的最大数值，防止serverOOM，message.max.bytes必然要小于socket.request.max.bytes，会被topic创建时的指定参数覆盖\nlog.segment.bytes =1024*1024*1024\n#topic的分区是以一堆segment文件存储的，这个控制每个segment的大小，会被topic创建时的指定参数覆盖\nlog.roll.hours =24*7\n#这个参数会在日志segment没有达到log.segment.bytes设置的大小，也会强制新建一个segment会被 topic创建时的指定参数覆盖\nlog.cleanup.policy = delete\n#日志清理策略选择有：delete和compact主要针对过期数据的处理，或是日志文件达到限制的额度，会被 topic创建时的指定参数覆盖\nlog.retention.minutes=60*24 # 一天后删除\n#数据存储的最大时间超过这个时间会根据log.cleanup.policy设置的策略处理数据，也就是消费端能够多久去消费数据\n#log.retention.bytes和log.retention.minutes任意一个达到要求，都会执行删除，会被topic创建时的指定参数覆盖\nlog.retention.bytes=-1\n#topic每个分区的最大文件大小，一个topic的大小限制 = 分区数*log.retention.bytes。-1没有大小限log.retention.bytes和log.retention.minutes任意一个达到要求，都会执行删除，会被topic创建时的指定参数覆盖\nlog.retention.check.interval.ms=5minutes\n#文件大小检查的周期时间，是否处罚 log.cleanup.policy中设置的策略\nlog.cleaner.enable=false\n#是否开启日志压缩\nlog.cleaner.threads = 2\n#日志压缩运行的线程数\nlog.cleaner.io.max.bytes.per.second=None\n#日志压缩时候处理的最大大小\nlog.cleaner.dedupe.buffer.size=500*1024*1024\n#日志压缩去重时候的缓存空间，在空间允许的情况下，越大越好\nlog.cleaner.io.buffer.size=512*1024\n#日志清理时候用到的IO块大小一般不需要修改\nlog.cleaner.io.buffer.load.factor =0.9\n#日志清理中hash表的扩大因子一般不需要修改\nlog.cleaner.backoff.ms =15000\n#检查是否处罚日志清理的间隔\nlog.cleaner.min.cleanable.ratio=0.5\n#日志清理的频率控制，越大意味着更高效的清理，同时会存在一些空间上的浪费，会被topic创建时的指定参数覆盖\nlog.cleaner.delete.retention.ms =1day\n#对于压缩的日志保留的最长时间，也是客户端消费消息的最长时间，同log.retention.minutes的区别在于一个控制未压缩数据，一个控制压缩后的数据。会被topic创建时的指定参数覆盖\nlog.index.size.max.bytes =10*1024*1024\n#对于segment日志的索引文件大小限制，会被topic创建时的指定参数覆盖\nlog.index.interval.bytes =4096\n#当执行一个fetch操作后，需要一定的空间来扫描最近的offset大小，设置越大，代表扫描速度越快，但是也更好内存，一般情况下不需要搭理这个参数\nlog.flush.interval.messages=None\n#log文件”sync”到磁盘之前累积的消息条数,因为磁盘IO操作是一个慢操作,但又是一个”数据可靠性\"的必要手段,所以此参数的设置,需要在\"数据可靠性\"与\"性能\"之间做必要的权衡.如果此值过大,将会导致每次\"fsync\"的时间较长(IO阻塞),如果此值过小,将会导致\"fsync\"的次数较多,这也意味着整体的client请求有一定的延迟.物理server故障,将会导致没有fsync的消息丢失.\nlog.flush.scheduler.interval.ms =3000\n#检查是否需要固化到硬盘的时间间隔\nlog.flush.interval.ms = None\n#仅仅通过interval来控制消息的磁盘写入时机,是不足的.此参数用于控制\"fsync\"的时间间隔,如果消息量始终没有达到阀值,但是离上一次磁盘同步的时间间隔达到阀值,也将触发.\nlog.delete.delay.ms =60000\n#文件在索引中清除后保留的时间一般不需要去修改\nlog.flush.offset.checkpoint.interval.ms =60000\n#控制上次固化硬盘的时间点，以便于数据恢复一般不需要去修改\nauto.create.topics.enable =true\n#是否允许自动创建topic，若是false，就需要通过命令创建topic\ndefault.replication.factor =1\n#是否允许自动创建topic，若是false，就需要通过命令创建topic\nnum.partitions =1\n#每个topic的分区个数，若是在topic创建时候没有指定的话会被topic创建时的指定参数覆盖\n\n\n##这是轻量级的配置文件\nbroker.id=0\n#当前机器在集群中的唯一标识，和zookeeper的myid性质一样\nport=9092\n#当前kafka对外提供服务的端口默认是9092\nhost.name=192.168.11.11\n#这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。\nnum.network.threads=3\n#这个是borker进行网络处理的线程数\nnum.io.threads=8\n#这个是borker进行I/O处理的线程数\nlog.dirs=/home/hadoop/logs/kafka-logs \n#消息存放的目录，这个目录可以配置为“，”逗号分割的表达式，上面的num.io.threads要大于这个目录的个数这个目录，如果配置多个目录，新创建的topic他把消息持久化的地方是，当前以逗号分割的目录中，那个分区数最少就放那一个\nsocket.send.buffer.bytes=102400\n#发送缓冲区buffer大小，数据不是一下子就发送的，先回存储到缓冲区了到达一定的大小后在发送，能提高性能\nsocket.receive.buffer.bytes=102400\n#kafka接收缓冲区大小，当数据到达一定大小后在序列化到磁盘\nsocket.request.max.bytes=104857600\n#这个参数是向kafka请求消息或者向kafka发送消息的请请求的最大数，这个值不能超过java的堆栈大小\nnum.partitions=1\n#默认的分区数，一个topic默认1个分区数\nlog.retention.hours=168\n#默认消息的最大持久化时间，168小时，7天\nmessage.max.byte=5242880\n#消息保存的最大值5M\ndefault.replication.factor=2\n#kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务\nreplica.fetch.max.bytes=5242880\n#取消息的最大直接数\nlog.segment.bytes=1073741824\n#这个参数是：因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新起一个文件\nlog.retention.check.interval.ms=300000\n#每隔300000毫秒去检查上面配置的log失效时间（log.retention.hours=168 ），到目录查看是否有过期的消息如果有，删除\nlog.cleaner.enable=false\n#是否启用log压缩，一般不用启用，启用的话可以提高性能\nzookeeper.connect=192.168.11.11:2181,192.168.11.12:2181,192.168.11.13:2181\n#设置zookeeper的连接端口\n\n```\n\n# 以下是kafka中Leader,replicas配置参数\n\n```\ncontroller.socket.timeout.ms =30000\n#partition leader与replicas之间通讯时,socket的超时时间\ncontroller.message.queue.size=10\n#partition leader与replicas数据同步时,消息的队列尺寸\nreplica.lag.time.max.ms =10000\n#replicas响应partition leader的最长等待时间，若是超过这个时间，就将replicas列入ISR(in-sync replicas)，并认为它是死的，不会再加入管理中\nreplica.lag.max.messages =4000\n#如果follower落后与leader太多,将会认为此follower[或者说partition relicas]已经失效\n###通常,在follower与leader通讯时,因为网络延迟或者链接断开,总会导致replicas中消息同步滞后\n##如果消息之后太多,leader将认为此follower网络延迟较大或者消息吞吐能力有限,将会把此replicas迁移\n##到其他follower中.\n##在broker数量较少,或者网络不足的环境中,建议提高此值.\nreplica.socket.timeout.ms=30*1000\n#follower与leader之间的socket超时时间\nreplica.socket.receive.buffer.bytes=64*1024\n#leader复制时候的socket缓存大小\nreplica.fetch.max.bytes =1024*1024\n#replicas每次获取数据的最大大小\nreplica.fetch.wait.max.ms =500\n#replicas同leader之间通信的最大等待时间，失败了会重试\nreplica.fetch.min.bytes =1\n#fetch的最小数据尺寸,如果leader中尚未同步的数据不足此值,将会阻塞,直到满足条件\nnum.replica.fetchers=1\n#leader进行复制的线程数，增大这个数值会增加follower的IO\nreplica.high.watermark.checkpoint.interval.ms =5000\n#每个replica检查是否将最高水位进行固化的频率\ncontrolled.shutdown.enable =false\n#是否允许控制器关闭broker ,若是设置为true,会关闭所有在这个broker上的leader，并转移到其他broker\ncontrolled.shutdown.max.retries =3\n#控制器关闭的尝试次数\ncontrolled.shutdown.retry.backoff.ms =5000\n#每次关闭尝试的时间间隔\nleader.imbalance.per.broker.percentage =10\n#leader的不平衡比例，若是超过这个数值，会对分区进行重新的平衡\nleader.imbalance.check.interval.seconds =300\n#检查leader是否不平衡的时间间隔\noffset.metadata.max.bytes\n#客户端保留offset信息的最大空间大小\n\n```\n\n# kafka中zookeeper参数配置\n\n```\nzookeeper.connect = bigdata1:2181,bigdata2:2181,bigdata3:2181\n#zookeeper集群的地址，可以是多个，多个之间用逗号分割 hostname1:port1,hostname2:port2,hostname3:port3\nzookeeper.session.timeout.ms=6000\n#ZooKeeper的最大超时时间，就是心跳的间隔，若是没有反映，那么认为已经死了，不易过大\nzookeeper.connection.timeout.ms =6000\n#ZooKeeper的连接超时时间\nzookeeper.sync.time.ms =2000\n```","slug":"Kafka集群配置及配置文件","published":1,"updated":"2018-08-13T07:21:34.526Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43w400242tpbtaw9tjzj"},{"title":"linux命令","date":"2018-05-10T07:37:28.983Z","toc":true,"_content":"\n\n简单linux命令\n\n`nohup   & ` \n后台运行\n\n<!-- more -->\n\n## 文件查找\n`find / -type f -size +10G`\n在Linux下如何让文件让按大小单位为M,G等易读格式，S size大小排序。  `ls -lhS`\n`du -h * | sort -n `  \n当然您也可以结合管道文件夹内最大的几个文件  ` du -h * | sort -n|head`\n动态显示机器各端口的链接情况`while :; do netstat -apn | grep \":80\" | wc -l; sleep 1; done`\n## sed\n更改第一行 `sed -i '1s/.*//'`     sed -i '1s/.*/想更改的内容/'\n\n```bash\nssh root@slave01 \"sed -i '6c advertised.host.name=slave01 ' $KAFKA_HOME/config/server.properties\"\n```\n\n删除第一行`sed -i '1d'  `     sed -i '1d' 文件名\n插入第一行 `sed -i '1i\\' `       sed -i ‘1i\\内容‘ 文件名\n\n## cpu\n`cat /proc/cpuinfo | grep processor | wc -l`\n`lscpu`\n\n## sz rz与服务器交互上传下载文件\n\n`sudo yum install lrzsz -y`\n\n## 挂载\n\n `sshfs  root@192.168.73.12:/home/ /csdn/win10/`\n\n即：sshfs 用户名@远程主机IP:远程主机路径 本地挂载点\nsshfs  root@master:/usr/hadoop  /usr/hive/hadoop\n\n## 查看端口是否被监听\n\n也可验证对应端口程序是否启动\n`netstat -nl|grep 10000`\n\n\n\n# 虚拟机共享文件夹\n\n在virtualbox中设置共享文件夹的share名称对应mac的目录\n虚拟机中的目录\nsudo mount -t vboxsf vagrant /Users/yaosong","source":"_posts/Linux命令积累.md","raw":"---\ntitle:  linux命令\ndate: 2018年08月11日 02时55分44秒\ntags:  [linux,开发]\ncategories: Linux\ntoc: true\n---\n\n\n简单linux命令\n\n`nohup   & ` \n后台运行\n\n<!-- more -->\n\n## 文件查找\n`find / -type f -size +10G`\n在Linux下如何让文件让按大小单位为M,G等易读格式，S size大小排序。  `ls -lhS`\n`du -h * | sort -n `  \n当然您也可以结合管道文件夹内最大的几个文件  ` du -h * | sort -n|head`\n动态显示机器各端口的链接情况`while :; do netstat -apn | grep \":80\" | wc -l; sleep 1; done`\n## sed\n更改第一行 `sed -i '1s/.*//'`     sed -i '1s/.*/想更改的内容/'\n\n```bash\nssh root@slave01 \"sed -i '6c advertised.host.name=slave01 ' $KAFKA_HOME/config/server.properties\"\n```\n\n删除第一行`sed -i '1d'  `     sed -i '1d' 文件名\n插入第一行 `sed -i '1i\\' `       sed -i ‘1i\\内容‘ 文件名\n\n## cpu\n`cat /proc/cpuinfo | grep processor | wc -l`\n`lscpu`\n\n## sz rz与服务器交互上传下载文件\n\n`sudo yum install lrzsz -y`\n\n## 挂载\n\n `sshfs  root@192.168.73.12:/home/ /csdn/win10/`\n\n即：sshfs 用户名@远程主机IP:远程主机路径 本地挂载点\nsshfs  root@master:/usr/hadoop  /usr/hive/hadoop\n\n## 查看端口是否被监听\n\n也可验证对应端口程序是否启动\n`netstat -nl|grep 10000`\n\n\n\n# 虚拟机共享文件夹\n\n在virtualbox中设置共享文件夹的share名称对应mac的目录\n虚拟机中的目录\nsudo mount -t vboxsf vagrant /Users/yaosong","slug":"Linux命令积累","published":1,"updated":"2018-08-24T02:14:22.229Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43w500282tpbpymizqp1"},{"title":"Linux安装mysql","date":"2018-07-24T15:42:18.647Z","toc":true,"_content":"\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu550945khj318w0n4wgq.jpg)\n\n在linux yum安装mysql\n\n<!--more -->\n\n```Bash\nyum install -y mysql-server\nchkconfig --add mysqld\nchkconfig mysqld on\nchkconfig --list mysqld\nservice mysqld start\nmysql -u root -p\nEnter password:           //默认密码为空，输入后回车即可\nset password for root@localhost=password('root'); 　　密码设置为root\nset password for root@=password('root');\n默认情况下Mysql只允许本地登录，所以只需配置root@localhost就好\n设置所有ip访问密码为root\nset password for root@%=password('root'); 　　　　　　密码设置为root （其实这一步可以不配）\n设置master访问密码为root\nset password for root@master=password('root'); 　　密码设置为root （其实这一步可以不配）\n查询密码\nselect user,host,password from mysql.user;  　　查看密码是否设置成功\n设置所有ip可以通过root访问\nGRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;\nGRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' IDENTIFIED BY 'hive' WITH GRANT OPTION;\n\n\n```\n\n","source":"_posts/Linux安装mysql.md","raw":"---\ntitle:  Linux安装mysql\ndate: 2018年06月21日 22时15分52秒\ntags:  [Linux,Mysql]\ncategories: 安装部署\ntoc: true\n---\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu550945khj318w0n4wgq.jpg)\n\n在linux yum安装mysql\n\n<!--more -->\n\n```Bash\nyum install -y mysql-server\nchkconfig --add mysqld\nchkconfig mysqld on\nchkconfig --list mysqld\nservice mysqld start\nmysql -u root -p\nEnter password:           //默认密码为空，输入后回车即可\nset password for root@localhost=password('root'); 　　密码设置为root\nset password for root@=password('root');\n默认情况下Mysql只允许本地登录，所以只需配置root@localhost就好\n设置所有ip访问密码为root\nset password for root@%=password('root'); 　　　　　　密码设置为root （其实这一步可以不配）\n设置master访问密码为root\nset password for root@master=password('root'); 　　密码设置为root （其实这一步可以不配）\n查询密码\nselect user,host,password from mysql.user;  　　查看密码是否设置成功\n设置所有ip可以通过root访问\nGRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;\nGRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' IDENTIFIED BY 'hive' WITH GRANT OPTION;\n\n\n```\n\n","slug":"Linux安装mysql","published":1,"updated":"2018-08-10T17:52:56.914Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43w6002b2tpb2tzro98h"},{"title":"MapReduce中Shuffle中的机制","date":"2018-08-20T02:12:26.791Z","toc":true,"_content":"\n[TOC]\n\n官方的shuffle流程\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuhbmle6ksj30ff07dglu.jpg)\n\n# shuffle原理\n\n提到MapReduce，就不得不提一下shuffle。\n\nMapReduce 框架的核心步骤主要分两部分：Map 和Reduce，一个是独立并发，一个是汇聚。当你向MapReduce 框架提交一个计算作业时，它会首先把计算作业拆分成若干个Map 任务，然后分配到不同的节点上去执行，每一个Map 任务处理输入数据中的一部分，当Map 任务完成后，它会生成一些中间文件，这些中间文件将会作为Reduce 任务的输入数据。Reduce 任务的主要目标就是把前面若干个Map 的输出汇总到一起并输出。\n\n<!-- more -->\n\n 我们知道每个reduce task输入的key都是按照key排序的。但是每个map的输出只是简单的key-value而非key-valuelist，所以shuffle的工作就是将map输出转化为reducer的输入的过程。Shuffle过程是指map产生输出结果开始，包括系统执行分区partition，排序sort，聚合Combiner（如有）以及传送map的输出到Reducer作为输入的过程。\n\n## shuffle过程分析\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuhbuwapflj30zc0j4mxv.jpg)\n\n\n\n\n\n## map端\n\n1. 从map段开始分析，当Map开始产生输出的时候，并不是简单吧数据写到磁盘，因为频繁的操作会导致性能严重下降，首先将数据写入到一个环形缓冲区（每个maptask都会有一个环形缓冲区，默认100M，可以通过io.sort.mb属性来设置具体的大小）并做一些预排序，以提升效率，当缓冲区中的数据量达到一个特定的阀值(io.sort.mb * io.sort.spill.percent，其中io.sort.spill.percent 默认是0.80，即默认为 80MB），溢写线程启动。\n\n2. 系统将会启动一个后台线程把缓冲区中的内容spill 到磁盘。即会锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。在spill过程中，Map的输出将会继续写入到缓冲区，但如果缓冲区已经满了，Map就会被阻塞直到spill完成。spill线程在把缓冲区的数据写到磁盘前，会对他\n\n   进行一个**二次排序**，**首先根据数据所属的partition排序（快速排序），然后每个partition中再按Key排序**。输出包括一个**索引文件和数据文件**。\n\n3. 如果设定了Combiner，将在排序输出的基础上进行。Combiner就是一个Mini Reducer，它在执行Map任务的节点本身运行，先对Map的输出作一次简单的Reduce，有些数据可能像这样：`“a”/1， “a”/1， “a”/1，`会合并成 `“a”/3`，使得更少的数据会被写入磁盘和传送到Reducer。\n\n4. Spill文件保存在由mapred.local.dir指定的目录中，Map任务结束后删除。每当内存中的数据达到spill阀值的时候，都会产生一个新的spill文件，所以在Map任务写完他的最后一个输出记录的时候，可能会有多个spill文件，在Map任务完成前，所有的spill文件将会被归并排序为一个索引文件和数据文件。这是一个多路归并过程，最大归并路数由io.sort.factor 控制(默认是10)。比如：“a”从某个map task读取过来时值是7，从另外一个map 读取时值是6，因为它们有相同的key，所以得merge成group。什么是group。对于“a”就是像这样的：{“a”, [7, 6, 2, …]}，数组中的值就是从不同溢写文件中读取出来的，然后再把这些值加起来。请注意，因为merge是将多个溢写文件合并到一个文件，所以可能也有相同的key存在。如果设定了Combiner，会使用combiner来合并相同key，这是map端的结果。\n\n\n\n## map端与reduce端的交互\n\nReduce是怎么知道从哪些TaskTrackers中获取Map的输出呢？当Map任务完成之后，会通知他们的父TaskTracker，告知状态更新，然后TaskTracker再转告JobTracker，这些通知信息是通过心跳通信机制传输的，因此针对以一个特定的作业，jobtracker知道Map输出与tasktrackers的映射关系。Reducer中有一个线程会间歇的向JobTracker询问Map输出的地址，直到把所有的数据都取到。在Reducer取走了Map输出之后，TaskTracker不会立即删除这些数据，因为Reducer可能会失败，他们会在整个作业完成之后，JobTracker告知他们要删除的时候才去删除\n\n\n\nmap端的所有工作结束后，最终生成的这个文件也存放在TaskTracker够得着的某个本地目录内。每个reduce task不断地通过RPC从JobTracker那里获取map task是否完成的信息，如果reduce task得到通知，获知某台TaskTracker上的map task执行完成，Shuffle的后半段过程开始启动。\n简单地说，reduce task在执行之前的工作就是不断地拉取当前job里每个map task的最终结果，然后对从不同地方拉取过来的数据不断地做merge，也最终形成一个文件作为reduce task的输入文件。\n\n\n\n\n\n## reduce端过程\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fuhbdybylfj30z40hyq3f.jpg)\n\n1. Copy过程，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。\n2. Merge阶段。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。这里需要强调的是，merge有三种形式：1)内存到内存 （默认不启用） 2)内存到磁盘  3)磁盘到磁盘。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的那个文件。\n3. Reducer的输入文件。不断地merge后，最后会生成一个文件。这个文件可能存在于磁盘上，也可能存在于内存中。就读取速度来说对于内存中，直接作为Reducer的输入，但默认情况下，这个文件是存放于磁盘中的。当Reducer的输入文件已定，整个Shuffle才最终结束。然后就是Reducer执行，一般是把结果放到HDFS上。\n\n\n\n\n\n## Speculative Execution\n\n是指当一个job的所有task都在running的时候，当某个task的进度比平均进度慢时才会启动一个和当前Task一模一样的任务，当其中一个task完成之后另外一个会被中止，所以Speculative Task不是重复Task而是对Task执行时候的一种优化策略\n\n\n\n\n\n# 任务分片与hdfs文件大小及文件块之间的关系\n\n在上面mapReduce中可以看到，任务的执行是基于hdfs文件的，任务分片和文件大小，文件块大小都有一定的联系。\n\n任务切片：将任务划分成切片**，一个切片交给一个task实例**处理，只是一个逻辑的偏移量划分而已\n\n1.在map task执行时，它的输入数据来源于HDFS的block，当然在MapReduce概念中，map task只读取split。Split与block的对应关系可能是多对一，默认是一对一。\n\n采用的算法是：\n\n**分片大小范围可以在 mapred-site.xml 中设置，mapred.min.split.size mapred.max.split.size，**minSplitSize 大小默认为 1B，**maxSplitSize 大小默认为 Long.MAX_VALUE = 9223372036854775807**\n\n```java\nminiSize = 1\nmaxSize = Long.MAXVALUE\nsplitSize = Math.max(miniSize,Math.min(maxSize,blockSize))\n```\n\n默认情况下，任务分片的大小为hdfs的blocksize 也就是块大小\n\n**所以在我们没有设置分片的范围的时候，分片大小是由 block 块大小决定的，和它的大小一样。比如把一个 258MB 的文件上传到 HDFS 上，假设 block 块大小是 128MB，那么它就会被分成三个 block 块，与之对应产生三个 split**，所以最终会产生三个 map task。我又发现了另一个问题，第三个 block 块里存的文件大小只有 2MB，而它的 block 块大小是 128MB，那它实际占用 Linux file system 的多大空间？**\n\n**答案是实际的文件大小，而非一个块的大小**\n\n\n\n切片的流程\n\n1. 遍历输入目录下的文件，得到文件集合 list\n\n2. 遍历文件集合list，循环操作集合下的文件\n\n   获取文件的blocksize 文件块，获取文件的长度，得到切片信息（split[文件路径,切片编号,偏移量范围]）,将各切片对象放入到一个splitList里面\n\n\n3. 遍历完成后，将切片信息splitList序列化到一个split描述文件中\n\n   ![](https://ws1.sinaimg.cn/large/006tNbRwgy1fufyt54ouvj30cq03c0so.jpg)\n\n\n\n\n\n默认情况下，默认的TextInputFormat对任务切片是按文件规划切片，不管文件多小，都会是一个单独的切片，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下\n\n\n\n对于以上原理，如果读取的是很多小文件，会产生大量的小切片，造成大量的maptask运行，对应的解决方法：\n\n1. 将小文件合并之后再上传到hdfs\n2. 如果小文件已经上传了，可以写MapReduce程序将小文件合并\n3. 可以用另一种InputFormat：CombineInputFormat(它可以将多个文件划分到一个切片中)\n\n\n\n\n\n","source":"_posts/MapReduce中Shuffle中的机制.md","raw":"---\ntitle: MapReduce中Shuffle中的机制\ndate: 2018年08月06日 22时15分52秒\ntags: [Hadoop,原理]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n官方的shuffle流程\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuhbmle6ksj30ff07dglu.jpg)\n\n# shuffle原理\n\n提到MapReduce，就不得不提一下shuffle。\n\nMapReduce 框架的核心步骤主要分两部分：Map 和Reduce，一个是独立并发，一个是汇聚。当你向MapReduce 框架提交一个计算作业时，它会首先把计算作业拆分成若干个Map 任务，然后分配到不同的节点上去执行，每一个Map 任务处理输入数据中的一部分，当Map 任务完成后，它会生成一些中间文件，这些中间文件将会作为Reduce 任务的输入数据。Reduce 任务的主要目标就是把前面若干个Map 的输出汇总到一起并输出。\n\n<!-- more -->\n\n 我们知道每个reduce task输入的key都是按照key排序的。但是每个map的输出只是简单的key-value而非key-valuelist，所以shuffle的工作就是将map输出转化为reducer的输入的过程。Shuffle过程是指map产生输出结果开始，包括系统执行分区partition，排序sort，聚合Combiner（如有）以及传送map的输出到Reducer作为输入的过程。\n\n## shuffle过程分析\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuhbuwapflj30zc0j4mxv.jpg)\n\n\n\n\n\n## map端\n\n1. 从map段开始分析，当Map开始产生输出的时候，并不是简单吧数据写到磁盘，因为频繁的操作会导致性能严重下降，首先将数据写入到一个环形缓冲区（每个maptask都会有一个环形缓冲区，默认100M，可以通过io.sort.mb属性来设置具体的大小）并做一些预排序，以提升效率，当缓冲区中的数据量达到一个特定的阀值(io.sort.mb * io.sort.spill.percent，其中io.sort.spill.percent 默认是0.80，即默认为 80MB），溢写线程启动。\n\n2. 系统将会启动一个后台线程把缓冲区中的内容spill 到磁盘。即会锁定这80MB的内存，执行溢写过程。Map task的输出结果还可以往剩下的20MB内存中写，互不影响。在spill过程中，Map的输出将会继续写入到缓冲区，但如果缓冲区已经满了，Map就会被阻塞直到spill完成。spill线程在把缓冲区的数据写到磁盘前，会对他\n\n   进行一个**二次排序**，**首先根据数据所属的partition排序（快速排序），然后每个partition中再按Key排序**。输出包括一个**索引文件和数据文件**。\n\n3. 如果设定了Combiner，将在排序输出的基础上进行。Combiner就是一个Mini Reducer，它在执行Map任务的节点本身运行，先对Map的输出作一次简单的Reduce，有些数据可能像这样：`“a”/1， “a”/1， “a”/1，`会合并成 `“a”/3`，使得更少的数据会被写入磁盘和传送到Reducer。\n\n4. Spill文件保存在由mapred.local.dir指定的目录中，Map任务结束后删除。每当内存中的数据达到spill阀值的时候，都会产生一个新的spill文件，所以在Map任务写完他的最后一个输出记录的时候，可能会有多个spill文件，在Map任务完成前，所有的spill文件将会被归并排序为一个索引文件和数据文件。这是一个多路归并过程，最大归并路数由io.sort.factor 控制(默认是10)。比如：“a”从某个map task读取过来时值是7，从另外一个map 读取时值是6，因为它们有相同的key，所以得merge成group。什么是group。对于“a”就是像这样的：{“a”, [7, 6, 2, …]}，数组中的值就是从不同溢写文件中读取出来的，然后再把这些值加起来。请注意，因为merge是将多个溢写文件合并到一个文件，所以可能也有相同的key存在。如果设定了Combiner，会使用combiner来合并相同key，这是map端的结果。\n\n\n\n## map端与reduce端的交互\n\nReduce是怎么知道从哪些TaskTrackers中获取Map的输出呢？当Map任务完成之后，会通知他们的父TaskTracker，告知状态更新，然后TaskTracker再转告JobTracker，这些通知信息是通过心跳通信机制传输的，因此针对以一个特定的作业，jobtracker知道Map输出与tasktrackers的映射关系。Reducer中有一个线程会间歇的向JobTracker询问Map输出的地址，直到把所有的数据都取到。在Reducer取走了Map输出之后，TaskTracker不会立即删除这些数据，因为Reducer可能会失败，他们会在整个作业完成之后，JobTracker告知他们要删除的时候才去删除\n\n\n\nmap端的所有工作结束后，最终生成的这个文件也存放在TaskTracker够得着的某个本地目录内。每个reduce task不断地通过RPC从JobTracker那里获取map task是否完成的信息，如果reduce task得到通知，获知某台TaskTracker上的map task执行完成，Shuffle的后半段过程开始启动。\n简单地说，reduce task在执行之前的工作就是不断地拉取当前job里每个map task的最终结果，然后对从不同地方拉取过来的数据不断地做merge，也最终形成一个文件作为reduce task的输入文件。\n\n\n\n\n\n## reduce端过程\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fuhbdybylfj30z40hyq3f.jpg)\n\n1. Copy过程，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求map task所在的TaskTracker获取map task的输出文件。因为map task早已结束，这些文件就归TaskTracker管理在本地磁盘中。\n2. Merge阶段。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置，因为Shuffle阶段Reducer不运行，所以应该把绝大部分的内存都给Shuffle用。这里需要强调的是，merge有三种形式：1)内存到内存 （默认不启用） 2)内存到磁盘  3)磁盘到磁盘。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的那个文件。\n3. Reducer的输入文件。不断地merge后，最后会生成一个文件。这个文件可能存在于磁盘上，也可能存在于内存中。就读取速度来说对于内存中，直接作为Reducer的输入，但默认情况下，这个文件是存放于磁盘中的。当Reducer的输入文件已定，整个Shuffle才最终结束。然后就是Reducer执行，一般是把结果放到HDFS上。\n\n\n\n\n\n## Speculative Execution\n\n是指当一个job的所有task都在running的时候，当某个task的进度比平均进度慢时才会启动一个和当前Task一模一样的任务，当其中一个task完成之后另外一个会被中止，所以Speculative Task不是重复Task而是对Task执行时候的一种优化策略\n\n\n\n\n\n# 任务分片与hdfs文件大小及文件块之间的关系\n\n在上面mapReduce中可以看到，任务的执行是基于hdfs文件的，任务分片和文件大小，文件块大小都有一定的联系。\n\n任务切片：将任务划分成切片**，一个切片交给一个task实例**处理，只是一个逻辑的偏移量划分而已\n\n1.在map task执行时，它的输入数据来源于HDFS的block，当然在MapReduce概念中，map task只读取split。Split与block的对应关系可能是多对一，默认是一对一。\n\n采用的算法是：\n\n**分片大小范围可以在 mapred-site.xml 中设置，mapred.min.split.size mapred.max.split.size，**minSplitSize 大小默认为 1B，**maxSplitSize 大小默认为 Long.MAX_VALUE = 9223372036854775807**\n\n```java\nminiSize = 1\nmaxSize = Long.MAXVALUE\nsplitSize = Math.max(miniSize,Math.min(maxSize,blockSize))\n```\n\n默认情况下，任务分片的大小为hdfs的blocksize 也就是块大小\n\n**所以在我们没有设置分片的范围的时候，分片大小是由 block 块大小决定的，和它的大小一样。比如把一个 258MB 的文件上传到 HDFS 上，假设 block 块大小是 128MB，那么它就会被分成三个 block 块，与之对应产生三个 split**，所以最终会产生三个 map task。我又发现了另一个问题，第三个 block 块里存的文件大小只有 2MB，而它的 block 块大小是 128MB，那它实际占用 Linux file system 的多大空间？**\n\n**答案是实际的文件大小，而非一个块的大小**\n\n\n\n切片的流程\n\n1. 遍历输入目录下的文件，得到文件集合 list\n\n2. 遍历文件集合list，循环操作集合下的文件\n\n   获取文件的blocksize 文件块，获取文件的长度，得到切片信息（split[文件路径,切片编号,偏移量范围]）,将各切片对象放入到一个splitList里面\n\n\n3. 遍历完成后，将切片信息splitList序列化到一个split描述文件中\n\n   ![](https://ws1.sinaimg.cn/large/006tNbRwgy1fufyt54ouvj30cq03c0so.jpg)\n\n\n\n\n\n默认情况下，默认的TextInputFormat对任务切片是按文件规划切片，不管文件多小，都会是一个单独的切片，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下\n\n\n\n对于以上原理，如果读取的是很多小文件，会产生大量的小切片，造成大量的maptask运行，对应的解决方法：\n\n1. 将小文件合并之后再上传到hdfs\n2. 如果小文件已经上传了，可以写MapReduce程序将小文件合并\n3. 可以用另一种InputFormat：CombineInputFormat(它可以将多个文件划分到一个切片中)\n\n\n\n\n\n","slug":"MapReduce中Shuffle中的机制","published":1,"updated":"2018-08-21T10:06:40.273Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43w8002e2tpb1muuuud6"},{"title":"MyBatis相关注解","date":"2018-05-17T12:09:55.000Z","toc":true,"top":true,"_content":"![](https://ws4.sinaimg.cn/large/006tNbRwgy1fu551ai6pij30mm064q3r.jpg)\n\n现接触MyBatic记录一些注解\n<!--more-->\n自动生成主键  \n\n        可以使用 @Options 注解的 userGeneratedKeys 和 keyProperty 属性让数据库产生 auto_increment（自增长）列的值，然后将生成的值设置到输入参数对象的属性中。 \n```java\n@Insert(\"insert into students(name,sex,age)  values(#{name},#{sex},#{age}\")  \n@Options(useGeneratedKeys = true, keyProperty =\"userId\")  \nint insertUser(User user);   \n```\n将自增的Id存入到userId属性中","source":"_posts/MyBatis注解.md","raw":"---\ntitle: MyBatis相关注解\ndate: 2018-05-17 20:09:55\ntags: [Java,SSH]\ncategories: 框架\ntoc: true\ntop: true\n---\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fu551ai6pij30mm064q3r.jpg)\n\n现接触MyBatic记录一些注解\n<!--more-->\n自动生成主键  \n\n        可以使用 @Options 注解的 userGeneratedKeys 和 keyProperty 属性让数据库产生 auto_increment（自增长）列的值，然后将生成的值设置到输入参数对象的属性中。 \n```java\n@Insert(\"insert into students(name,sex,age)  values(#{name},#{sex},#{age}\")  \n@Options(useGeneratedKeys = true, keyProperty =\"userId\")  \nint insertUser(User user);   \n```\n将自增的Id存入到userId属性中","slug":"MyBatis注解","published":1,"updated":"2018-08-10T17:52:53.516Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43w9002i2tpbnqf7wiej"},{"title":"Scala基本使用-杂记","date":"2018-08-15T15:52:20.469Z","toc":true,"_content":"\n[TOC]\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fukx028pz0j313t0op400.jpg)\n\nVar和val\n\nvar 修饰的变量可改变，val 修饰的变量不可改变；但真的如此吗？事实上，var 修饰的对象引用可以改变，val 修饰的则不可改变，但对象的状态却是可以改变的。\n\n## 定义方法\n```scala\ndef m1(x:Int,y:Int):Int=x*y\n```\n\n<!-- more -->\n\n## 函数的定义=>\n\n```scala\nval func:Int =>String={x=x.toString}\n也可以写成\nval func1=(x:Int)=>x.toString\n```\n\n\n## 函数和方法的区别\n```scala\n定义一个方法\ndef m2(f:(Int,Int)=>Int) =f(2,6)\n定义一个函数\nval f2 =(x:Int,y:Int) =>x-y\n调用\nf2(m2)\n```\n\n\n## 神奇的下划线\n将方法转换成函数\n\n```scala\ndef m2(x:Int,y:Int):Int = x+y\nval f2(a:Int,y:Int)=>x+y\nval f2 = m2 _\n```\n\n\n\n\n# 数组、映射、元组、集合\n## 数组\n```scala\nval arr = new Array[Int](10) //创建数组，无值\nval arr1 = Array(1,2,3,4,5)  //直接实例化\nfor(e<- arr) **yield** e*2; 用yield关键字可以生成一个新的数组 生成的类型和循环的类型是一样的\n```\n\n\n## map方法\nmap方法是将每一个元素拿来操作\narr.map(_*2) map方法更好用  map是排序？\n\n```scala\nval a = Array(1,2,3,4,5)\na.map((x:Int)=>x*10)//匿名函数\n```\n\n\n由于知道a中的数据类型 可以将Int省略\na.map(x=>x*10) \n还可以用_占位符，进行进一步的省略\na.map(_ * 10)\n\n\n所有偶数取出来然后再乘以10\n```scala\narr.filter((x:Int)=>x%2==0)\narr.filter(x=>x%2 == 0)\narr.filter(_%2==0)\narr.filter(_%2==0).map(_*10)\nval p =  println _\n```\n\n\n\n可以将方法转换成函数用利用 _\n### trait 关键字\n## 数组的常用函数\n在超类\nTraversableLike中定义了这些函数\n```scala\nval arr = Array(1,2,3,4,5,6)\narr.sum 总数\narr.sorted 排序\narr.sorted.reverse 逆序\narr.sortBy(x=>x)按照本身来排序\narr.sortWith(_>_)从大到小排序 < 从小到大排序\narr.sortWith((x,y))\n## \n```\n\n映射 类比java中的map\n```scala\nval m = Map(\"a\"->1,\"b\"->2)\nm(\"a\") 取值 \n```\n\n\n里边的值不能更改\n如果是mulitble的包就可以更改了\n```scala\nm.getOrElse(\"c\",0) 找c 如果没有c就创建0\n```\n\n\n### apply方法\n##  元组\n\nval t = (1,\"spark\",2.0); 值类型都不一定\n\nm.+=((\"c\",1))  与 m+=(\"m\"->1) 写法相同\nval t,(x,y,z) = (\"a\",1,2.0) \nt是变量，x y z 是键 对应三个值，取值的时候 直接用x就可以取值\n\n### 将对偶的转成映射\n```scala\nval arr  = Array((\"a\",1),(\"b\",2))\narr.toMap 就可以转成映射\n```\n\n\n### 拉链操作\n```scala\nval a = Array(\"a\",\"b\",\"c\")\nval b = Array(1,2,3);\na.zip(b) \n```\n\n\n将其拉在一起变成数组，元素为元组\n# 集合\n序列Seq,集Set,映射Map\n集合分为可变和不可变的 mutable 和 immutable\n注意和val的对比\n将0插入来lst前面生成一个新的集合\n\n```scala\nval lst2 = 1 :: lst1; \nval lst3 = lst1.::(0)\nval lst4 = 0 +: lst1\nval lst5 = lst1.+:(0)\n```\n\n\n将一个元素添加到lst后面产生一个新的集合\n\n```scala\nval lst6 = lst1:+3\nval lst0 = List(4,5,6)\n```\n\n\n将2个list合并成一个新的List\n\n```scala\nval lst7 = lst1 ++ lst0\n```\n\n\n将lst0插入到lst前面生成一个新的集合\n\n```scala\nval lst8 = lst1 ++: lst0\n```\n\n\n\n## foreach\nforeach是将其值取出来，不会新生成一个集合\nmap将值取出来会新生成一个集合\n## Map\nMap本身不支持排序\n但是有toList方法 无括号，可以支持排序\n## List\n```scala\nval list = List(1,2,3,4,5,6);\n```\n\n\nlist.par转成并行化集合\nlist.par.reduce(_+_) 将其放在多个reduce中执行，数据量大的时候将会变得很快\nreduce不是并行集合的话，就是调用的底层\nreduceLeft(不能并行了)\n## fold可以指定初始值\n```scala\nlist.par.fold(0)(_+_)\n```\n\n\n## aggregate 聚合\n需要传两个函数，第一个函数是对元素进行操作，第二函数是对局部操作的结果进行操作\n\n```scala\nList（List(1,7,9,8),List(0,1,2,3)）\nlist.aggregate(0)(_+_.sum,_+_)\n```\n\n\n\nunion 并集，intersect交集，diff差集\n\n\n## flatten 将数据压缩\n```scala\nList（List(1,7,9,8),List(0,1,2,3)）\nlistAll.flatten\nList(1,7,9,8,0,1,2,3) 产生新的集合\n```\n\n\n\n# 对象\n\n主构造器里面的所有方法都会被执行\n## 单例对象\n所有的object都是一个单例（把class替换成object）\n不要new，直接等于类名就是调用的一个**单例对象**\n\n```scala\nobject Dog{\n  def main(){\n  val d = Dog\n  }\n}\n```\n\n\n## 伴生对象\n就是对象名和类名一样，并且在一个scala文件中\n可以和类**互相访问私有属性**\nscala中返回的就是unit就是返回的一个括号\n### apply方法\n```scala\nObject Dog{\n def apply():Unit = {\n  print ();\n }\n def apply(name:String):Unit = {\n  print(name)\n }\n def main(){\n //会调用第一个无参数的apply方法\n val d1 = Dog()\n //会调用第有参数的apply方法\n val d2 = Dog(\"haha\");\n }\n}\n```\n\n## 应用程序对象\n没有什么实际作用\n## 构造函数\n用this关键字定义辅助构造器\n```scala\ndef this(name:String,age:Int,gender:String){\n//每个服务构造器必须以主构造器或者其他的辅助构造器的调用开始\n//主构造器就是类名上直接填写参数\n}\n```\n\n函数与方法的互换， 神奇的下划线\n要想传到map里面，必须得是函数\n\n```scala\ndef  fangfa方法\nval  func 函数\nval arr = Array(1,2,3,4,5,6)\narr.map(func(5))\narr.map(func())\nval m = fangfa _  方法func转成函数\nfangfa()  也可以转函数\n\ndef m(x:Int) = (y:int)=>x*y\n```\n\n\n\n## 柯里化的两种表达方式\n柯里化是主要是通过类型类匹配的\n\n```scala\ndef m1(x:Int) = (y:Int)=>x*y\ndef m2(x:Int)(y:Int)=>x*y\n```\n\n\n柯里化会先执行一部分，返回一个函数\n```scala\n def multi= (x:Int) =>{\n    x*x\n  }\n\n  def main(args: Array[String]): Unit = {\n    val arr = Array(1,2,3,5,4)\n    val a1 = multi(10)\n    println(s\"平方式：${a1}\")\n    //按照规则 map只能参数只能是函数，multi是一个方法，但是在柯里化的时候，会先返回一个中间结果是函数\n    val a2 = arr.map(multi)\n  }\n\n```\n## 继承 代理 装饰 之间的区别\n继承是类的增强\n\n代理是对实例，方法的增强\n\n装饰也是对方法的增强\n\nimplicit def 隐式的，隐式转化的包在predef中\n# 泛型\n\n```scala\n<? extends clazz> 传入的数据是clazz的子类   \n<? super clazz> 传入的数据是clazz的父类   \n```\n\n\n\n## > < >= <=\n\n以上操作符，在scala中都是方法\n\n\n\n\n\n### 视图定界 view bound <%\n\nscala泛型\n\n```scala\nclass Person[T] { \ndef chooser[T <: Comparable[T]](firit: T, second: T): T = { \n\tfirst \n\t} \n} \n```\n\n\n隐式转换：我自己的隐式上下文\n\n```scala\nobject MyPredef{ \nimplicit 函数 \nimplicit 值 \n} \n```\n\n\nviewbound要求传入一个隐式转换函数\n\n```scala\nclass Chooser[T <% Ordered[T]] { \ndef bigger(first: T, second: T) : T = { \n\tif(first > second) first else second \n\t} \n} \n\n\nclass Chooser[T] { \ndef bigger(first: T, second: T)(implicit ord: T => Ordered[T]) : T = { \n\tif(first > second) first else second \n\t} \n} \n```\n\n\ncontextbound要求传入一个隐式转换值\n\n```scala\nclass Chooser[T: Ordering] { \ndef bigger(first: T, second: T) : T = { \nval ord = implicitly[Ordering[T]] \nif(ord.gt(first, second)) first else second \n} \n} \nclass Chooser[T] { \ndef bigger(first: T, second: T)(implicit ord : Ordering[T]) : T = { \nif(ord.gt(first, second)) first else second \n} \n} \n```\n\n\n[+T] \n[-T]\n\n相当于传入了一个隐式转换的函数\n一定要传入一个隐式转换函数\n\n```scala\nclass Chooser [t <% Order[T]]{\n  def choose(first T,second T) :T ={\n  //val ord = implicitly[Ordering[T]]\n  // if(ord.gt(first,second))) first else second;\n   if(first.compare(second) > 0) first else second;\n  }\n}\n```\n\n```scala\nimplict object girlOrdering extends Ordering[girl]{\noverride def compare(x:girl,y:girl):Int = {\n\n}\n}\n\n== 和这个实现的效果是一样的,只是取了一个名字\nimplicit val girlOrder = new Ordering[Girl]{\n\n}\n\n```\n\n\n\n\n### 上下文定界 : content bound\n相当于传入了一个隐式转换的值\n\n关于实体类Predef的关系","source":"_posts/Scala基本使用.md","raw":"---\ntitle: Scala基本使用-杂记\ndate: 2018年08月06日 22时15分52秒\ntags: [Scala]\ncategories: 语言\ntoc: true\n---\n\n[TOC]\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fukx028pz0j313t0op400.jpg)\n\nVar和val\n\nvar 修饰的变量可改变，val 修饰的变量不可改变；但真的如此吗？事实上，var 修饰的对象引用可以改变，val 修饰的则不可改变，但对象的状态却是可以改变的。\n\n## 定义方法\n```scala\ndef m1(x:Int,y:Int):Int=x*y\n```\n\n<!-- more -->\n\n## 函数的定义=>\n\n```scala\nval func:Int =>String={x=x.toString}\n也可以写成\nval func1=(x:Int)=>x.toString\n```\n\n\n## 函数和方法的区别\n```scala\n定义一个方法\ndef m2(f:(Int,Int)=>Int) =f(2,6)\n定义一个函数\nval f2 =(x:Int,y:Int) =>x-y\n调用\nf2(m2)\n```\n\n\n## 神奇的下划线\n将方法转换成函数\n\n```scala\ndef m2(x:Int,y:Int):Int = x+y\nval f2(a:Int,y:Int)=>x+y\nval f2 = m2 _\n```\n\n\n\n\n# 数组、映射、元组、集合\n## 数组\n```scala\nval arr = new Array[Int](10) //创建数组，无值\nval arr1 = Array(1,2,3,4,5)  //直接实例化\nfor(e<- arr) **yield** e*2; 用yield关键字可以生成一个新的数组 生成的类型和循环的类型是一样的\n```\n\n\n## map方法\nmap方法是将每一个元素拿来操作\narr.map(_*2) map方法更好用  map是排序？\n\n```scala\nval a = Array(1,2,3,4,5)\na.map((x:Int)=>x*10)//匿名函数\n```\n\n\n由于知道a中的数据类型 可以将Int省略\na.map(x=>x*10) \n还可以用_占位符，进行进一步的省略\na.map(_ * 10)\n\n\n所有偶数取出来然后再乘以10\n```scala\narr.filter((x:Int)=>x%2==0)\narr.filter(x=>x%2 == 0)\narr.filter(_%2==0)\narr.filter(_%2==0).map(_*10)\nval p =  println _\n```\n\n\n\n可以将方法转换成函数用利用 _\n### trait 关键字\n## 数组的常用函数\n在超类\nTraversableLike中定义了这些函数\n```scala\nval arr = Array(1,2,3,4,5,6)\narr.sum 总数\narr.sorted 排序\narr.sorted.reverse 逆序\narr.sortBy(x=>x)按照本身来排序\narr.sortWith(_>_)从大到小排序 < 从小到大排序\narr.sortWith((x,y))\n## \n```\n\n映射 类比java中的map\n```scala\nval m = Map(\"a\"->1,\"b\"->2)\nm(\"a\") 取值 \n```\n\n\n里边的值不能更改\n如果是mulitble的包就可以更改了\n```scala\nm.getOrElse(\"c\",0) 找c 如果没有c就创建0\n```\n\n\n### apply方法\n##  元组\n\nval t = (1,\"spark\",2.0); 值类型都不一定\n\nm.+=((\"c\",1))  与 m+=(\"m\"->1) 写法相同\nval t,(x,y,z) = (\"a\",1,2.0) \nt是变量，x y z 是键 对应三个值，取值的时候 直接用x就可以取值\n\n### 将对偶的转成映射\n```scala\nval arr  = Array((\"a\",1),(\"b\",2))\narr.toMap 就可以转成映射\n```\n\n\n### 拉链操作\n```scala\nval a = Array(\"a\",\"b\",\"c\")\nval b = Array(1,2,3);\na.zip(b) \n```\n\n\n将其拉在一起变成数组，元素为元组\n# 集合\n序列Seq,集Set,映射Map\n集合分为可变和不可变的 mutable 和 immutable\n注意和val的对比\n将0插入来lst前面生成一个新的集合\n\n```scala\nval lst2 = 1 :: lst1; \nval lst3 = lst1.::(0)\nval lst4 = 0 +: lst1\nval lst5 = lst1.+:(0)\n```\n\n\n将一个元素添加到lst后面产生一个新的集合\n\n```scala\nval lst6 = lst1:+3\nval lst0 = List(4,5,6)\n```\n\n\n将2个list合并成一个新的List\n\n```scala\nval lst7 = lst1 ++ lst0\n```\n\n\n将lst0插入到lst前面生成一个新的集合\n\n```scala\nval lst8 = lst1 ++: lst0\n```\n\n\n\n## foreach\nforeach是将其值取出来，不会新生成一个集合\nmap将值取出来会新生成一个集合\n## Map\nMap本身不支持排序\n但是有toList方法 无括号，可以支持排序\n## List\n```scala\nval list = List(1,2,3,4,5,6);\n```\n\n\nlist.par转成并行化集合\nlist.par.reduce(_+_) 将其放在多个reduce中执行，数据量大的时候将会变得很快\nreduce不是并行集合的话，就是调用的底层\nreduceLeft(不能并行了)\n## fold可以指定初始值\n```scala\nlist.par.fold(0)(_+_)\n```\n\n\n## aggregate 聚合\n需要传两个函数，第一个函数是对元素进行操作，第二函数是对局部操作的结果进行操作\n\n```scala\nList（List(1,7,9,8),List(0,1,2,3)）\nlist.aggregate(0)(_+_.sum,_+_)\n```\n\n\n\nunion 并集，intersect交集，diff差集\n\n\n## flatten 将数据压缩\n```scala\nList（List(1,7,9,8),List(0,1,2,3)）\nlistAll.flatten\nList(1,7,9,8,0,1,2,3) 产生新的集合\n```\n\n\n\n# 对象\n\n主构造器里面的所有方法都会被执行\n## 单例对象\n所有的object都是一个单例（把class替换成object）\n不要new，直接等于类名就是调用的一个**单例对象**\n\n```scala\nobject Dog{\n  def main(){\n  val d = Dog\n  }\n}\n```\n\n\n## 伴生对象\n就是对象名和类名一样，并且在一个scala文件中\n可以和类**互相访问私有属性**\nscala中返回的就是unit就是返回的一个括号\n### apply方法\n```scala\nObject Dog{\n def apply():Unit = {\n  print ();\n }\n def apply(name:String):Unit = {\n  print(name)\n }\n def main(){\n //会调用第一个无参数的apply方法\n val d1 = Dog()\n //会调用第有参数的apply方法\n val d2 = Dog(\"haha\");\n }\n}\n```\n\n## 应用程序对象\n没有什么实际作用\n## 构造函数\n用this关键字定义辅助构造器\n```scala\ndef this(name:String,age:Int,gender:String){\n//每个服务构造器必须以主构造器或者其他的辅助构造器的调用开始\n//主构造器就是类名上直接填写参数\n}\n```\n\n函数与方法的互换， 神奇的下划线\n要想传到map里面，必须得是函数\n\n```scala\ndef  fangfa方法\nval  func 函数\nval arr = Array(1,2,3,4,5,6)\narr.map(func(5))\narr.map(func())\nval m = fangfa _  方法func转成函数\nfangfa()  也可以转函数\n\ndef m(x:Int) = (y:int)=>x*y\n```\n\n\n\n## 柯里化的两种表达方式\n柯里化是主要是通过类型类匹配的\n\n```scala\ndef m1(x:Int) = (y:Int)=>x*y\ndef m2(x:Int)(y:Int)=>x*y\n```\n\n\n柯里化会先执行一部分，返回一个函数\n```scala\n def multi= (x:Int) =>{\n    x*x\n  }\n\n  def main(args: Array[String]): Unit = {\n    val arr = Array(1,2,3,5,4)\n    val a1 = multi(10)\n    println(s\"平方式：${a1}\")\n    //按照规则 map只能参数只能是函数，multi是一个方法，但是在柯里化的时候，会先返回一个中间结果是函数\n    val a2 = arr.map(multi)\n  }\n\n```\n## 继承 代理 装饰 之间的区别\n继承是类的增强\n\n代理是对实例，方法的增强\n\n装饰也是对方法的增强\n\nimplicit def 隐式的，隐式转化的包在predef中\n# 泛型\n\n```scala\n<? extends clazz> 传入的数据是clazz的子类   \n<? super clazz> 传入的数据是clazz的父类   \n```\n\n\n\n## > < >= <=\n\n以上操作符，在scala中都是方法\n\n\n\n\n\n### 视图定界 view bound <%\n\nscala泛型\n\n```scala\nclass Person[T] { \ndef chooser[T <: Comparable[T]](firit: T, second: T): T = { \n\tfirst \n\t} \n} \n```\n\n\n隐式转换：我自己的隐式上下文\n\n```scala\nobject MyPredef{ \nimplicit 函数 \nimplicit 值 \n} \n```\n\n\nviewbound要求传入一个隐式转换函数\n\n```scala\nclass Chooser[T <% Ordered[T]] { \ndef bigger(first: T, second: T) : T = { \n\tif(first > second) first else second \n\t} \n} \n\n\nclass Chooser[T] { \ndef bigger(first: T, second: T)(implicit ord: T => Ordered[T]) : T = { \n\tif(first > second) first else second \n\t} \n} \n```\n\n\ncontextbound要求传入一个隐式转换值\n\n```scala\nclass Chooser[T: Ordering] { \ndef bigger(first: T, second: T) : T = { \nval ord = implicitly[Ordering[T]] \nif(ord.gt(first, second)) first else second \n} \n} \nclass Chooser[T] { \ndef bigger(first: T, second: T)(implicit ord : Ordering[T]) : T = { \nif(ord.gt(first, second)) first else second \n} \n} \n```\n\n\n[+T] \n[-T]\n\n相当于传入了一个隐式转换的函数\n一定要传入一个隐式转换函数\n\n```scala\nclass Chooser [t <% Order[T]]{\n  def choose(first T,second T) :T ={\n  //val ord = implicitly[Ordering[T]]\n  // if(ord.gt(first,second))) first else second;\n   if(first.compare(second) > 0) first else second;\n  }\n}\n```\n\n```scala\nimplict object girlOrdering extends Ordering[girl]{\noverride def compare(x:girl,y:girl):Int = {\n\n}\n}\n\n== 和这个实现的效果是一样的,只是取了一个名字\nimplicit val girlOrder = new Ordering[Girl]{\n\n}\n\n```\n\n\n\n\n### 上下文定界 : content bound\n相当于传入了一个隐式转换的值\n\n关于实体类Predef的关系","slug":"Scala基本使用","published":1,"updated":"2018-08-24T09:24:13.619Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wc002m2tpbvhwo66ny"},{"title":"Spark-On-Yarn模式","date":"2018-08-15T17:58:01.513Z","toc":true,"_content":"\n[TOC]\n\n# SparkOnYarn\n\n# 两种模式区别\n\n## cluster模式：\n\nDriver程序在YARN中运行，应用的运行结果不能在客户端显示，所以最好运行那些将结果最终保存在外部存储介质（如HDFS、Redis、Mysql）而非stdout输出的应用程序，客户端的终端显示的仅是作为YARN的job的简单运行状况。\n\n```bash\n./bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode cluster \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 2 \\\n--queue default \\\nlib/spark-examples*.jar \\\n10\n\n./bin/spark-submit --class cn.itcast.spark.day1.WordCount \\\n--master yarn \\\n--deploy-mode cluster \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 2 \\\n--queue default \\\n/home/bigdata/hello-spark-1.0.jar \\\nhdfs://master:9000/wc hdfs://master:9000/out-yarn-1\n\n```\n\n<!-- more --> \n\n## client模式：\n\nDriver运行在Client上，应用程序运行结果会在客户端显示，所有适合运行结果有输出的应用程序（如spark-shell）\n\n```bash\nclient模式\n./bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode client \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 2 \\\n--queue default \\\nlib/spark-examples*.jar \\\n10\n\nspark-shell必须使用client模式\n./bin/spark-shell --master yarn --deploy-mode client\n\n```\n\n\n\n# 原理\n\n## cluster模式：\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuaxd9man3j31020o60w1.jpg)\n\nSpark Driver首先作为一个ApplicationMaster在YARN集群中启动，客户端提交给ResourceManager的每一个job都会在集群的NodeManager节点上分配一个唯一的ApplicationMaster，由该ApplicationMaster管理全生命周期的应用。具体过程：\n\n1. 由client向ResourceManager提交请求，并上传jar到HDFS上\n  这期间包括四个步骤：\n  a).连接到RM\n  b).从RM的ASM（ApplicationsManager ）中获得metric、queue和resource等信息。\n  c). upload app jar and spark-assembly jar\n  d).设置运行环境和container上下文（launch-container.sh等脚本)\n\n2. ResouceManager向NodeManager申请资源，创建Spark ApplicationMaster（每个SparkContext都有一个ApplicationMaster）\n3. NodeManager启动ApplicationMaster，并向ResourceManager AsM注册\n4. ApplicationMaster从HDFS中找到jar文件，启动SparkContext、DAGscheduler和YARN Cluster Scheduler\n5. ResourceManager向ResourceManager AsM注册申请container资源\n6. ResourceManager通知NodeManager分配Container，这时可以收到来自ASM关于container的报告。（每个container对应一个executor）\n7. Spark ApplicationMaster直接和container（executor）进行交互，完成这个分布式任务。\n\n## client模式\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuaxdd9n3tj310c0jbq5y.jpg)\n\n在client模式下，Driver运行在Client上，通过ApplicationMaster向RM获取资源。本地Driver负责与所有的executor container进行交互，并将最后的结果汇总。结束掉终端，相当于kill掉这个spark应用。一般来说，如果运行的结果仅仅返回到terminal上时需要配置这个。\n\n客户端的Driver将应用提交给Yarn后，Yarn会先后启动ApplicationMaster和executor，另外ApplicationMaster和executor都 是装载在container里运行，container默认的内存是1G，ApplicationMaster分配的内存是driver- memory，executor分配的内存是executor-memory。同时，因为Driver在客户端，所以程序的运行结果可以在客户端显 示，Driver以进程名为SparkSubmit的形式存在。","source":"_posts/Spark-On-yarn.md","raw":"---\ntitle: Spark-On-Yarn模式\ndate: 2018年08月06日 22时15分52秒\ntags: [Spark,使用]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n# SparkOnYarn\n\n# 两种模式区别\n\n## cluster模式：\n\nDriver程序在YARN中运行，应用的运行结果不能在客户端显示，所以最好运行那些将结果最终保存在外部存储介质（如HDFS、Redis、Mysql）而非stdout输出的应用程序，客户端的终端显示的仅是作为YARN的job的简单运行状况。\n\n```bash\n./bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode cluster \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 2 \\\n--queue default \\\nlib/spark-examples*.jar \\\n10\n\n./bin/spark-submit --class cn.itcast.spark.day1.WordCount \\\n--master yarn \\\n--deploy-mode cluster \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 2 \\\n--queue default \\\n/home/bigdata/hello-spark-1.0.jar \\\nhdfs://master:9000/wc hdfs://master:9000/out-yarn-1\n\n```\n\n<!-- more --> \n\n## client模式：\n\nDriver运行在Client上，应用程序运行结果会在客户端显示，所有适合运行结果有输出的应用程序（如spark-shell）\n\n```bash\nclient模式\n./bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode client \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 2 \\\n--queue default \\\nlib/spark-examples*.jar \\\n10\n\nspark-shell必须使用client模式\n./bin/spark-shell --master yarn --deploy-mode client\n\n```\n\n\n\n# 原理\n\n## cluster模式：\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuaxd9man3j31020o60w1.jpg)\n\nSpark Driver首先作为一个ApplicationMaster在YARN集群中启动，客户端提交给ResourceManager的每一个job都会在集群的NodeManager节点上分配一个唯一的ApplicationMaster，由该ApplicationMaster管理全生命周期的应用。具体过程：\n\n1. 由client向ResourceManager提交请求，并上传jar到HDFS上\n  这期间包括四个步骤：\n  a).连接到RM\n  b).从RM的ASM（ApplicationsManager ）中获得metric、queue和resource等信息。\n  c). upload app jar and spark-assembly jar\n  d).设置运行环境和container上下文（launch-container.sh等脚本)\n\n2. ResouceManager向NodeManager申请资源，创建Spark ApplicationMaster（每个SparkContext都有一个ApplicationMaster）\n3. NodeManager启动ApplicationMaster，并向ResourceManager AsM注册\n4. ApplicationMaster从HDFS中找到jar文件，启动SparkContext、DAGscheduler和YARN Cluster Scheduler\n5. ResourceManager向ResourceManager AsM注册申请container资源\n6. ResourceManager通知NodeManager分配Container，这时可以收到来自ASM关于container的报告。（每个container对应一个executor）\n7. Spark ApplicationMaster直接和container（executor）进行交互，完成这个分布式任务。\n\n## client模式\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuaxdd9n3tj310c0jbq5y.jpg)\n\n在client模式下，Driver运行在Client上，通过ApplicationMaster向RM获取资源。本地Driver负责与所有的executor container进行交互，并将最后的结果汇总。结束掉终端，相当于kill掉这个spark应用。一般来说，如果运行的结果仅仅返回到terminal上时需要配置这个。\n\n客户端的Driver将应用提交给Yarn后，Yarn会先后启动ApplicationMaster和executor，另外ApplicationMaster和executor都 是装载在container里运行，container默认的内存是1G，ApplicationMaster分配的内存是driver- memory，executor分配的内存是executor-memory。同时，因为Driver在客户端，所以程序的运行结果可以在客户端显 示，Driver以进程名为SparkSubmit的形式存在。","slug":"Spark-On-yarn","published":1,"updated":"2018-08-17T02:19:27.066Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wd002q2tpbwy68odk1"},{"title":"Spark-on-Yarn源码解析(三)client做的事情","date":"2018-09-04T08:13:34.283Z","toc":true,"_content":"\n[TOC]\n\n\nspark-on-yarn系列\n\n[Spark-on-Yarn 源码解析 (一)Yarn 任务解析](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(一)Yarn任务解析/)\n[Spark-on-Yarn 源码解析 (二)Spark-Submit 解析](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(二)Spark-Submit解析/)\n[Spark-on-Yarn 源码解析 (三)client 做的事情](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(三)client做的事情/)\n[Spark-on-Yarn 源码解析 (四)Spark 业务代码的执行及其任务分配调度 stage 划分](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分/)\n\n\n\n\norg.apache.spark.deploy.yarn.Client\n\n话不多说，先上源码，当然还是简洁版本的\n\n这儿我先上一下最简洁的调用链。\n\n    Client.main()\n        ->new Client().run()\n             ->monitorApplication(submitApplication())\n                ->submitApplication()\n                    ->createContainerLaunchContext()会封装一些启动信息如我们启动的类 --class\n                        ->userClass\n                        ->amArgs\n                        ->commands\n                        ->printableCommands\n                        ->amClass applicationMaster启动的真实类\n                        \n                    ->createApplicationSubmissionContext()\n                        ->Records.newRecord(classOf[Resource])启动\n                    ->yarnClientImpl.submitApplication(appContext)                \n\n<!--more-->\n\n最终是调用的client里面main方法->run->\n\nmonitorApplication(submitApplication())\n\n    object Client extends Logging {\n      def main(argStrings: Array[String]) {\n        ... ...\n        val sparkConf = new SparkConf\n        val args = new ClientArguments(argStrings, sparkConf)\n        new Client(args, sparkConf).run()\n        ... ...\n      }\n    }\n    \n    ... ...\n    def run(): Unit = {\n        val (yarnApplicationState, finalApplicationStatus) = monitorApplication(submitApplication())\n        }\n    ... ...\n    def submitApplication(): ApplicationId = {\n        // TODO: 初始化并且启动client\n        yarnClient.init(yarnConf)\n        yarnClient.start()\n        // TODO: 准备提交请求到resouceManager\n        val newApp = yarnClient.createApplication()\n        val newAppResponse = newApp.getNewApplicationResponse()\n        val appId = newAppResponse.getApplicationId()\n        // TODO: 检查集群的内存是否满足当前的任务要求\n        verifyClusterResources(newAppResponse)\n        // TODO:  设置适当上下文环境来启动applicationMaster\n        val containerContext = createContainerLaunchContext(newAppResponse)\n        val appContext = createApplicationSubmissionContext(newApp, containerContext)\n        // TODO: 提交application\n        yarnClient.submitApplication(appContext)\n        appId\n      }\n       private def createContainerLaunchContext(newAppResponse: GetNewApplicationResponse)\n        : ContainerLaunchContext = {\n        ... ...\n              val userClass =\n          if (isClusterMode) {\n            Seq(\"--class\", YarnSparkHadoopUtil.escapeForShell(args.userClass))\n          } else {\n            Nil\n          }\n          ...\n           val amClass =\n          if (isClusterMode) {\n    Class.forName(\"org.apache.spark.deploy.yarn.ApplicationMaster\").getName\n          } else {\n    Class.forName(\"org.apache.spark.deploy.yarn.ExecutorLauncher\").getName\n          }\n              val amArgs =\n          Seq(amClass) ++ userClass ++ userJar ++ primaryPyFile ++ pyFiles ++ userArgs ++\n            Seq(\n              \"--executor-memory\", args.executorMemory.toString + \"m\",\n              \"--executor-cores\", args.executorCores.toString,\n              \"--num-executors \", args.numExecutors.toString)\n    \n        val commands = prefixEnv ++ Seq(YarnSparkHadoopUtil.expandEnvironment(Environment.JAVA_HOME) + \"/bin/java\", \"-server\"\n          ) ++\n          javaOpts ++ amArgs ++\n        ... ...\n         val printableCommands = commands.map(s => if (s == null) \"null\" else s).toList\n        amContainer.setCommands(printableCommands)\n        }\n        ... ...\n    def createApplicationSubmissionContext(\n          newApp: YarnClientApplication,\n          containerContext: ContainerLaunchContext): ApplicationSubmissionContext = {\n        val appContext = newApp.getApplicationSubmissionContext\n        appContext.setApplicationName(args.appName)\n        appContext.setQueue(args.amQueue)\n        appContext.setAMContainerSpec(containerContext)\n        appContext.setApplicationType(\"SPARK\")\n        sparkConf.getOption(\"spark.yarn.maxAppAttempts\").map(_.toInt) match {\n          case Some(v) => appContext.setMaxAppAttempts(v)\n          case None => logDebug(\"spark.yarn.maxAppAttempts is not set. \" +\n              \"Cluster's default value will be used.\")\n        }\n       \n        val capability = Records.newRecord(classOf[Resource])\n        capability.setMemory(args.amMemory + amMemoryOverhead)\n        capability.setVirtualCores(args.amCores)\n        appContext.setResource(capability)\n        appContext\n      }\n    \n    //yarnClient.submitApplication(appContext)提交的真实处  \n    @Override\n      public ApplicationId\n          submitApplication(ApplicationSubmissionContext appContext)\n              throws YarnException, IOException {\n    ...\n    //此处通过yarn的协议对applicationMaster进行提交和启动 （此处为个人理解有疑惑，如有错误，还望留言分享，会立即作出更正）\n        SubmitApplicationRequest request =\n            Records.newRecord(SubmitApplicationRequest.class);\n        request.setApplicationSubmissionContext(appContext);\n    ...\n\n此处client的事情都已经做完了，请摄影师将镜头切换到applicationMaster\n\n\n\n小细节用户业务代码信息的封装及流转\n\n我们提交的class的封装流程\n\n    \n\n    ->sublimit的prepareSubmitEnvironment中封装到childArgs中--class\n    ->传入到client的构造函数里面作为clientArgs，将其封装到userClass属性里面\n    \n    在submitApplication中createContainerLaunchContext会将其通过重新封到userClass\n        userClass->amArgs->commands->printableCommands\n        ->amContainer.setCommands(printableCommands)\n        在此，createContainerLaunchContext方法接收到amContainer赋名为containerContext传递给createApplicationSubmissionContext(..,containerContext)\n        \n    那么在createApplicationSubmissionContext中又有哪些惊天变化（其实并没有）\n    \n    appContext.setAMContainerSpec(containerContext)\n    那么appContext作为createApplicationSubmissionContext方法返回值，由appContext接收，看码\n    \n    appContext = createApplicationSubmissionContext(newApp, containerContext)\n    最后，由yarnClientImpl提交\n    yarnClient.submitApplication(appContext)\n    码又来了，最终执行的是\n    SubmitApplicationRequest request =Records.newRecord(SubmitApplicationRequest.class);\n\n启动applicationMaster\n\n对于client的封装，对于applicationMaster需要启动的信息(如资源信息)及用户提交的业务代码（wordcount的类信息）信息都已经封装到appContext，并且传递到applicationmaster，那么来看看applicationMaster的执行流程。\n\n程序调用结构\n\n    ApplicationMaster.main()\n        ->run()\n            ->runDriver()\n                ->run()\n                    ->startUserApplication()\n                        //启动userClass\n                        ->userClassLoader.loadClass(args.userClass)\n          .getMethod(\"main\", classOf[Array[String]])\n                         ->mainMethod.invoke(null, mainArgs)\n                    runAMActor()\n                    registerAM()\n                        ->yarnRmClient.register()->return new YarnAllocater(......)\n                        ->yarnAllocator.allocateResources()\n                            ->yarnAllocator.handleAllocatedContainers()\n                            //启动executor\n                            ->yarnAllocator.runAllocatedContainers(containersToUse)\n\nrunAllocatedContainers(containersToUse)是去启动 executor，最终真正执行启动Container的是在 ExecutorRunnable.run()中。\n\n创建了 NMClient 客户端调用提供的 API 最终实现在 NM 上启动 Container，具体如何启动 Container 将在后文中进行介绍。\n\n    \n\nlauncherPool线程池会将container，driver等相关信息封装成ExecutorRunnable对象，通过ExecutorRunnable启动新的container以运行executor。在此过程中，指定启动executor的类是\n\norg.apache.spark.executor.CoarseGrainedExecutorBackend。spark yarn cluster 模式下任务提交和计算流程分析\n\n程序的细节\n\n    def main(args: Array[String]) = {\n        SignalLogger.register(log)\n        val amArgs = new ApplicationMasterArguments(args)\n        SparkHadoopUtil.get.runAsSparkUser { () =>\n          master = new ApplicationMaster(amArgs, new YarnRMClient(amArgs))\n          System.exit(master.run())\n        }\n      }\n      \n      \n      ......\n      final def run(): Int = {\n      ....\n        if (isClusterMode) {\n            runDriver(securityMgr)\n          } else {\n            runExecutorLauncher(securityMgr)\n          }\n    ...\n    }\n    \n      private def runDriver(securityMgr: SecurityManager): Unit = {\n        addAmIpFilter()\n        // TODO:  启动我们自定的类，也就是启动submit里面的--class的东西\n        userClassThread = startUserApplication()\n        val sc = waitForSparkContextInitialized()\n    ...\n      actorSystem = sc.env.actorSystem\n      runAMActor(\n        sc.getConf.get(\"spark.driver.host\"),\n        sc.getConf.get(\"spark.driver.port\"),\n        isClusterMode = true)\n      registerAM(sc.ui.map(_.appUIAddress).getOrElse(\"\"), securityMgr)\n      userClassThread.join()\n        ...\n      }\n\n在ApplicationMasterArguments设置了要启动的信息\n\n    class ApplicationMasterArguments(val args: Array[String]) {\n      var userJar: String = null\n      var userClass: String = null\n      var primaryPyFile: String = null\n      var pyFiles: String = null\n      var userArgs: Seq[String] = Seq[String]()\n      var executorMemory = 1024\n      var executorCores = 1\n      var numExecutors = DEFAULT_NUMBER_EXECUTORS\n      ......\n      }\n\nstartUserApplication 主要执行了调用用户的代码，以及创建了一个 spark driver 的进程。 \n\nStart the user class, which contains the spark driver, in a separate Thread.\n\n    private def startUserApplication(): Thread = {\n      val classpath = Client.getUserClasspath(sparkConf)\n    val urls = classpath.map { entry =>\n      new URL(\"file:\" + new File(entry.getPath()).getAbsolutePath())\n    }\n    val userClassLoader =\n    ...\n    // TODO:  userClass就是submit里面的--class 提交的类\n    val mainMethod = userClassLoader.loadClass(args.userClass)\n      .getMethod(\"main\", classOf[Array[String]])\n    userThread.setContextClassLoader(userClassLoader)\n    userThread.setName(\"Driver\")\n    userThread.start()\n    userThread\n    }\n\n从userThread.setName(\"Driver\")也可以看出创建的是名为driver的进程\n\nregisterAM 向 resourceManager 中正式注册 applicationMaster。注册applicationMaster 以后，并且分配资源，这样，用户代码就可以执行了，任务切分、调度、执行。\n\n然后，用户代码中的 action 会调用 SparkContext 的 runJob，SparkContext 中有很多个 runJob，但最后都是调用 DAGScheduler 的 runJob\n\n        // registerAM\n         private def registerAM(uiAddress: String, securityMgr: SecurityManager) = {\n         .....\n        allocator = client.register(yarnConf,\n          if (sc != null) sc.getConf else sparkConf,\n          if (sc != null) sc.preferredNodeLocationData else Map(),\n          uiAddress,\n          historyAddress,\n          securityMgr)\n          //为exector分配资源\n        allocator.allocateResources()\n        reporterThread = launchReporterThread()\n        ......\n      }\n\n\n\n","source":"_posts/Spark-on-Yarn源码解析(三)client做的事情.md","raw":"---\ntitle: Spark-on-Yarn源码解析(三)client做的事情\ndate: 2018年09月04日\ntags: [Spark,原理]\ncategories: Spark-On-Yarn\ntoc: true\n---\n\n[TOC]\n\n\nspark-on-yarn系列\n\n[Spark-on-Yarn 源码解析 (一)Yarn 任务解析](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(一)Yarn任务解析/)\n[Spark-on-Yarn 源码解析 (二)Spark-Submit 解析](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(二)Spark-Submit解析/)\n[Spark-on-Yarn 源码解析 (三)client 做的事情](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(三)client做的事情/)\n[Spark-on-Yarn 源码解析 (四)Spark 业务代码的执行及其任务分配调度 stage 划分](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分/)\n\n\n\n\norg.apache.spark.deploy.yarn.Client\n\n话不多说，先上源码，当然还是简洁版本的\n\n这儿我先上一下最简洁的调用链。\n\n    Client.main()\n        ->new Client().run()\n             ->monitorApplication(submitApplication())\n                ->submitApplication()\n                    ->createContainerLaunchContext()会封装一些启动信息如我们启动的类 --class\n                        ->userClass\n                        ->amArgs\n                        ->commands\n                        ->printableCommands\n                        ->amClass applicationMaster启动的真实类\n                        \n                    ->createApplicationSubmissionContext()\n                        ->Records.newRecord(classOf[Resource])启动\n                    ->yarnClientImpl.submitApplication(appContext)                \n\n<!--more-->\n\n最终是调用的client里面main方法->run->\n\nmonitorApplication(submitApplication())\n\n    object Client extends Logging {\n      def main(argStrings: Array[String]) {\n        ... ...\n        val sparkConf = new SparkConf\n        val args = new ClientArguments(argStrings, sparkConf)\n        new Client(args, sparkConf).run()\n        ... ...\n      }\n    }\n    \n    ... ...\n    def run(): Unit = {\n        val (yarnApplicationState, finalApplicationStatus) = monitorApplication(submitApplication())\n        }\n    ... ...\n    def submitApplication(): ApplicationId = {\n        // TODO: 初始化并且启动client\n        yarnClient.init(yarnConf)\n        yarnClient.start()\n        // TODO: 准备提交请求到resouceManager\n        val newApp = yarnClient.createApplication()\n        val newAppResponse = newApp.getNewApplicationResponse()\n        val appId = newAppResponse.getApplicationId()\n        // TODO: 检查集群的内存是否满足当前的任务要求\n        verifyClusterResources(newAppResponse)\n        // TODO:  设置适当上下文环境来启动applicationMaster\n        val containerContext = createContainerLaunchContext(newAppResponse)\n        val appContext = createApplicationSubmissionContext(newApp, containerContext)\n        // TODO: 提交application\n        yarnClient.submitApplication(appContext)\n        appId\n      }\n       private def createContainerLaunchContext(newAppResponse: GetNewApplicationResponse)\n        : ContainerLaunchContext = {\n        ... ...\n              val userClass =\n          if (isClusterMode) {\n            Seq(\"--class\", YarnSparkHadoopUtil.escapeForShell(args.userClass))\n          } else {\n            Nil\n          }\n          ...\n           val amClass =\n          if (isClusterMode) {\n    Class.forName(\"org.apache.spark.deploy.yarn.ApplicationMaster\").getName\n          } else {\n    Class.forName(\"org.apache.spark.deploy.yarn.ExecutorLauncher\").getName\n          }\n              val amArgs =\n          Seq(amClass) ++ userClass ++ userJar ++ primaryPyFile ++ pyFiles ++ userArgs ++\n            Seq(\n              \"--executor-memory\", args.executorMemory.toString + \"m\",\n              \"--executor-cores\", args.executorCores.toString,\n              \"--num-executors \", args.numExecutors.toString)\n    \n        val commands = prefixEnv ++ Seq(YarnSparkHadoopUtil.expandEnvironment(Environment.JAVA_HOME) + \"/bin/java\", \"-server\"\n          ) ++\n          javaOpts ++ amArgs ++\n        ... ...\n         val printableCommands = commands.map(s => if (s == null) \"null\" else s).toList\n        amContainer.setCommands(printableCommands)\n        }\n        ... ...\n    def createApplicationSubmissionContext(\n          newApp: YarnClientApplication,\n          containerContext: ContainerLaunchContext): ApplicationSubmissionContext = {\n        val appContext = newApp.getApplicationSubmissionContext\n        appContext.setApplicationName(args.appName)\n        appContext.setQueue(args.amQueue)\n        appContext.setAMContainerSpec(containerContext)\n        appContext.setApplicationType(\"SPARK\")\n        sparkConf.getOption(\"spark.yarn.maxAppAttempts\").map(_.toInt) match {\n          case Some(v) => appContext.setMaxAppAttempts(v)\n          case None => logDebug(\"spark.yarn.maxAppAttempts is not set. \" +\n              \"Cluster's default value will be used.\")\n        }\n       \n        val capability = Records.newRecord(classOf[Resource])\n        capability.setMemory(args.amMemory + amMemoryOverhead)\n        capability.setVirtualCores(args.amCores)\n        appContext.setResource(capability)\n        appContext\n      }\n    \n    //yarnClient.submitApplication(appContext)提交的真实处  \n    @Override\n      public ApplicationId\n          submitApplication(ApplicationSubmissionContext appContext)\n              throws YarnException, IOException {\n    ...\n    //此处通过yarn的协议对applicationMaster进行提交和启动 （此处为个人理解有疑惑，如有错误，还望留言分享，会立即作出更正）\n        SubmitApplicationRequest request =\n            Records.newRecord(SubmitApplicationRequest.class);\n        request.setApplicationSubmissionContext(appContext);\n    ...\n\n此处client的事情都已经做完了，请摄影师将镜头切换到applicationMaster\n\n\n\n小细节用户业务代码信息的封装及流转\n\n我们提交的class的封装流程\n\n    \n\n    ->sublimit的prepareSubmitEnvironment中封装到childArgs中--class\n    ->传入到client的构造函数里面作为clientArgs，将其封装到userClass属性里面\n    \n    在submitApplication中createContainerLaunchContext会将其通过重新封到userClass\n        userClass->amArgs->commands->printableCommands\n        ->amContainer.setCommands(printableCommands)\n        在此，createContainerLaunchContext方法接收到amContainer赋名为containerContext传递给createApplicationSubmissionContext(..,containerContext)\n        \n    那么在createApplicationSubmissionContext中又有哪些惊天变化（其实并没有）\n    \n    appContext.setAMContainerSpec(containerContext)\n    那么appContext作为createApplicationSubmissionContext方法返回值，由appContext接收，看码\n    \n    appContext = createApplicationSubmissionContext(newApp, containerContext)\n    最后，由yarnClientImpl提交\n    yarnClient.submitApplication(appContext)\n    码又来了，最终执行的是\n    SubmitApplicationRequest request =Records.newRecord(SubmitApplicationRequest.class);\n\n启动applicationMaster\n\n对于client的封装，对于applicationMaster需要启动的信息(如资源信息)及用户提交的业务代码（wordcount的类信息）信息都已经封装到appContext，并且传递到applicationmaster，那么来看看applicationMaster的执行流程。\n\n程序调用结构\n\n    ApplicationMaster.main()\n        ->run()\n            ->runDriver()\n                ->run()\n                    ->startUserApplication()\n                        //启动userClass\n                        ->userClassLoader.loadClass(args.userClass)\n          .getMethod(\"main\", classOf[Array[String]])\n                         ->mainMethod.invoke(null, mainArgs)\n                    runAMActor()\n                    registerAM()\n                        ->yarnRmClient.register()->return new YarnAllocater(......)\n                        ->yarnAllocator.allocateResources()\n                            ->yarnAllocator.handleAllocatedContainers()\n                            //启动executor\n                            ->yarnAllocator.runAllocatedContainers(containersToUse)\n\nrunAllocatedContainers(containersToUse)是去启动 executor，最终真正执行启动Container的是在 ExecutorRunnable.run()中。\n\n创建了 NMClient 客户端调用提供的 API 最终实现在 NM 上启动 Container，具体如何启动 Container 将在后文中进行介绍。\n\n    \n\nlauncherPool线程池会将container，driver等相关信息封装成ExecutorRunnable对象，通过ExecutorRunnable启动新的container以运行executor。在此过程中，指定启动executor的类是\n\norg.apache.spark.executor.CoarseGrainedExecutorBackend。spark yarn cluster 模式下任务提交和计算流程分析\n\n程序的细节\n\n    def main(args: Array[String]) = {\n        SignalLogger.register(log)\n        val amArgs = new ApplicationMasterArguments(args)\n        SparkHadoopUtil.get.runAsSparkUser { () =>\n          master = new ApplicationMaster(amArgs, new YarnRMClient(amArgs))\n          System.exit(master.run())\n        }\n      }\n      \n      \n      ......\n      final def run(): Int = {\n      ....\n        if (isClusterMode) {\n            runDriver(securityMgr)\n          } else {\n            runExecutorLauncher(securityMgr)\n          }\n    ...\n    }\n    \n      private def runDriver(securityMgr: SecurityManager): Unit = {\n        addAmIpFilter()\n        // TODO:  启动我们自定的类，也就是启动submit里面的--class的东西\n        userClassThread = startUserApplication()\n        val sc = waitForSparkContextInitialized()\n    ...\n      actorSystem = sc.env.actorSystem\n      runAMActor(\n        sc.getConf.get(\"spark.driver.host\"),\n        sc.getConf.get(\"spark.driver.port\"),\n        isClusterMode = true)\n      registerAM(sc.ui.map(_.appUIAddress).getOrElse(\"\"), securityMgr)\n      userClassThread.join()\n        ...\n      }\n\n在ApplicationMasterArguments设置了要启动的信息\n\n    class ApplicationMasterArguments(val args: Array[String]) {\n      var userJar: String = null\n      var userClass: String = null\n      var primaryPyFile: String = null\n      var pyFiles: String = null\n      var userArgs: Seq[String] = Seq[String]()\n      var executorMemory = 1024\n      var executorCores = 1\n      var numExecutors = DEFAULT_NUMBER_EXECUTORS\n      ......\n      }\n\nstartUserApplication 主要执行了调用用户的代码，以及创建了一个 spark driver 的进程。 \n\nStart the user class, which contains the spark driver, in a separate Thread.\n\n    private def startUserApplication(): Thread = {\n      val classpath = Client.getUserClasspath(sparkConf)\n    val urls = classpath.map { entry =>\n      new URL(\"file:\" + new File(entry.getPath()).getAbsolutePath())\n    }\n    val userClassLoader =\n    ...\n    // TODO:  userClass就是submit里面的--class 提交的类\n    val mainMethod = userClassLoader.loadClass(args.userClass)\n      .getMethod(\"main\", classOf[Array[String]])\n    userThread.setContextClassLoader(userClassLoader)\n    userThread.setName(\"Driver\")\n    userThread.start()\n    userThread\n    }\n\n从userThread.setName(\"Driver\")也可以看出创建的是名为driver的进程\n\nregisterAM 向 resourceManager 中正式注册 applicationMaster。注册applicationMaster 以后，并且分配资源，这样，用户代码就可以执行了，任务切分、调度、执行。\n\n然后，用户代码中的 action 会调用 SparkContext 的 runJob，SparkContext 中有很多个 runJob，但最后都是调用 DAGScheduler 的 runJob\n\n        // registerAM\n         private def registerAM(uiAddress: String, securityMgr: SecurityManager) = {\n         .....\n        allocator = client.register(yarnConf,\n          if (sc != null) sc.getConf else sparkConf,\n          if (sc != null) sc.preferredNodeLocationData else Map(),\n          uiAddress,\n          historyAddress,\n          securityMgr)\n          //为exector分配资源\n        allocator.allocateResources()\n        reporterThread = launchReporterThread()\n        ......\n      }\n\n\n\n","slug":"Spark-on-Yarn源码解析(三)client做的事情","published":1,"updated":"2018-09-04T10:14:03.710Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wf002u2tpbk2wc6334"},{"title":"Spark-on-Yarn源码解析(一)Yarn任务解析","date":"2018-09-04T07:44:58.661Z","toc":true,"_content":"\n[TOC]\n\nspark-on-yarn系列\n[Spark-on-Yarn 源码解析 (一)Yarn 任务解析](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(一)Yarn任务解析/)\n[Spark-on-Yarn 源码解析 (二)Spark-Submit 解析](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(二)Spark-Submit解析/)\n[Spark-on-Yarn 源码解析 (三)client 做的事情](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(三)client做的事情/)\n[Spark-on-Yarn 源码解析 (四)Spark 业务代码的执行及其任务分配调度 stage 划分](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分/)\n\n了解spark-on-yarn,首先我们了解一下yarn提交的流程，俗话说，欲练此功，错了，我们还是先看吧\n\n# yarn任务的提交\nYARN 的基本架构和工作流程\n\n![](http://pebgsxjpj.bkt.clouddn.com/15358192541466.jpg)\n\nYARN 的基本架构如上图所示，由三大功能模块组成，分别是 1) RM (ResourceManager) 2) NM (Node Manager) 3) AM(Application Master)\n<!--more-->\n作业提交\n1. 用户通过 Client 向 ResourceManager 提交 Application， ResourceManager 根据用户请求分配合适的 Container, 然后在指定的 NodeManager 上运行 Container 以启动 ApplicationMaster\n2. ApplicationMaster 启动完成后，向 ResourceManager 注册自己\n3. 对于用户的 Task，ApplicationMaster 需要首先跟 ResourceManager 进行协商以获取运行用户 Task 所需要的 Container，在获取成功后，ApplicationMaster 将任务发送给指定的 NodeManager\n4. NodeManager 启动相应的 Container，并运行用户 Task\n\n\n<!--more-->\n\n# Spark-On-Yarn的流程提交\n\n在 **yarn-cluster 模式下，Spark driver 运行在 application master 进程中**，这个进程被集群中的 YARN 所管理，客户端会在初始化应用程序 之后关闭。在 yarn-client 模式下，driver 运行在客户端进程中，application master 仅仅用来向 YARN 请求资源\n\n\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuaxd9man3j31020o60w1.jpg)\n\nSpark Driver首先作为一个ApplicationMaster在YARN集群中启动，客户端提交给ResourceManager的时候，每一个job都会在集群的NodeManager节点上分配一个唯一的ApplicationMaster，由该ApplicationMaster管理全生命周期的应用。具体过程：\n\n1. 由client向ResourceManager提交请求，并上传jar到HDFS上\n   这期间包括四个步骤：\n   a).连接到RM\n   b).从RM的ASM（ApplicationsManager ）中获得metric、queue和resource等信息。\n   c). upload app jar and spark-assembly jar\n   d).设置运行环境和container上下文（launch-container.sh等脚本)\n2. ResouceManager向NodeManager申请资源，创建Spark ApplicationMaster（每个SparkContext都有一个ApplicationMaster）\n3. NodeManager启动ApplicationMaster，并向ResourceManager AsM注册\n4. ApplicationMaster从HDFS中找到jar文件，启动SparkContext、DAGscheduler和YARN Cluster Scheduler\n5. ResourceManager向ResourceManager AsM注册申请container资源\n6. ResourceManager通知NodeManager分配Container，这时可以收到来自ASM关于container的报告。（每个container对应一个executor）\n7. Spark ApplicationMaster直接和container（executor）进行交互，完成这个分布式任务。\n\n\n\n# ApplicationMaster和Driver的区别\n\n首先区分下 AppMaster 和 Driver，任何一个 yarn 上运行的任务都必须有一个 AppMaster，而任何一个 Spark 任务都会有一个 Driver，Driver 就是运行 SparkContext(它会构建 TaskScheduler 和 DAGScheduler) 的进程，当然在 Driver 上你也可以做很多非 Spark 的事情，这些事情只会在 Driver 上面执行，而由 SparkContext 上牵引出来的代码则会由 DAGScheduler 分析，并形成 Job 和 Stage 交由 TaskScheduler，再由 TaskScheduler 交由各 Executor 分布式执行。\n\n所以 Driver 和 AppMaster 是两个完全不同的东西，Driver 是控制 Spark 计算和任务资源的，而 AppMaster 是控制 yarn app 运行和任务资源的，只不过在 Spark on Yarn 上，这两者就出现了交叉，而在 standalone 模式下，资源则由 Driver 管理。在 Spark on Yarn 上，Driver 会和 AppMaster 通信，资源的申请由 AppMaster 来完成，而任务的调度和执行则由 Driver 完成，Driver 会通过与 AppMaster 通信来让 Executor 的执行具体的任务。\n\n> [Spark on Yarn](https://www.cnblogs.com/hseagle/p/3728713.html)\n\n\n","source":"_posts/Spark-on-Yarn源码解析(一)Yarn任务解析.md","raw":"---\ntitle: Spark-on-Yarn源码解析(一)Yarn任务解析\ndate: 2018年09月04日\ntags: [Spark,原理,Yarn]\ncategories: Spark-On-Yarn\ntoc: true\n---\n\n[TOC]\n\nspark-on-yarn系列\n[Spark-on-Yarn 源码解析 (一)Yarn 任务解析](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(一)Yarn任务解析/)\n[Spark-on-Yarn 源码解析 (二)Spark-Submit 解析](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(二)Spark-Submit解析/)\n[Spark-on-Yarn 源码解析 (三)client 做的事情](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(三)client做的事情/)\n[Spark-on-Yarn 源码解析 (四)Spark 业务代码的执行及其任务分配调度 stage 划分](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分/)\n\n了解spark-on-yarn,首先我们了解一下yarn提交的流程，俗话说，欲练此功，错了，我们还是先看吧\n\n# yarn任务的提交\nYARN 的基本架构和工作流程\n\n![](http://pebgsxjpj.bkt.clouddn.com/15358192541466.jpg)\n\nYARN 的基本架构如上图所示，由三大功能模块组成，分别是 1) RM (ResourceManager) 2) NM (Node Manager) 3) AM(Application Master)\n<!--more-->\n作业提交\n1. 用户通过 Client 向 ResourceManager 提交 Application， ResourceManager 根据用户请求分配合适的 Container, 然后在指定的 NodeManager 上运行 Container 以启动 ApplicationMaster\n2. ApplicationMaster 启动完成后，向 ResourceManager 注册自己\n3. 对于用户的 Task，ApplicationMaster 需要首先跟 ResourceManager 进行协商以获取运行用户 Task 所需要的 Container，在获取成功后，ApplicationMaster 将任务发送给指定的 NodeManager\n4. NodeManager 启动相应的 Container，并运行用户 Task\n\n\n<!--more-->\n\n# Spark-On-Yarn的流程提交\n\n在 **yarn-cluster 模式下，Spark driver 运行在 application master 进程中**，这个进程被集群中的 YARN 所管理，客户端会在初始化应用程序 之后关闭。在 yarn-client 模式下，driver 运行在客户端进程中，application master 仅仅用来向 YARN 请求资源\n\n\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuaxd9man3j31020o60w1.jpg)\n\nSpark Driver首先作为一个ApplicationMaster在YARN集群中启动，客户端提交给ResourceManager的时候，每一个job都会在集群的NodeManager节点上分配一个唯一的ApplicationMaster，由该ApplicationMaster管理全生命周期的应用。具体过程：\n\n1. 由client向ResourceManager提交请求，并上传jar到HDFS上\n   这期间包括四个步骤：\n   a).连接到RM\n   b).从RM的ASM（ApplicationsManager ）中获得metric、queue和resource等信息。\n   c). upload app jar and spark-assembly jar\n   d).设置运行环境和container上下文（launch-container.sh等脚本)\n2. ResouceManager向NodeManager申请资源，创建Spark ApplicationMaster（每个SparkContext都有一个ApplicationMaster）\n3. NodeManager启动ApplicationMaster，并向ResourceManager AsM注册\n4. ApplicationMaster从HDFS中找到jar文件，启动SparkContext、DAGscheduler和YARN Cluster Scheduler\n5. ResourceManager向ResourceManager AsM注册申请container资源\n6. ResourceManager通知NodeManager分配Container，这时可以收到来自ASM关于container的报告。（每个container对应一个executor）\n7. Spark ApplicationMaster直接和container（executor）进行交互，完成这个分布式任务。\n\n\n\n# ApplicationMaster和Driver的区别\n\n首先区分下 AppMaster 和 Driver，任何一个 yarn 上运行的任务都必须有一个 AppMaster，而任何一个 Spark 任务都会有一个 Driver，Driver 就是运行 SparkContext(它会构建 TaskScheduler 和 DAGScheduler) 的进程，当然在 Driver 上你也可以做很多非 Spark 的事情，这些事情只会在 Driver 上面执行，而由 SparkContext 上牵引出来的代码则会由 DAGScheduler 分析，并形成 Job 和 Stage 交由 TaskScheduler，再由 TaskScheduler 交由各 Executor 分布式执行。\n\n所以 Driver 和 AppMaster 是两个完全不同的东西，Driver 是控制 Spark 计算和任务资源的，而 AppMaster 是控制 yarn app 运行和任务资源的，只不过在 Spark on Yarn 上，这两者就出现了交叉，而在 standalone 模式下，资源则由 Driver 管理。在 Spark on Yarn 上，Driver 会和 AppMaster 通信，资源的申请由 AppMaster 来完成，而任务的调度和执行则由 Driver 完成，Driver 会通过与 AppMaster 通信来让 Executor 的执行具体的任务。\n\n> [Spark on Yarn](https://www.cnblogs.com/hseagle/p/3728713.html)\n\n\n","slug":"Spark-on-Yarn源码解析(一)Yarn任务解析","published":1,"updated":"2018-09-04T10:13:56.834Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wg002y2tpbzuy6icg8"},{"title":"Spark-on-Yarn源码解析(二)Spark-Submit解析","date":"2018-09-04T08:03:50.700Z","toc":true,"_content":"\n[TOC]\n\nspark-on-yarn系列\n\n[Spark-on-Yarn 源码解析 (一)Yarn 任务解析](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(一)Yarn任务解析/)\n[Spark-on-Yarn 源码解析 (二)Spark-Submit 解析](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(二)Spark-Submit解析/)\n[Spark-on-Yarn 源码解析 (三)client 做的事情](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(三)client做的事情/)\n[Spark-on-Yarn 源码解析 (四)Spark 业务代码的执行及其任务分配调度 stage 划分](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分/)\n\n\n\n\n\n\n\n\n上文我们了解到了yarn的架构和执行任务的流程，接下来我们看看\n\n# spark-submit命令\n\n    $SPARK_HOME/bin/spark-submit \\\n    --master yarn \\ //提交模式 yarn\n    --deploy-mode cluster \\ //运行的模式，还有一种client模式，但大多用于调试，此处使用cluster模式\n    --class me.yao.spark.me.yao.spark.WordCount \\ //提交的任务\n    --name \"wc\" \\ //任务名字\n    --queue root.default \\ //提交的队列\n    --driver-memory 3g \\ //为driver申请的内存\n    --num-executors 1 \\ //executors的数量，可以理解为线程数，对应yarn中的Container个数\n    --executor-memory 6g \\ //为每一个executor申请的内存\n    --executor-cores 4 \\ //为每一个executor申请的core\n    --conf spark.yarn.driver.memoryOverhead=1g \\ //driver可使用的非堆内存，这些内存用于如VM，字符 串常量池以及其他额外本地开销等\n    --conf spark.yarn.executor.memoryOverhead=2g \\ //每个executor可使用的非堆内存，这些内存用于如 VM，字符串常量池以及其他额外本地开销等\n\n这是通常我们提交spark程序的submit命令，以此为切入点，对spark程序的运行流程做一个跟踪和分析。\n<!--more-->\n查看spark-submit脚本\n![](http://pebgsxjpj.bkt.clouddn.com/15359432887877.jpg)\n\n查看spark-submit脚本的信息，初步可以看到submit启动的类为org.apache.spark.deploy.SparkSubmit，更多细节其实不重要（开个开玩，极客可以求甚解）如果觉得要深究一下为什么是submit的main方法的可以参考一下spark on yarn 作业提交源码分析\n\n\n\n接下来查看该类内部的处理逻辑\n\nSparkSumbmit的类（为了简洁和文章篇幅，只保留了关键流程的信息）\n\n      def main(args: Array[String]): Unit = {\n        val appArgs = new SparkSubmitArguments(args)\n        if (appArgs.verbose) {\n          printStream.println(appArgs)\n        }\n        appArgs.action match {\n          case SparkSubmitAction.SUBMIT => submit(appArgs)\n          case SparkSubmitAction.KILL => kill(appArgs)\n          case SparkSubmitAction.REQUEST_STATUS => requestStatus(appArgs)\n        }\n      }\n    ......\n    \n      private[spark] def submit(args: SparkSubmitArguments): Unit = {\n        val (childArgs, childClasspath, sysProps, childMainClass) = \n    prepareSubmitEnvironment(args)\n    .....\n    .....\n     runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)\n    }\n    \n     private[spark] def prepareSubmitEnvironment(args: SparkSubmitArguments)\n          : (Seq[String], Seq[String], Map[String, String], String) = {\n          ......\n          // In yarn-cluster mode, use yarn.Client as a wrapper around the user class\n        if (isYarnCluster) {\n          childMainClass = \"org.apache.spark.deploy.yarn.Client\"\n          .......\n          }\n    //在submit方法中最终调用的是\n    runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)\n    try {\n          mainClass = Class.forName(childMainClass, true, loader)\n        } catch {\n        ......\n        System.exit(CLASS_NOT_FOUND_EXIT_STATUS)\n        }\n        // SPARK-4170\n        private def runMain(\n          childArgs: Seq[String],\n          childClasspath: Seq[String],\n          sysProps: Map[String, String],\n          childMainClass: String,\n          verbose: Boolean): Unit = {\n        ... ...\n        \n        mainClass = Class.forName(childMainClass, true, loader)\n        ... ...\n        val mainMethod = mainClass.getMethod(\"main\", new Array[String](0).getClass)\n        ... ...\n        mainMethod.invoke(null, childArgs.toArray)\n        ... ...\n        }\n\n通过上面的流程可以看到，这样一个调用链(未特殊表明类名，表明为该步上一步的同一类)，我们代码简化一下，看得舒心明了，再配上解说\n\n    submit.main()\n        ->submit()模式匹配到该方法，因为我们就是submit提交任务\n            ->prepareSubmitEnvironment()该方法中指明了要启动的类，就是大明湖畔的Client\n            ->runMain()通过上步指定的类，然后通过反射调用main方法\n\n既然我们的线路走到org.apache.spark.deploy.yarn.Client        ，那我们再去这个类一看究竟，且听下回分解\n\n","source":"_posts/Spark-on-Yarn源码解析(二)Spark-Submit解析.md","raw":"---\ntitle: Spark-on-Yarn源码解析(二)Spark-Submit解析\ndate: 2018年09月04日\ntags: [Spark,原理]\ncategories: Spark-On-Yarn\ntoc: true\n---\n\n[TOC]\n\nspark-on-yarn系列\n\n[Spark-on-Yarn 源码解析 (一)Yarn 任务解析](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(一)Yarn任务解析/)\n[Spark-on-Yarn 源码解析 (二)Spark-Submit 解析](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(二)Spark-Submit解析/)\n[Spark-on-Yarn 源码解析 (三)client 做的事情](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(三)client做的事情/)\n[Spark-on-Yarn 源码解析 (四)Spark 业务代码的执行及其任务分配调度 stage 划分](http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分/)\n\n\n\n\n\n\n\n\n上文我们了解到了yarn的架构和执行任务的流程，接下来我们看看\n\n# spark-submit命令\n\n    $SPARK_HOME/bin/spark-submit \\\n    --master yarn \\ //提交模式 yarn\n    --deploy-mode cluster \\ //运行的模式，还有一种client模式，但大多用于调试，此处使用cluster模式\n    --class me.yao.spark.me.yao.spark.WordCount \\ //提交的任务\n    --name \"wc\" \\ //任务名字\n    --queue root.default \\ //提交的队列\n    --driver-memory 3g \\ //为driver申请的内存\n    --num-executors 1 \\ //executors的数量，可以理解为线程数，对应yarn中的Container个数\n    --executor-memory 6g \\ //为每一个executor申请的内存\n    --executor-cores 4 \\ //为每一个executor申请的core\n    --conf spark.yarn.driver.memoryOverhead=1g \\ //driver可使用的非堆内存，这些内存用于如VM，字符 串常量池以及其他额外本地开销等\n    --conf spark.yarn.executor.memoryOverhead=2g \\ //每个executor可使用的非堆内存，这些内存用于如 VM，字符串常量池以及其他额外本地开销等\n\n这是通常我们提交spark程序的submit命令，以此为切入点，对spark程序的运行流程做一个跟踪和分析。\n<!--more-->\n查看spark-submit脚本\n![](http://pebgsxjpj.bkt.clouddn.com/15359432887877.jpg)\n\n查看spark-submit脚本的信息，初步可以看到submit启动的类为org.apache.spark.deploy.SparkSubmit，更多细节其实不重要（开个开玩，极客可以求甚解）如果觉得要深究一下为什么是submit的main方法的可以参考一下spark on yarn 作业提交源码分析\n\n\n\n接下来查看该类内部的处理逻辑\n\nSparkSumbmit的类（为了简洁和文章篇幅，只保留了关键流程的信息）\n\n      def main(args: Array[String]): Unit = {\n        val appArgs = new SparkSubmitArguments(args)\n        if (appArgs.verbose) {\n          printStream.println(appArgs)\n        }\n        appArgs.action match {\n          case SparkSubmitAction.SUBMIT => submit(appArgs)\n          case SparkSubmitAction.KILL => kill(appArgs)\n          case SparkSubmitAction.REQUEST_STATUS => requestStatus(appArgs)\n        }\n      }\n    ......\n    \n      private[spark] def submit(args: SparkSubmitArguments): Unit = {\n        val (childArgs, childClasspath, sysProps, childMainClass) = \n    prepareSubmitEnvironment(args)\n    .....\n    .....\n     runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)\n    }\n    \n     private[spark] def prepareSubmitEnvironment(args: SparkSubmitArguments)\n          : (Seq[String], Seq[String], Map[String, String], String) = {\n          ......\n          // In yarn-cluster mode, use yarn.Client as a wrapper around the user class\n        if (isYarnCluster) {\n          childMainClass = \"org.apache.spark.deploy.yarn.Client\"\n          .......\n          }\n    //在submit方法中最终调用的是\n    runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)\n    try {\n          mainClass = Class.forName(childMainClass, true, loader)\n        } catch {\n        ......\n        System.exit(CLASS_NOT_FOUND_EXIT_STATUS)\n        }\n        // SPARK-4170\n        private def runMain(\n          childArgs: Seq[String],\n          childClasspath: Seq[String],\n          sysProps: Map[String, String],\n          childMainClass: String,\n          verbose: Boolean): Unit = {\n        ... ...\n        \n        mainClass = Class.forName(childMainClass, true, loader)\n        ... ...\n        val mainMethod = mainClass.getMethod(\"main\", new Array[String](0).getClass)\n        ... ...\n        mainMethod.invoke(null, childArgs.toArray)\n        ... ...\n        }\n\n通过上面的流程可以看到，这样一个调用链(未特殊表明类名，表明为该步上一步的同一类)，我们代码简化一下，看得舒心明了，再配上解说\n\n    submit.main()\n        ->submit()模式匹配到该方法，因为我们就是submit提交任务\n            ->prepareSubmitEnvironment()该方法中指明了要启动的类，就是大明湖畔的Client\n            ->runMain()通过上步指定的类，然后通过反射调用main方法\n\n既然我们的线路走到org.apache.spark.deploy.yarn.Client        ，那我们再去这个类一看究竟，且听下回分解\n\n","slug":"Spark-on-Yarn源码解析(二)Spark-Submit解析","published":1,"updated":"2018-09-04T10:13:14.301Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wh00322tpbjviuf6mi"},{"title":"Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分","date":"2018-09-04T08:16:39.931Z","toc":true,"_content":"\nspark-on-yarn系列\n[Spark-on-Yarn 源码解析 (一)Yarn 任务解析](\"http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(一)Yarn任务解析/\")\n[Spark-on-Yarn 源码解析 (二)Spark-Submit 解析](\"http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(二)Spark-Submit解析/\")\n[Spark-on-Yarn 源码解析 (三)client 做的事情](\"http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(三)client做的事情/\")\n[Spark-on-Yarn 源码解析 (四)Spark 业务代码的执行及其任务分配调度 stage 划分](\"http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分/\")\n\n\n# 看看自定义的类\n\n```scala\nobject WordCount {\n\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"yaoWordCount\").setMaster(\"local[2]\")\n    val sc = new SparkContext(conf)\n    var hadoopRDD: RDD[String] = sc.textFile(args(0))\n    var hdfsRDD: RDD[String] = hadoopRDD.flatMap(_.split(\"\"))\n    //单词和出现的次数，构建RDD并且调用了他的Transformation\n    //返回的是一个hadoopRDD\n    //transFormation都是返回的RDD\n    var wordAndCount: RDD[(String, Int)] = hdfsRDD.map((_, 1))\n    //创建RDD 这里面有两个RDD,一个是hadoopRDD，然后会生成一个paritionRDD\n    //savaasTextfile还会产生一个RDD,因为会调用mapPartitons\n    //调用RDD的action 开始真正提交任务\n    var reducedRDD: RDD[(String, Int)] = wordAndCount.reduceByKey(_ + _)\n    reducedRDD.saveAsTextFile(args(1))\n    //关闭saprkContext资源\n    sc.stop()\n  }\n}\n```\n<!--more-->\n# sparkContext的初始化\n\n对于Spark程序入口为SparkContext,当我们使用spark-submit/spark-shell等命令来启动一个客户端,客户端与集群需要建立链接，建立的这个链接对象就叫做sparkContext，只有这个对象创建成功才标志这这个客户端与spark集群链接成功。现就将从SparkContext展开来描述一下Spark的任务启动和执行流程。\nSparkContext 完成了以下几个主要的功能： \n（1）创建 RDD，通过类似 textFile 等的方法。 \n（2）与资源管理器交互，通过 runJob 等方法启动应用。 \n（3）创建 DAGScheduler、TaskScheduler 等。 \n\n在SparkContext类中，SparkContext主构造器主要做\n\n我们看一下SparkContext的主构造器\n\n- 调用CreateSparkEnv方法创建SparkEnv(将driver的信息，url，ip等都封装)，SparkEnv中有一个对象ActorSystem\n- 创建TaskScheduler ，根据提交任务的URL（如：spark://(.*)\"，local[1]等，去创建TaskSchedulerImpl ，然后再创建SparkDeploySchedulerBackend(先后创建driverActor和clientActor)\n- 创建DAGScheduler\n- TaskScheduler启动，TaskScheduler.start()\n\n\n\n\n\n## SparkEnv\n\n最终将driver的host,port端口等各种信息都封装到里面\n\n```scala\nnew SparkEnv(\n  executorId,\n  actorSystem,\n  serializer,\n  closureSerializer,\n  cacheManager,\n  mapOutputTracker,\n  shuffleManager,\n  broadcastManager,\n  blockTransferService,\n  blockManager,\n  securityManager,\n  httpFileServer,\n  sparkFilesDir,\n  metricsSystem,\n  shuffleMemoryManager,\n  outputCommitCoordinator,\n  conf)\n```\n\n\n\n## TaskScheduler\n\n在SparkContext类中可以看到，TaskScheduler根据url类型匹配创建TaskSchedulerImpl\n\n```scala\n //TODO 根据提交任务时指定的URL创建相应的TaskScheduler\n  private def createTaskScheduler(\n      sc: SparkContext,\n      master: String): (SchedulerBackend, TaskScheduler) = {\n      ...\n      case \"yarn-standalone\" | \"yarn-cluster\" =>\n...\n        val scheduler = try {\n          val clazz = Class.forName(\"org.apache.spark.scheduler.cluster.YarnClusterScheduler\")\n          val cons = clazz.getConstructor(classOf[SparkContext])\n          cons.newInstance(sc).asInstanceOf[TaskSchedulerImpl]\n          }\n      ...\n        val backend = try {\n          val clazz =\n            Class.forName(\"org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend\")\n          val cons = clazz.getConstructor(classOf[TaskSchedulerImpl], classOf[SparkContext])\n          cons.newInstance(scheduler, sc).asInstanceOf[CoarseGrainedSchedulerBackend]\n        } \n        scheduler.initialize(backend)\n        (backend, scheduler)\n        ....\n        }\n```\n<!--more-->\n可知\nTaskScheduler 的实现类`org.apache.spark.scheduler.cluster.YarnScheduler`\nTaskSchedulerBacked 的实现类为`org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend`\n且TaskScheduler对TaskSchedulerBacked保持了引用\nscheduler.initialize(backend)\n\n### 启动TaskScheduler\n\n在Spark的构造函数中,会启动TaskScheduler\n\n```scala\ntaskScheduler.start()\n```\n\n可以看到继承关系\n\n```scala\nprivate[spark] class YarnClusterScheduler(sc: SparkContext) extends YarnScheduler(sc) \nprivate[spark] class YarnScheduler(sc: SparkContext) extends TaskSchedulerImpl(sc)\n```\n\n可以跟踪到，start方法最终调用的是TaskSchedulerImpl里面start方法，在start方法里面\n\n```scala\n override def start() {\n    //TODO 首先调用SparkDeploySchedulerBackend的start方法\n    backend.start()\n    ......\n}\n```\n\n,这里的backend就是YarnClusterSchedulerBackend，而这个最终继承的是CoarseGrainedSchedulerBackend中start方法\n\n```scala\n  override def start() {\n  ...\n    driverActor = actorSystem.actorOf(\n      Props(new DriverActor(properties)), name = CoarseGrainedSchedulerBackend.ACTOR_NAME)\n  }\n```\n\n获取到spark的配置信息后，会创建driverActor\n\n## DAGScheduler\n\n在SparkContext的构造函数中，会创建DAGScheduler\n\n```scala\n    dagScheduler= new DAGScheduler(this)\n```\n\n在DAGScheduler构造函数中\n\n```scala\n def this(sc: SparkContext) = this(sc, sc.taskScheduler)\n```\n\n可以看到DAGScheduler对TaskScheduler保持了引用\n\n```scala\nclass DAGScheduler(\n    private[scheduler] val sc: SparkContext,\n    private[scheduler] val taskScheduler: TaskScheduler,\n    listenerBus: LiveListenerBus,\n    mapOutputTracker: MapOutputTrackerMaster,\n    blockManagerMaster: BlockManagerMaster,\n    env: SparkEnv,\n    clock: Clock = new SystemClock())\n  extends Logging {\n  ......\n  }\n```\n\n- mapOutputTracker：是运行在 Driver 端管理 shuffle 的中间输出位置信息的。 \n- blockManagerMaster：也是运行在 Driver 端的，它是管理整个 Job 的 Bolck 信息。\n\n# RDD的构建过程\n\n其中hadoopRDD，hdfsRDD，wordRDD，reduceRDD是经过一系列transformation装换rdd，只有等到action时，才会触发数据的流转\n\n该例的action为saveAsTextFile调用链为\n\n```\n   saveAsTextFile()\n       saveAsHadoopFile()\n            saveAsHadoopFile（重载函数）\n                    saveAsHadoopDataset()\n                        runJob()之间会调用几个重载函数\n                        dagScheduler.runJob()最终调用\n```\n\n# 作业提交\n\n# 任务流转\n\n首先注意区分 2 个概述： \njob: 每个 action 都是执行 runJob 方法，可以将之视为一个 job。 \nstage：在这个 job 内部，会根据宽依赖，划分成多个 stage。\n\n在action触发后，最最终调用的是DAGScheduler.runJob()\n\n```scala\ndagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)\n```\n\n而runJob() 的核心代码为：\n\n```scala\nval waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)\n```\n\n即调用 submitJob 方法，我们进一步看看 submitJob()\n\n```scala\n  def submitJob[T, U](\n      rdd: RDD[T],\n      func: (TaskContext, Iterator[T]) => U,\n      partitions: Seq[Int],\n      callSite: CallSite,\n      resultHandler: (Int, U) => Unit,\n      properties: Properties): JobWaiter[U] = {\n....    \n    val jobId = nextJobId.getAndIncrement()\n.....\n\n    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)\n    eventProcessLoop.post(JobSubmitted(\n      jobId, rdd, func2, partitions.toArray, callSite, waiter,\n      SerializationUtils.clone(properties)))\n    waiter\n  } \n```\n\nsubmitJob() 方法主要完成了以下 3 个工作： \n\n- 获取一个新的 jobId \n- 生成一个 JobWaiter，它会监听 Job 的执行状态，而 Job 是由多个 Task 组成的，因此只有当 Job 的所有 Task 均已完成，Job 才会标记成功 \n- 最后调用 eventProcessLoop.post() 将 Job 提交到一个队列中，等待处理。这是一个典型的生产者消费者模式。这些消息都是通过 handleJobSubmitted 来处理。\n\n简单看一下 handleJobSubmitted 是如何被调用的。 \n首先是 DAGSchedulerEventProcessLoop#onReceive 调用 \n\n```scala\n  //TODO 通过模式匹配判断事件的类型 比如任务提交，作业取消 ...\n  override def onReceive(event: DAGSchedulerEvent): Unit = event match {\n      //TODO 提交计算任务\n    case JobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite, listener, properties) =>\n      //todo 调用dagScheduler的handlerJobSubmitted方法处理\n      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite,  listener, properties)\n    ... ...   \n```\n\nDAGSchedulerEventProcessLoop 是 EventLoop 的子类，它重写了 EventLoop 的 onReceive 方法。以后再分析这个 EventLoop。\nonReceive 会调用 handleJobSubmitted。\n\n## stage 的划分\n\n刚才说到 handleJobSubmitted 会从 eventProcessLoop 中取出 Job 来进行处理，处理的第一步就是将 Job 划分成不同的 stage。handleJobSubmitted 主要 2 个工作，一是进行 stage 的划分，这是这部分要介绍的内容；二是创建一个 activeJob，并生成一个任务，这在下一小节介绍。\n\n还是先看看调用链\n\n```\n   handleJobSubmitted\n       ->newStage()\n         ->getParentStages()//此处会遍历RDD所有依赖\n            ->getShuffleMapStage()//如果是ShuffleDependency（宽依赖，获取到一个Map）\n                ->newOrUsedStage()//这就可以解释我们常说的遇到宽依赖就会划分stage，并且返回stage                \n```\n\n所以最终返回的是一个拥有款依赖的           \n\n```scala\n  private[scheduler] def handleJobSubmitted(jobId: Int,\n      finalRDD: RDD[_],\n      func: (TaskContext, Iterator[_]) => _,\n      partitions: Array[Int],\n      callSite: CallSite,\n      listener: JobListener,\n      properties: Properties) {\n      ...\n      //todo 重要：该方法用于划分stage，主要依赖的是finalStage\n       finalStage = newStage(finalRDD, partitions.size, None, jobId, callSite)\n      .....\n    //TODO 集群模式\n      activeJobs += job\n      ......\n    //todo 提交stage\n      submitStage(finalStage)\n    }\n    //TODO  开始向集群提交还在等待的stage\n    submitWaitingStages()\n  }\n```\n\ngetParentStages()。 \n因为是从最终的 stage 往回推算的，这需要计算最终 stage 所依赖的各个 stage。\n\n```scala\n //TODO 用于获取父stage\n  private def getParentStages(rdd: RDD[_], jobId: Int): List[Stage] = {\n    val parents = new HashSet[Stage]\n    val waitingForVisit = new Stack[RDD[_]]\n    def visit(r: RDD[_]) {\n      if (!visited(r)) {\n        visited + r\n        for (dep <- r.dependencies) {\n          dep match {\n            case shufDep: ShuffleDependency[_, _, _] =>\n              //TODO 把宽依赖传进去，获得父stage\n              parents += getShuffleMapStage(shufDep, jobId)\n            case _ =>\n              waitingForVisit.push(dep.rdd)\n          }\n        }\n      }\n    }\n    waitingForVisit.push(rdd)\n    while (!waitingForVisit.isEmpty) {\n      visit(waitingForVisit.pop())\n    }\n    parents.toList\n  }\n```\n\n## 任务的生成\n\n回到 handleJobSubmitted 中的代码：\n\n```\nsubmitStage(finalStage)\n```\n\nsubmitStage 会提交 finalStage，如果这个 stage 的某些 parentStage 未提交，则递归调用 submitStage()，直至所有的 stage 均已计算完成。\n\nsubmitStage() 会调用 submitMissingTasks():\n\nsubmitMissingTasks(stage, jobId.get)\n\n而 submitMissingTasks() 会完成 DAGScheduler 最后的工作：它判断出哪些 Partition 需要计算，为每个 Partition 生成 Task，然后这些 Task 就会封闭到 TaskSet\n\n```scala\n //TODO  DAG提交stage  根据最后一个stage  开始找到第一个stage递归提交stage\n  /** Submits stage, but first recursively submits any missing parents. */\n  private def submitStage(stage: Stage) {\n    val jobId = activeJobForStage(stage)\n    if (jobId.isDefined) {\n      \n      if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {\n        //TODO 获取他的父stage 没有提交的stage\n        val missing = getMissingParentStages(stage).sortBy(_.id)\n        //todo 判断父stage是否为空，为空就以为着他是第一stage\n        if (missing == Nil) {\n       //TODO 开始提交最前面的stage, DAG提交stage给TaskScheduler 会将stage转换成taskSet\n          submitMissingTasks(stage, jobId.get)\n        } else {\n          //TODO 有父stage  就递归提交\n          for (parent <- missing) {\n            submitStage(parent)\n          }\n          waitingStages += stage\n        }\n      }\n    } else {\n      abortStage(stage, \"No active job for stage \" + stage.id)\n    }\n  }\n\n```\n\nsubmitMissingTasks在最后提交给 TaskScheduler 进行处理\n\n```scala\n  //TODO  DAG提交stage给TaskScheduler 会将stage转换成taskSet\n  private def submitMissingTasks(stage: Stage, jobId: Int) {\n...\n//TODO 创建多少个Task\n\n    val tasks: Seq[Task[_]] = if (stage.isShuffleMap) {\n      partitionsToCompute.map { id =>\n        //TODO 数据存储的最佳位置   移动计算，而不是移动数据\n        val locs = getPreferredLocs(stage.rdd, id)\n        val part = stage.rdd.partitions(id)\n        //TODO 从上游拉取数据\n        new ShuffleMapTask(stage.id, taskBinary, part, locs)\n      }\n    } else {\n      val job = stage.resultOfJob.get\n      partitionsToCompute.map { id =>\n        val p: Int = job.partitions(id)\n        val part = stage.rdd.partitions(p)\n        val locs = getPreferredLocs(stage.rdd, p)\n        //TODO  将数据写入某个介质里面，nosql hdfs 等等\n        new ResultTask(stage.id, taskBinary, part, locs, id)\n      }\n    }\n\n//TODO task的数量最好和分区数一样  如果分区数大于0\n //TODO task的数量最好和分区数一样  如果分区数大于0\n    if (tasks.size > 0) {\n      logInfo(\"Submitting \" + tasks.size + \" missing tasks from \" + stage + \" (\" + stage.rdd + \")\")\n      stage.pendingTasks ++= tasks\n\n      //TODO 调用taskScheduler的submitTasks提交taskSet 现在将task转换成一个array\ntaskScheduler.submitTasks(new TaskSet(\n        tasks.toArray, stage.id, stage.latestInfo.attemptId, stage.firstJobId, properties))\n      stage.latestInfo.submissionTime = Some(clock.getTimeMillis())\n      .....\n}\n```\n\n## TaskScheduler && TaskSchedulerBackend\n\n上文分析到在 DAGScheduler 中最终会执行 taskScheduler.submitTasks() 方法，我们先简单看一下从这里开始往下的执行逻辑：\n\n```\n①taskScheduler.submitTasks()\n    ->②schedulableBuilder.addTaskSetManager() 调度模式，是先来先服务还是公平调度模式\n    ->③CoarseGrainedSchedulerBackend.reviveOffers() 这个是向driverActor发送消息driverActor ! ReviveOffers\n        ->④CoarseGrainedSchedulerBackend.receiveWithLogging 这是driverActor接收消息的部分\n            ->⑤CoarseGrainedSchedulerBackend.makeOffers() //case ReviveOffers =>makeOffers()\n        这个模式匹配会调用maksOffers方法\n                ->⑥launchTasks()调用launchTask向Executor提交task\n                    ->⑦ executorData.executorActor ! LaunchTask(new SerializableBuffer(serializedTask))向executor发送序列化好的task，发送一个Task\n```\n\n步骤一、二中主要将这组\n任务的 TaskSet 加入到一个 TaskSetManager 中。TaskSetManager 会根据数据就近原则为 task 分配计算资源，监控 task 的执行状态等，比如失败重试，推测执行等。 \n步骤三、四逻辑较为简单。 \n步骤五为每个 task 具体分配资源，它的输入是一个 Executor 的列表，输出是 TaskDescription 的二维数组。TaskDescription 包含了 TaskID, Executor ID 和 task 执行的依赖信息等。 \n步骤六、七就是将任务真正的发送到 executor 中执行了，并等待 executor 的状态返回。\n\n​                 \n\n","source":"_posts/Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分.md","raw":"---\ntitle: Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分\ndate: 2018年09月04日\ntags: [Spark,原理]\ncategories: Spark-On-Yarn\ntoc: true\n---\n\nspark-on-yarn系列\n[Spark-on-Yarn 源码解析 (一)Yarn 任务解析](\"http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(一)Yarn任务解析/\")\n[Spark-on-Yarn 源码解析 (二)Spark-Submit 解析](\"http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(二)Spark-Submit解析/\")\n[Spark-on-Yarn 源码解析 (三)client 做的事情](\"http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(三)client做的事情/\")\n[Spark-on-Yarn 源码解析 (四)Spark 业务代码的执行及其任务分配调度 stage 划分](\"http://www.gangtieguo.cn/2018/09/04/Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分/\")\n\n\n# 看看自定义的类\n\n```scala\nobject WordCount {\n\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"yaoWordCount\").setMaster(\"local[2]\")\n    val sc = new SparkContext(conf)\n    var hadoopRDD: RDD[String] = sc.textFile(args(0))\n    var hdfsRDD: RDD[String] = hadoopRDD.flatMap(_.split(\"\"))\n    //单词和出现的次数，构建RDD并且调用了他的Transformation\n    //返回的是一个hadoopRDD\n    //transFormation都是返回的RDD\n    var wordAndCount: RDD[(String, Int)] = hdfsRDD.map((_, 1))\n    //创建RDD 这里面有两个RDD,一个是hadoopRDD，然后会生成一个paritionRDD\n    //savaasTextfile还会产生一个RDD,因为会调用mapPartitons\n    //调用RDD的action 开始真正提交任务\n    var reducedRDD: RDD[(String, Int)] = wordAndCount.reduceByKey(_ + _)\n    reducedRDD.saveAsTextFile(args(1))\n    //关闭saprkContext资源\n    sc.stop()\n  }\n}\n```\n<!--more-->\n# sparkContext的初始化\n\n对于Spark程序入口为SparkContext,当我们使用spark-submit/spark-shell等命令来启动一个客户端,客户端与集群需要建立链接，建立的这个链接对象就叫做sparkContext，只有这个对象创建成功才标志这这个客户端与spark集群链接成功。现就将从SparkContext展开来描述一下Spark的任务启动和执行流程。\nSparkContext 完成了以下几个主要的功能： \n（1）创建 RDD，通过类似 textFile 等的方法。 \n（2）与资源管理器交互，通过 runJob 等方法启动应用。 \n（3）创建 DAGScheduler、TaskScheduler 等。 \n\n在SparkContext类中，SparkContext主构造器主要做\n\n我们看一下SparkContext的主构造器\n\n- 调用CreateSparkEnv方法创建SparkEnv(将driver的信息，url，ip等都封装)，SparkEnv中有一个对象ActorSystem\n- 创建TaskScheduler ，根据提交任务的URL（如：spark://(.*)\"，local[1]等，去创建TaskSchedulerImpl ，然后再创建SparkDeploySchedulerBackend(先后创建driverActor和clientActor)\n- 创建DAGScheduler\n- TaskScheduler启动，TaskScheduler.start()\n\n\n\n\n\n## SparkEnv\n\n最终将driver的host,port端口等各种信息都封装到里面\n\n```scala\nnew SparkEnv(\n  executorId,\n  actorSystem,\n  serializer,\n  closureSerializer,\n  cacheManager,\n  mapOutputTracker,\n  shuffleManager,\n  broadcastManager,\n  blockTransferService,\n  blockManager,\n  securityManager,\n  httpFileServer,\n  sparkFilesDir,\n  metricsSystem,\n  shuffleMemoryManager,\n  outputCommitCoordinator,\n  conf)\n```\n\n\n\n## TaskScheduler\n\n在SparkContext类中可以看到，TaskScheduler根据url类型匹配创建TaskSchedulerImpl\n\n```scala\n //TODO 根据提交任务时指定的URL创建相应的TaskScheduler\n  private def createTaskScheduler(\n      sc: SparkContext,\n      master: String): (SchedulerBackend, TaskScheduler) = {\n      ...\n      case \"yarn-standalone\" | \"yarn-cluster\" =>\n...\n        val scheduler = try {\n          val clazz = Class.forName(\"org.apache.spark.scheduler.cluster.YarnClusterScheduler\")\n          val cons = clazz.getConstructor(classOf[SparkContext])\n          cons.newInstance(sc).asInstanceOf[TaskSchedulerImpl]\n          }\n      ...\n        val backend = try {\n          val clazz =\n            Class.forName(\"org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend\")\n          val cons = clazz.getConstructor(classOf[TaskSchedulerImpl], classOf[SparkContext])\n          cons.newInstance(scheduler, sc).asInstanceOf[CoarseGrainedSchedulerBackend]\n        } \n        scheduler.initialize(backend)\n        (backend, scheduler)\n        ....\n        }\n```\n<!--more-->\n可知\nTaskScheduler 的实现类`org.apache.spark.scheduler.cluster.YarnScheduler`\nTaskSchedulerBacked 的实现类为`org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend`\n且TaskScheduler对TaskSchedulerBacked保持了引用\nscheduler.initialize(backend)\n\n### 启动TaskScheduler\n\n在Spark的构造函数中,会启动TaskScheduler\n\n```scala\ntaskScheduler.start()\n```\n\n可以看到继承关系\n\n```scala\nprivate[spark] class YarnClusterScheduler(sc: SparkContext) extends YarnScheduler(sc) \nprivate[spark] class YarnScheduler(sc: SparkContext) extends TaskSchedulerImpl(sc)\n```\n\n可以跟踪到，start方法最终调用的是TaskSchedulerImpl里面start方法，在start方法里面\n\n```scala\n override def start() {\n    //TODO 首先调用SparkDeploySchedulerBackend的start方法\n    backend.start()\n    ......\n}\n```\n\n,这里的backend就是YarnClusterSchedulerBackend，而这个最终继承的是CoarseGrainedSchedulerBackend中start方法\n\n```scala\n  override def start() {\n  ...\n    driverActor = actorSystem.actorOf(\n      Props(new DriverActor(properties)), name = CoarseGrainedSchedulerBackend.ACTOR_NAME)\n  }\n```\n\n获取到spark的配置信息后，会创建driverActor\n\n## DAGScheduler\n\n在SparkContext的构造函数中，会创建DAGScheduler\n\n```scala\n    dagScheduler= new DAGScheduler(this)\n```\n\n在DAGScheduler构造函数中\n\n```scala\n def this(sc: SparkContext) = this(sc, sc.taskScheduler)\n```\n\n可以看到DAGScheduler对TaskScheduler保持了引用\n\n```scala\nclass DAGScheduler(\n    private[scheduler] val sc: SparkContext,\n    private[scheduler] val taskScheduler: TaskScheduler,\n    listenerBus: LiveListenerBus,\n    mapOutputTracker: MapOutputTrackerMaster,\n    blockManagerMaster: BlockManagerMaster,\n    env: SparkEnv,\n    clock: Clock = new SystemClock())\n  extends Logging {\n  ......\n  }\n```\n\n- mapOutputTracker：是运行在 Driver 端管理 shuffle 的中间输出位置信息的。 \n- blockManagerMaster：也是运行在 Driver 端的，它是管理整个 Job 的 Bolck 信息。\n\n# RDD的构建过程\n\n其中hadoopRDD，hdfsRDD，wordRDD，reduceRDD是经过一系列transformation装换rdd，只有等到action时，才会触发数据的流转\n\n该例的action为saveAsTextFile调用链为\n\n```\n   saveAsTextFile()\n       saveAsHadoopFile()\n            saveAsHadoopFile（重载函数）\n                    saveAsHadoopDataset()\n                        runJob()之间会调用几个重载函数\n                        dagScheduler.runJob()最终调用\n```\n\n# 作业提交\n\n# 任务流转\n\n首先注意区分 2 个概述： \njob: 每个 action 都是执行 runJob 方法，可以将之视为一个 job。 \nstage：在这个 job 内部，会根据宽依赖，划分成多个 stage。\n\n在action触发后，最最终调用的是DAGScheduler.runJob()\n\n```scala\ndagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)\n```\n\n而runJob() 的核心代码为：\n\n```scala\nval waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)\n```\n\n即调用 submitJob 方法，我们进一步看看 submitJob()\n\n```scala\n  def submitJob[T, U](\n      rdd: RDD[T],\n      func: (TaskContext, Iterator[T]) => U,\n      partitions: Seq[Int],\n      callSite: CallSite,\n      resultHandler: (Int, U) => Unit,\n      properties: Properties): JobWaiter[U] = {\n....    \n    val jobId = nextJobId.getAndIncrement()\n.....\n\n    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)\n    eventProcessLoop.post(JobSubmitted(\n      jobId, rdd, func2, partitions.toArray, callSite, waiter,\n      SerializationUtils.clone(properties)))\n    waiter\n  } \n```\n\nsubmitJob() 方法主要完成了以下 3 个工作： \n\n- 获取一个新的 jobId \n- 生成一个 JobWaiter，它会监听 Job 的执行状态，而 Job 是由多个 Task 组成的，因此只有当 Job 的所有 Task 均已完成，Job 才会标记成功 \n- 最后调用 eventProcessLoop.post() 将 Job 提交到一个队列中，等待处理。这是一个典型的生产者消费者模式。这些消息都是通过 handleJobSubmitted 来处理。\n\n简单看一下 handleJobSubmitted 是如何被调用的。 \n首先是 DAGSchedulerEventProcessLoop#onReceive 调用 \n\n```scala\n  //TODO 通过模式匹配判断事件的类型 比如任务提交，作业取消 ...\n  override def onReceive(event: DAGSchedulerEvent): Unit = event match {\n      //TODO 提交计算任务\n    case JobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite, listener, properties) =>\n      //todo 调用dagScheduler的handlerJobSubmitted方法处理\n      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite,  listener, properties)\n    ... ...   \n```\n\nDAGSchedulerEventProcessLoop 是 EventLoop 的子类，它重写了 EventLoop 的 onReceive 方法。以后再分析这个 EventLoop。\nonReceive 会调用 handleJobSubmitted。\n\n## stage 的划分\n\n刚才说到 handleJobSubmitted 会从 eventProcessLoop 中取出 Job 来进行处理，处理的第一步就是将 Job 划分成不同的 stage。handleJobSubmitted 主要 2 个工作，一是进行 stage 的划分，这是这部分要介绍的内容；二是创建一个 activeJob，并生成一个任务，这在下一小节介绍。\n\n还是先看看调用链\n\n```\n   handleJobSubmitted\n       ->newStage()\n         ->getParentStages()//此处会遍历RDD所有依赖\n            ->getShuffleMapStage()//如果是ShuffleDependency（宽依赖，获取到一个Map）\n                ->newOrUsedStage()//这就可以解释我们常说的遇到宽依赖就会划分stage，并且返回stage                \n```\n\n所以最终返回的是一个拥有款依赖的           \n\n```scala\n  private[scheduler] def handleJobSubmitted(jobId: Int,\n      finalRDD: RDD[_],\n      func: (TaskContext, Iterator[_]) => _,\n      partitions: Array[Int],\n      callSite: CallSite,\n      listener: JobListener,\n      properties: Properties) {\n      ...\n      //todo 重要：该方法用于划分stage，主要依赖的是finalStage\n       finalStage = newStage(finalRDD, partitions.size, None, jobId, callSite)\n      .....\n    //TODO 集群模式\n      activeJobs += job\n      ......\n    //todo 提交stage\n      submitStage(finalStage)\n    }\n    //TODO  开始向集群提交还在等待的stage\n    submitWaitingStages()\n  }\n```\n\ngetParentStages()。 \n因为是从最终的 stage 往回推算的，这需要计算最终 stage 所依赖的各个 stage。\n\n```scala\n //TODO 用于获取父stage\n  private def getParentStages(rdd: RDD[_], jobId: Int): List[Stage] = {\n    val parents = new HashSet[Stage]\n    val waitingForVisit = new Stack[RDD[_]]\n    def visit(r: RDD[_]) {\n      if (!visited(r)) {\n        visited + r\n        for (dep <- r.dependencies) {\n          dep match {\n            case shufDep: ShuffleDependency[_, _, _] =>\n              //TODO 把宽依赖传进去，获得父stage\n              parents += getShuffleMapStage(shufDep, jobId)\n            case _ =>\n              waitingForVisit.push(dep.rdd)\n          }\n        }\n      }\n    }\n    waitingForVisit.push(rdd)\n    while (!waitingForVisit.isEmpty) {\n      visit(waitingForVisit.pop())\n    }\n    parents.toList\n  }\n```\n\n## 任务的生成\n\n回到 handleJobSubmitted 中的代码：\n\n```\nsubmitStage(finalStage)\n```\n\nsubmitStage 会提交 finalStage，如果这个 stage 的某些 parentStage 未提交，则递归调用 submitStage()，直至所有的 stage 均已计算完成。\n\nsubmitStage() 会调用 submitMissingTasks():\n\nsubmitMissingTasks(stage, jobId.get)\n\n而 submitMissingTasks() 会完成 DAGScheduler 最后的工作：它判断出哪些 Partition 需要计算，为每个 Partition 生成 Task，然后这些 Task 就会封闭到 TaskSet\n\n```scala\n //TODO  DAG提交stage  根据最后一个stage  开始找到第一个stage递归提交stage\n  /** Submits stage, but first recursively submits any missing parents. */\n  private def submitStage(stage: Stage) {\n    val jobId = activeJobForStage(stage)\n    if (jobId.isDefined) {\n      \n      if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {\n        //TODO 获取他的父stage 没有提交的stage\n        val missing = getMissingParentStages(stage).sortBy(_.id)\n        //todo 判断父stage是否为空，为空就以为着他是第一stage\n        if (missing == Nil) {\n       //TODO 开始提交最前面的stage, DAG提交stage给TaskScheduler 会将stage转换成taskSet\n          submitMissingTasks(stage, jobId.get)\n        } else {\n          //TODO 有父stage  就递归提交\n          for (parent <- missing) {\n            submitStage(parent)\n          }\n          waitingStages += stage\n        }\n      }\n    } else {\n      abortStage(stage, \"No active job for stage \" + stage.id)\n    }\n  }\n\n```\n\nsubmitMissingTasks在最后提交给 TaskScheduler 进行处理\n\n```scala\n  //TODO  DAG提交stage给TaskScheduler 会将stage转换成taskSet\n  private def submitMissingTasks(stage: Stage, jobId: Int) {\n...\n//TODO 创建多少个Task\n\n    val tasks: Seq[Task[_]] = if (stage.isShuffleMap) {\n      partitionsToCompute.map { id =>\n        //TODO 数据存储的最佳位置   移动计算，而不是移动数据\n        val locs = getPreferredLocs(stage.rdd, id)\n        val part = stage.rdd.partitions(id)\n        //TODO 从上游拉取数据\n        new ShuffleMapTask(stage.id, taskBinary, part, locs)\n      }\n    } else {\n      val job = stage.resultOfJob.get\n      partitionsToCompute.map { id =>\n        val p: Int = job.partitions(id)\n        val part = stage.rdd.partitions(p)\n        val locs = getPreferredLocs(stage.rdd, p)\n        //TODO  将数据写入某个介质里面，nosql hdfs 等等\n        new ResultTask(stage.id, taskBinary, part, locs, id)\n      }\n    }\n\n//TODO task的数量最好和分区数一样  如果分区数大于0\n //TODO task的数量最好和分区数一样  如果分区数大于0\n    if (tasks.size > 0) {\n      logInfo(\"Submitting \" + tasks.size + \" missing tasks from \" + stage + \" (\" + stage.rdd + \")\")\n      stage.pendingTasks ++= tasks\n\n      //TODO 调用taskScheduler的submitTasks提交taskSet 现在将task转换成一个array\ntaskScheduler.submitTasks(new TaskSet(\n        tasks.toArray, stage.id, stage.latestInfo.attemptId, stage.firstJobId, properties))\n      stage.latestInfo.submissionTime = Some(clock.getTimeMillis())\n      .....\n}\n```\n\n## TaskScheduler && TaskSchedulerBackend\n\n上文分析到在 DAGScheduler 中最终会执行 taskScheduler.submitTasks() 方法，我们先简单看一下从这里开始往下的执行逻辑：\n\n```\n①taskScheduler.submitTasks()\n    ->②schedulableBuilder.addTaskSetManager() 调度模式，是先来先服务还是公平调度模式\n    ->③CoarseGrainedSchedulerBackend.reviveOffers() 这个是向driverActor发送消息driverActor ! ReviveOffers\n        ->④CoarseGrainedSchedulerBackend.receiveWithLogging 这是driverActor接收消息的部分\n            ->⑤CoarseGrainedSchedulerBackend.makeOffers() //case ReviveOffers =>makeOffers()\n        这个模式匹配会调用maksOffers方法\n                ->⑥launchTasks()调用launchTask向Executor提交task\n                    ->⑦ executorData.executorActor ! LaunchTask(new SerializableBuffer(serializedTask))向executor发送序列化好的task，发送一个Task\n```\n\n步骤一、二中主要将这组\n任务的 TaskSet 加入到一个 TaskSetManager 中。TaskSetManager 会根据数据就近原则为 task 分配计算资源，监控 task 的执行状态等，比如失败重试，推测执行等。 \n步骤三、四逻辑较为简单。 \n步骤五为每个 task 具体分配资源，它的输入是一个 Executor 的列表，输出是 TaskDescription 的二维数组。TaskDescription 包含了 TaskID, Executor ID 和 task 执行的依赖信息等。 \n步骤六、七就是将任务真正的发送到 executor 中执行了，并等待 executor 的状态返回。\n\n​                 \n\n","slug":"Spark-on-Yarn源码解析(四)Spark业务代码的执行及其任务分配调度stage划分","published":1,"updated":"2018-09-04T10:16:15.370Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wk00362tpb09u3s2qy"},{"title":"SparkRDD介绍","date":"2018-08-15T16:49:49.734Z","toc":true,"_content":"\n[TOC]\n\n```scala\nsc.textfile(\"hdfs://master:9000/wc\").flatMap(_.split(\"分隔符\")).map((_,1)).reduceByKey(_+_).saveAsTextFile(\"hdfs://master:9000/wcResult\")\n```\n\n<!-- more -->\n\n当rdd形成过程中，worker的分区中只是预留了存放数据的位置，只有当action触发的时候，worker的分区中才会存在数据，sparkSubmit submit的命令行默认的是driver ，RDD的创建都是在在driver上创建的 \n\n\n\n\n\n# spark的分区与hdfs数据块的关系\n\nPartitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。\n\n# SparkRDD\n\nRDD（ResilientDistributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。\n\n![](https://ws1.sinaimg.cn/large/0069RVTdgy1fuawo5mvk1j31c20bq3zz.jpg)\n\n1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。\n\n2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。\n\n3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。\n\n 4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。\n\n 5）一个列表，存储存取每个Partition的优先位置（preferredlocation）。对于一个HDFS文件来说，**这个列表保存的就是每个****Partition所在的块的位置**。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 血缘依赖\n\nRDD 5个特性 \n一个function作用一个partition \n如果是key-value格式的有一个默认的partitioner 默认是hashpartitioner \n如果是从hdfs这种文件系统类型读取的数据，会有一个prefered location，因为在大数据领域宁愿移动计算，也不愿移动数据，通常叫做数据本地化， \n\n# RDD数据读取\n\nrdd向hdfs中读取数据是一行一行读取放在迭代器里面，而不是一下子全部读取数据\n\nrdd向hdfs中读取数据，hdfs文件有几个数据块就会创建几个分区 \n\n读取数据还是用的hadoop的inputFormat来读取的\n\n# RDD的生成方式\n\n\n\n## RDD算子\n\n### Transformation\n\nRDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。\n\n常用的Transformation：\n\n| **转换**                                   | **含义**                                   |\n| ---------------------------------------- | ---------------------------------------- |\n| **map**(func)                            | 返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成      |\n| **filter**(func)                         | 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成 |\n| **flatMap**(func)                        | 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素） |\n| **mapPartitions**(func)                  | 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] => Iterator[U] |\n| **mapPartitionsWithIndex**(func)         | 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是  (Int,  Interator[T]) => Iterator[U] |\n| **sample**(withReplacement, fraction, seed) | 根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子 |\n| **union**(otherDataset)                  | 对源RDD和参数RDD求并集后返回一个新的RDD                 |\n| **intersection**(otherDataset)           | 对源RDD和参数RDD求交集后返回一个新的RDD                 |\n| **distinct**([numTasks]))                | 对源RDD进行去重后返回一个新的RDD                      |\n| **groupByKey**([numTasks])               | 在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD |\n| **reduceByKey**(func, [numTasks])        | 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置 |\n| **aggregateByKey**(zeroValue)(seqOp, combOp, [numTasks]) |                                          |\n| **sortByKey**([ascending], [numTasks])   | 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD |\n| **sortBy**(func,[ascending], [numTasks]) | 与sortByKey类似，但是更灵活                       |\n| **join**(otherDataset, [numTasks])       | 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD |\n| **cogroup**(otherDataset, [numTasks])    | 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD |\n| **cartesian**(otherDataset)              | 笛卡尔积                                     |\n| **pipe**(command, [envVars])             |                                          |\n| **coalesce**(numPartitions**)   **       |                                          |\n| **repartition**(numPartitions)           |                                          |\n| **repartitionAndSortWithinPartitions**(partitioner) |                                          |\n\n### Action\n\n| **动作**                                   | **含义**                                   |\n| ---------------------------------------- | ---------------------------------------- |\n| **reduce**(*func*)                       | 通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的      |\n| **collect**()                            | 在驱动程序中，以数组的形式返回数据集的所有元素                  |\n| **count**()                              | 返回RDD的元素个数                               |\n| **first**()                              | 返回RDD的第一个元素（类似于take(1)）                  |\n| **take**(*n*)                            | 返回一个由数据集的前n个元素组成的数组                      |\n| **takeSample**(*withReplacement*,*num*, [*seed*]) | 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子 |\n| **takeOrdered**(*n*, *[ordering]*)       |                                          |\n| **saveAsTextFile**(*path*)               | 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本 |\n| **saveAsSequenceFile**(*path*)           | 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。 |\n| **saveAsObjectFile**(*path*)             |                                          |\n| **countByKey**()                         | 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 |\n\n\n\n# 宽依赖窄依赖区分\n\n![](https://ws3.sinaimg.cn/large/006tNbRwly1fuhtdoo3nvj30gf087jrt.jpg)\n\n## 窄依赖 narrow dependencies\n\n三个小分块是RDD的分区，组合起来的大框是RDD，后面的是子rdd的分区，一个父rdd的分区只对应一个子rdd的分区（类比独生子女） ，一个子可以对应多个父分区（可以类比父母分区）\n\n如map，filter，union等算子都是操作的原来分区里面的数据,操作之后也在原来的分区 \njoin大多数情况下是宽依赖，在一种特殊情况下是窄依赖 (join是针对key value形式的rdd，相同key的会join在一起)\n\n## 宽依赖 wide dependencies\n\n父rdd一个分区会流向多个子rdd的分区类比多子女情况\n\ngroupBy ，reduceByKey ，join等\n\n下图b到g不是一个stage是因为，提前已经分好组，所以是窄依赖，没有stage \n![](https://ws3.sinaimg.cn/large/006tNbRwly1fuhu3makagj30af05rmxa.jpg)\n\n### Lineage\n\nRDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。\n\n\n\n##  RDD的缓存\n\nSpark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。\n\n### 缓存方式\n\nRDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。\n\n\n\n通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在object StorageLevel中定义的。\n\n缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。\n\n\n\n\n\n\n\n# RDD缓存\n\nval rdd = sc.textFile(\"hdfs:hadoop1:9000/yao\").cache(); \ncache是trancsformation也是懒加载，遇到action 如count.collect才会，缓存到内存里面,而不是文件系统中读取 \ncache()调用的persist()\n\nrdd.unpersist() 就会将内存中的缓存释放掉 \nrdd.unpersist(true)\n\n\n\n# CheckPoint的背景\n\ncheckpoint属于transaction\n\n云计算一边要将中间结果进行产生多个RDD和多次运算，特别是机器学习，需要中间结果计算很多很多次迭代，有可能上百次 \n这样就需要将中间RDD结果保存下来，这就是我们的checkpoint，一般保存在高可用中，比如hdfs就是高可用的。 \n\n只有rdd才能checkPoint \n缓存cache到内存中，直接到内存中拿 \ncheckPoint是到hdfs\n\n# CheckPoint命令\n\n**设定目录，创建目录** ，\n必须指定缓存到哪个目录 \n\n```scala\nval rdd = sc.setCheckpointDir(\"hdfs://master:9000/ckpoint\")\nval rdd = sc.textFile(\"hdfs://master:9000/yao\")\nrdd.checkpoint\nrdd.count\n```\n\n会触发**两个**任务，一个任务计算，一个任务写入到ck指定的hdfs目录 \n\n为减小持久化的数据量，最好将RDD过滤出有节点意义的数据再进行ck操作，直接ck会把文件记录起来到hdfs中 ，但是count产生的数据不能ck，因为返回的是Long类型的，单数据类型的数据不能checkpoint\n\n\n在ck操作以后，RDD和数据的关联都取消了，ck成功以后，数据直接从ckpoint里面读取即可，由于ck属于transaction故ck必须在触发action之前执行 \n\n> 如果把RDD缓存到内存（即在ck之前有cache rdd到内存的操作）就不会另起一个作业一步一步从原始数据运行，然后再ck到hdfs目录，而是直接从内存中读取数据\n\n\n\n\n\n# 广播变量\n\n为了提高效率，比如mapreduce 使用join。当map段所需要的数据量不是很大，避免网络浪费，使用mapAsJoin把规则加入map端内存当中，这样mapreduce在map端可以直接在缓存中拿到规则，这样可以提高效率。广播变量的原理也是如此 \n\n```scala\nval bd = sc.broadcast(ruleArray)\n```\n\n广播出去 广播之后所有的executer都能收到，而且是相当于在每个executor中都存有这一小部分数据，不用通过网络传输，提高效率\n\n在rdd中拿到广播中的数据， \n\n```scala\nval arr  = bd.value \n//将数据展示\narr.toBuffer \n```\n\n","source":"_posts/SparkRDD介绍.md","raw":"---\ntitle: SparkRDD介绍\ndate: 2018年08月06日 22时15分52秒\ntags: [Spark,原理,RDD]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n```scala\nsc.textfile(\"hdfs://master:9000/wc\").flatMap(_.split(\"分隔符\")).map((_,1)).reduceByKey(_+_).saveAsTextFile(\"hdfs://master:9000/wcResult\")\n```\n\n<!-- more -->\n\n当rdd形成过程中，worker的分区中只是预留了存放数据的位置，只有当action触发的时候，worker的分区中才会存在数据，sparkSubmit submit的命令行默认的是driver ，RDD的创建都是在在driver上创建的 \n\n\n\n\n\n# spark的分区与hdfs数据块的关系\n\nPartitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。\n\n# SparkRDD\n\nRDD（ResilientDistributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。\n\n![](https://ws1.sinaimg.cn/large/0069RVTdgy1fuawo5mvk1j31c20bq3zz.jpg)\n\n1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。\n\n2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。\n\n3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。\n\n 4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。\n\n 5）一个列表，存储存取每个Partition的优先位置（preferredlocation）。对于一个HDFS文件来说，**这个列表保存的就是每个****Partition所在的块的位置**。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 血缘依赖\n\nRDD 5个特性 \n一个function作用一个partition \n如果是key-value格式的有一个默认的partitioner 默认是hashpartitioner \n如果是从hdfs这种文件系统类型读取的数据，会有一个prefered location，因为在大数据领域宁愿移动计算，也不愿移动数据，通常叫做数据本地化， \n\n# RDD数据读取\n\nrdd向hdfs中读取数据是一行一行读取放在迭代器里面，而不是一下子全部读取数据\n\nrdd向hdfs中读取数据，hdfs文件有几个数据块就会创建几个分区 \n\n读取数据还是用的hadoop的inputFormat来读取的\n\n# RDD的生成方式\n\n\n\n## RDD算子\n\n### Transformation\n\nRDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。\n\n常用的Transformation：\n\n| **转换**                                   | **含义**                                   |\n| ---------------------------------------- | ---------------------------------------- |\n| **map**(func)                            | 返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成      |\n| **filter**(func)                         | 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成 |\n| **flatMap**(func)                        | 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素） |\n| **mapPartitions**(func)                  | 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] => Iterator[U] |\n| **mapPartitionsWithIndex**(func)         | 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是  (Int,  Interator[T]) => Iterator[U] |\n| **sample**(withReplacement, fraction, seed) | 根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子 |\n| **union**(otherDataset)                  | 对源RDD和参数RDD求并集后返回一个新的RDD                 |\n| **intersection**(otherDataset)           | 对源RDD和参数RDD求交集后返回一个新的RDD                 |\n| **distinct**([numTasks]))                | 对源RDD进行去重后返回一个新的RDD                      |\n| **groupByKey**([numTasks])               | 在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD |\n| **reduceByKey**(func, [numTasks])        | 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置 |\n| **aggregateByKey**(zeroValue)(seqOp, combOp, [numTasks]) |                                          |\n| **sortByKey**([ascending], [numTasks])   | 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD |\n| **sortBy**(func,[ascending], [numTasks]) | 与sortByKey类似，但是更灵活                       |\n| **join**(otherDataset, [numTasks])       | 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD |\n| **cogroup**(otherDataset, [numTasks])    | 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<V>,Iterable<W>))类型的RDD |\n| **cartesian**(otherDataset)              | 笛卡尔积                                     |\n| **pipe**(command, [envVars])             |                                          |\n| **coalesce**(numPartitions**)   **       |                                          |\n| **repartition**(numPartitions)           |                                          |\n| **repartitionAndSortWithinPartitions**(partitioner) |                                          |\n\n### Action\n\n| **动作**                                   | **含义**                                   |\n| ---------------------------------------- | ---------------------------------------- |\n| **reduce**(*func*)                       | 通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的      |\n| **collect**()                            | 在驱动程序中，以数组的形式返回数据集的所有元素                  |\n| **count**()                              | 返回RDD的元素个数                               |\n| **first**()                              | 返回RDD的第一个元素（类似于take(1)）                  |\n| **take**(*n*)                            | 返回一个由数据集的前n个元素组成的数组                      |\n| **takeSample**(*withReplacement*,*num*, [*seed*]) | 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子 |\n| **takeOrdered**(*n*, *[ordering]*)       |                                          |\n| **saveAsTextFile**(*path*)               | 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本 |\n| **saveAsSequenceFile**(*path*)           | 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。 |\n| **saveAsObjectFile**(*path*)             |                                          |\n| **countByKey**()                         | 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 |\n\n\n\n# 宽依赖窄依赖区分\n\n![](https://ws3.sinaimg.cn/large/006tNbRwly1fuhtdoo3nvj30gf087jrt.jpg)\n\n## 窄依赖 narrow dependencies\n\n三个小分块是RDD的分区，组合起来的大框是RDD，后面的是子rdd的分区，一个父rdd的分区只对应一个子rdd的分区（类比独生子女） ，一个子可以对应多个父分区（可以类比父母分区）\n\n如map，filter，union等算子都是操作的原来分区里面的数据,操作之后也在原来的分区 \njoin大多数情况下是宽依赖，在一种特殊情况下是窄依赖 (join是针对key value形式的rdd，相同key的会join在一起)\n\n## 宽依赖 wide dependencies\n\n父rdd一个分区会流向多个子rdd的分区类比多子女情况\n\ngroupBy ，reduceByKey ，join等\n\n下图b到g不是一个stage是因为，提前已经分好组，所以是窄依赖，没有stage \n![](https://ws3.sinaimg.cn/large/006tNbRwly1fuhu3makagj30af05rmxa.jpg)\n\n### Lineage\n\nRDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。\n\n\n\n##  RDD的缓存\n\nSpark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。\n\n### 缓存方式\n\nRDD通过persist方法或cache方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。\n\n\n\n通过查看源码发现cache最终也是调用了persist方法，默认的存储级别都是仅在内存存储一份，Spark的存储级别还有好多种，存储级别在object StorageLevel中定义的。\n\n缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。\n\n\n\n\n\n\n\n# RDD缓存\n\nval rdd = sc.textFile(\"hdfs:hadoop1:9000/yao\").cache(); \ncache是trancsformation也是懒加载，遇到action 如count.collect才会，缓存到内存里面,而不是文件系统中读取 \ncache()调用的persist()\n\nrdd.unpersist() 就会将内存中的缓存释放掉 \nrdd.unpersist(true)\n\n\n\n# CheckPoint的背景\n\ncheckpoint属于transaction\n\n云计算一边要将中间结果进行产生多个RDD和多次运算，特别是机器学习，需要中间结果计算很多很多次迭代，有可能上百次 \n这样就需要将中间RDD结果保存下来，这就是我们的checkpoint，一般保存在高可用中，比如hdfs就是高可用的。 \n\n只有rdd才能checkPoint \n缓存cache到内存中，直接到内存中拿 \ncheckPoint是到hdfs\n\n# CheckPoint命令\n\n**设定目录，创建目录** ，\n必须指定缓存到哪个目录 \n\n```scala\nval rdd = sc.setCheckpointDir(\"hdfs://master:9000/ckpoint\")\nval rdd = sc.textFile(\"hdfs://master:9000/yao\")\nrdd.checkpoint\nrdd.count\n```\n\n会触发**两个**任务，一个任务计算，一个任务写入到ck指定的hdfs目录 \n\n为减小持久化的数据量，最好将RDD过滤出有节点意义的数据再进行ck操作，直接ck会把文件记录起来到hdfs中 ，但是count产生的数据不能ck，因为返回的是Long类型的，单数据类型的数据不能checkpoint\n\n\n在ck操作以后，RDD和数据的关联都取消了，ck成功以后，数据直接从ckpoint里面读取即可，由于ck属于transaction故ck必须在触发action之前执行 \n\n> 如果把RDD缓存到内存（即在ck之前有cache rdd到内存的操作）就不会另起一个作业一步一步从原始数据运行，然后再ck到hdfs目录，而是直接从内存中读取数据\n\n\n\n\n\n# 广播变量\n\n为了提高效率，比如mapreduce 使用join。当map段所需要的数据量不是很大，避免网络浪费，使用mapAsJoin把规则加入map端内存当中，这样mapreduce在map端可以直接在缓存中拿到规则，这样可以提高效率。广播变量的原理也是如此 \n\n```scala\nval bd = sc.broadcast(ruleArray)\n```\n\n广播出去 广播之后所有的executer都能收到，而且是相当于在每个executor中都存有这一小部分数据，不用通过网络传输，提高效率\n\n在rdd中拿到广播中的数据， \n\n```scala\nval arr  = bd.value \n//将数据展示\narr.toBuffer \n```\n\n","slug":"SparkRDD介绍","published":1,"updated":"2018-08-24T01:34:10.728Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wl003a2tpbcpxo6pm5"},{"title":"SparkSQL介绍","date":"2018-08-15T18:04:32.317Z","toc":true,"_content":"\n[TOC]\n\nHive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！\n\n<!-- more -->\n\n> 需要将hive-site.xml拷到spark的配置文件夹\n\n#### hive只认latin1编码\n\nlinux下的mysql编码是latin1 \nwindows下的也设置成latin1.如果要和hive搭配使用的话\n\n#### 进入sparksql和hive连接的命令\n\n```bash\n/home/bigdata/apps/spark/bin/spark-sql --master spark://bigdata1:7077 --driver-class-path /home/bigdata/apps/hive/lib/mysql-connector-java-5.1.31-bin.jar\n```\n\n需要注意的是，需要是集群模式，--master 等等，还要指定一个jdbc的连接驱动 \nsparksql也会走hive的元数据库\n\n**hive语法**在spark-sql下 \n\n```bash\n'>create table person(id bigint,name string,age int) row format delimited fields terminated by ',';\n```\n\n在hive下： \n\n```bash\nload data inpath \"hdfs://master:9000/person.txt\" into table person;\n```\n\n","source":"_posts/SparkSQL介绍.md","raw":"---\ntitle: SparkSQL介绍\ndate: 2018年08月06日 22时15分52秒\ntags: [Spark,SparkSQL]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\nHive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！\n\n<!-- more -->\n\n> 需要将hive-site.xml拷到spark的配置文件夹\n\n#### hive只认latin1编码\n\nlinux下的mysql编码是latin1 \nwindows下的也设置成latin1.如果要和hive搭配使用的话\n\n#### 进入sparksql和hive连接的命令\n\n```bash\n/home/bigdata/apps/spark/bin/spark-sql --master spark://bigdata1:7077 --driver-class-path /home/bigdata/apps/hive/lib/mysql-connector-java-5.1.31-bin.jar\n```\n\n需要注意的是，需要是集群模式，--master 等等，还要指定一个jdbc的连接驱动 \nsparksql也会走hive的元数据库\n\n**hive语法**在spark-sql下 \n\n```bash\n'>create table person(id bigint,name string,age int) row format delimited fields terminated by ',';\n```\n\n在hive下： \n\n```bash\nload data inpath \"hdfs://master:9000/person.txt\" into table person;\n```\n\n","slug":"SparkSQL介绍","published":1,"updated":"2018-08-20T01:38:13.848Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wm003d2tpb69p3x7d8"},{"title":"SparkStreaming介绍","date":"2018-08-15T17:50:47.842Z","toc":true,"_content":"\n[TOC]\n\n大数据领域，分为离线计算和实时计算 \n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuaxkz7halj30i6057dfw.jpg)\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuaxl270fuj30kj034jrd.jpg)\n\n<!-- more -->\n\n## Streaming和Storm比较\n\n在时效性上比storm弱，在吞吐量上比storm大 \nstreaming需要设置时间间隔，设置多长时间产生一个批次记录到streaming放在RDD里面 \n比如设置5s，每隔5s就会产生一个RDD \nRDD需要是有序的\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuax1z5n2wj30ne0ifgou.jpg)\n\n","source":"_posts/SparkStreaming介绍.md","raw":"---\ntitle: SparkStreaming介绍\ndate: 2018年08月06日 22时15分52秒\ntags: [Spark,SparkStreaming,原理]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n大数据领域，分为离线计算和实时计算 \n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuaxkz7halj30i6057dfw.jpg)\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuaxl270fuj30kj034jrd.jpg)\n\n<!-- more -->\n\n## Streaming和Storm比较\n\n在时效性上比storm弱，在吞吐量上比storm大 \nstreaming需要设置时间间隔，设置多长时间产生一个批次记录到streaming放在RDD里面 \n比如设置5s，每隔5s就会产生一个RDD \nRDD需要是有序的\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuax1z5n2wj30ne0ifgou.jpg)\n\n","slug":"SparkStreaming介绍","published":1,"updated":"2018-08-15T18:09:40.373Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wo003i2tpb7kj9sjz5"},{"title":"SparkStreaming消费Kafka数据","date":"2018-08-15T15:53:30.330Z","toc":true,"_content":"\n[TOC]\n\nStreaming消费Kafka有两种方式\n\n#### 1、reciver方式\n\n根据时间来划分批次，缺点：有可能一个时间段会出现数据爆炸，有保存log到hdfs机制，但消耗大（zk来管理偏移量）\n\n#### 2、direct方式 1.3.6后推出\n\nexecutor和kafka的partition是一一对应的（是rdd的分区和kafka对应，如果一个executor的rdd有多个分区，那么一个executor可以对应多个partition）必须自己来管理偏移量，最好把偏移量写在zk或者其他第三方介质里面\n\n<!-- more -->\n\n# 非直连方式 receive方式\n\n \nspark cluster先与kafka集群建立连接 \n在每一个executor中创建一个Reciver \n当executor启动kafka启动后，reciver会一直消费kafka中的数据，一直拉，直连是每隔一个时间段去拉取 \n这种方式是zookeeper来管理数据的偏移量 \n问题:在时间间隔中，executor接收的数据超过了Executor的内存数，会造成数据的丢失 \n为了防止数据丢失，可以做checkpoint，或者是记录日志 \n可以多个reciver，一个reciver可以指定多个线程 \n**可以读读文章 Kafka Intergration Guide 官网链接**\n\n# 直连Direct Approach (No Receives)\n\n\nB-*代表broker，每一个broker中有三个分区（假设），但是每个broker里面只有一个分区是活着的 \n直连是每隔一个时间段去拉取 \n对消费数据的位置保证 \n会定期实时查询kafka topic+partition的偏移量，会根据偏移量的范围来处理每一个批次， executor不会接收超出接收范围的数据，而是记录下偏移量，下次接着拉取 \n一个kafka的partiton对应rdd的一个partition\n\n问题：一个partition只有一个Executor连接上，不能并行读去数据。 \n解决办法，可以repartition，分散到多个partition上去读取。 \n怎样让executor读取kafka分区里面的数据的速度快？ \n将boker的分区数创建成和worker的数目一样，也就是executor的数目一样，一个executor消费一个分区，这样数据读取比较快。并行读取数据，也可以控制读取的速度。 \n这样需要自己管理偏移量，以前的方式是zk管理偏移量 \n最好是将偏移量保存到zk里面，不过是自己控制的，防止将偏移量保存到本地宕机无法恢复。 \n**需要读博客 Spark streaming整合kafka** \n官方文档显示，一个数据保证只会消费一次，不会重复消费，更加高效 \n简化并行，一对一消费，高效，没有reciver作为消费者","source":"_posts/SparkStreaming消费Kafka数据.md","raw":"---\ntitle: SparkStreaming消费Kafka数据\ndate: 2018年08月06日 22时15分52秒\ntags: [Spark,SparkStreaming]\ncategories:\ntoc: true\n---\n\n[TOC]\n\nStreaming消费Kafka有两种方式\n\n#### 1、reciver方式\n\n根据时间来划分批次，缺点：有可能一个时间段会出现数据爆炸，有保存log到hdfs机制，但消耗大（zk来管理偏移量）\n\n#### 2、direct方式 1.3.6后推出\n\nexecutor和kafka的partition是一一对应的（是rdd的分区和kafka对应，如果一个executor的rdd有多个分区，那么一个executor可以对应多个partition）必须自己来管理偏移量，最好把偏移量写在zk或者其他第三方介质里面\n\n<!-- more -->\n\n# 非直连方式 receive方式\n\n \nspark cluster先与kafka集群建立连接 \n在每一个executor中创建一个Reciver \n当executor启动kafka启动后，reciver会一直消费kafka中的数据，一直拉，直连是每隔一个时间段去拉取 \n这种方式是zookeeper来管理数据的偏移量 \n问题:在时间间隔中，executor接收的数据超过了Executor的内存数，会造成数据的丢失 \n为了防止数据丢失，可以做checkpoint，或者是记录日志 \n可以多个reciver，一个reciver可以指定多个线程 \n**可以读读文章 Kafka Intergration Guide 官网链接**\n\n# 直连Direct Approach (No Receives)\n\n\nB-*代表broker，每一个broker中有三个分区（假设），但是每个broker里面只有一个分区是活着的 \n直连是每隔一个时间段去拉取 \n对消费数据的位置保证 \n会定期实时查询kafka topic+partition的偏移量，会根据偏移量的范围来处理每一个批次， executor不会接收超出接收范围的数据，而是记录下偏移量，下次接着拉取 \n一个kafka的partiton对应rdd的一个partition\n\n问题：一个partition只有一个Executor连接上，不能并行读去数据。 \n解决办法，可以repartition，分散到多个partition上去读取。 \n怎样让executor读取kafka分区里面的数据的速度快？ \n将boker的分区数创建成和worker的数目一样，也就是executor的数目一样，一个executor消费一个分区，这样数据读取比较快。并行读取数据，也可以控制读取的速度。 \n这样需要自己管理偏移量，以前的方式是zk管理偏移量 \n最好是将偏移量保存到zk里面，不过是自己控制的，防止将偏移量保存到本地宕机无法恢复。 \n**需要读博客 Spark streaming整合kafka** \n官方文档显示，一个数据保证只会消费一次，不会重复消费，更加高效 \n简化并行，一对一消费，高效，没有reciver作为消费者","slug":"SparkStreaming消费Kafka数据","published":1,"updated":"2018-08-20T01:39:18.490Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wp003l2tpbrd765l15"},{"title":"Spark启动流程及一些小总结","date":"2018-08-15T15:54:20.258Z","toc":true,"_content":"\n[TOC]\n\n# spark的架构模型\n\n![](https://ws3.sinaimg.cn/large/006tNbRwly1fuhrnmk1skj310i0ssgma.jpg)\n\n# 角色功能\n\nDriver：以spark-submit提交程序为例，执行该命令的主机为driver（在任意一台安装了spark（spark submit）的机器上启动一个任务的客户端也就是Driver 。客户端与集群需要建立链接，建立的这个链接对象叫做sparkContext，只有这个对象创建成功才标志这这个客户端与spark集群链接成功。SparkContext是driver进程中的一个对象，提交任务的时候，指定了每台机器需要多少个核cores，需要的内存 ）\n\nMaster: 给任务提供资源，分配资源，master跟worker通信，报活和更新资源 \n\nWorker:  以子进程的方式启动executor\n\nExecutor：Driver提交程序到executor(CoarseGrainedExecutorBankend)，执行任务的进程，exector是task运行的容器\n\n<!-- more -->\n\n## Driver端\n\ndriver创建sparkContext于spark集群建立连接，然后向master申请资源，master来调度决定向worker的哪台机器上执行 ，在合适的worker（资源分配策略）上以子进程的方式启动excutor来执行 （Class.forName()会只在自己的进程里面执行，反射是在同一进程 ，启动java子进程的方法 是另一个进程，debug跳不到子进程中去，进程之间进行方法调用只有rpc才可以 ）此过程只是启动executor，最终任务的执行需要等到action触发，提交程序最终也是提交到executor上，在整个过程中，\n\n\n\n> driver提交一个application时就会在driver主机生成一个sparkSubmit进程来监控任务的执行情况。action触发程序运行后，executor只会通过driverClinet与driver交互，applicationMaster 不负责具体某个任务的进度 ,只负责资源的调度和监控。driver在action的时候才会把任务提交给executor。\n\n## master与worker交互\n\nworker和master以进程方式启动后 ，worker会向master进行注册,worker在启动的时候，会在spark env中指定worker的资源，如果没指定的话，会默认使用机器的所有核数，所有内存-1G的内存，预留1G给操作系统，然后worker会告知master自己拥有的资源，如核数cores，内存等。在之后的过程中，worker会实时向master发送心跳报活。\n\n\n\n## 资源分配策略\n\nspark任务资源分配有两种策略：尽量打散 和尽量集中\n\n以task1需要2core为例：\n\n尽量打散是尽可能的将任务分散到不同的worker来启动exector，机器1分配1core，机器2分配1core\n\n尽量集中是尽可能在更少的机器上来启动worker并启动exector，在分配的机器上尽可能多的分配资源，达到集中在更少的机器上的目的。如在机器1上分配2core（前提是机器1 剩余core数>=2）\n\n\n\n\n\n# RDD整个运行流程\n\n首先rdd产生过后，经过一系列的处理，构成相互依赖，在有宽依赖的情况下会划分stage，构成一个有向无环图也就是DAG，整个rdd的构建，完成在action触发之前，action触发就会将task提交到executor容器中运行\n\n\n\n\n\n## Driver与executor交互\n\n在任务提交的时候，会将driver信息封装，先告诉给master，master再告诉给worker，然后worker启动executor进程的时候，会将信息告诉给executor，故而最终exector能获取到driver的信息\n\n在executor创建成功后，就会和driver建立连接，而不再与Master通信，这样是为了减少Master压力，也不让Master成为性能瓶颈。\n\n由于executor有可能在很多台机器上启动，driver无法知晓在哪台机器上启动有exector，所以需要executor和driver进行rpc通信（netty或者akka）主动建立连接。建立通信后exector向driver汇报状态及其执行进度，driver向executor提交计算任务，然后在executor中执行，只有当RDD触发action的时候，driver才将taskset以stage的形式提交任务给executor，任务的最细粒度是task\n\n然后executor就跟driver进行通信，Rdd执行逻辑的时候就不再通过Master，这是为了减少Master压力，也不让Master成为性能瓶颈。\n\n\n\n# RDD的构建详细阶段\n\n\n\n![](https://ws2.sinaimg.cn/large/006tNbRwly1fuhtcjutpej30e007hmxg.jpg)\n\n\n\n## 第一阶段 rdd创建\n\nrdd创建都是在driver中执行，只有运行时有数据流向rdd才会提交到executor。如hdfs中读取数据，会产生很多的RDD….，RDD之间存在着依赖关系， RDD之间的**转换**都是由transformation产生的（故transformation返回对象为RDD），**在action触发后**，DAG就确定了RDD就形成了数据的流向hdfs->RDD1->RDD2->RDD3....->RDDn\n\n## 第二阶段 DAGScheduler\n\nstage的划分，也就是DAGScheduler的执行\n\nstage切分依据：有宽依赖（后面对宽窄依赖有说明）就会切分，更明确的说就是有shuffle的时候切分\n\n宽依赖：遇到shuffle就是宽依赖 \n窄依赖：没有shuffle就是窄依赖 \n\n把DAG切分成stage，然后以taskSet(流水线task的集合，所有的task业务逻辑都是一样的，只是计算的数据块不同，因为每一个task只计算一个分区，且taskSet集合中的任务会并行操作）\n\ntask由master决定在哪个executor上运行，之后由driver提交task到executor来执行\n\n> 前面的stage先提交，因为stage的划分意味着有shuffle，那么后面stage会依赖前面stage的数据，故此前面stage先提交。\n\n\n\nAction触发的时候，DAG就可以确定了，transformation的调用都是在driver中完成的，一旦调用了action，会提交这个任务。\n\n## 第三阶段 TaskScheduler\n\n**TaskScheduler**\n\n**cluster Manager** ：就是Master，启动work上的executor \n**stragling tasks**：例子：当100个任务，99都完成了，剩下了一个任务1，会再启动和剩下的一模一样的任务2，任务1和任务2谁先完成，就用谁的结果\n\n\n\n## 第四阶段 任务执行\n\nBlock Manager管理数据\n\n也就是action触发，数据开始流通，driver提交task到worker上的executor，执行真正的业务逻辑\n\n\n\n计算完成之后，若将这些结果数据collect聚合，则会将所有executor的部分结果聚合在一起，比如count、sum的，多个worker中每一个worker只会保存一部分数据，driver保存了所有数据，、都是在driver里面\n\n\n\n# task与读取hdfs数据的关系\n\ntask提交之后，如果向hdfs中拉取数据，driver已经将要获取数据的元数据都已经获取到了，task会和分区数挂钩，在executor上一个task读取一个分区。**task读取数据不是一下将所有数据加载到内存里面，是先构建一个迭代器，拿到一条处理一条**。处理完成之后会将数据保存在executor的内存中，如果内存放不下，就将数据持久化到磁盘上\n\n","source":"_posts/Spark启动流程及一些小总结.md","raw":"---\ntitle: Spark启动流程及一些小总结\ndate: 2018年08月06日 22时15分52秒\ntags: [Spark,原理]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n# spark的架构模型\n\n![](https://ws3.sinaimg.cn/large/006tNbRwly1fuhrnmk1skj310i0ssgma.jpg)\n\n# 角色功能\n\nDriver：以spark-submit提交程序为例，执行该命令的主机为driver（在任意一台安装了spark（spark submit）的机器上启动一个任务的客户端也就是Driver 。客户端与集群需要建立链接，建立的这个链接对象叫做sparkContext，只有这个对象创建成功才标志这这个客户端与spark集群链接成功。SparkContext是driver进程中的一个对象，提交任务的时候，指定了每台机器需要多少个核cores，需要的内存 ）\n\nMaster: 给任务提供资源，分配资源，master跟worker通信，报活和更新资源 \n\nWorker:  以子进程的方式启动executor\n\nExecutor：Driver提交程序到executor(CoarseGrainedExecutorBankend)，执行任务的进程，exector是task运行的容器\n\n<!-- more -->\n\n## Driver端\n\ndriver创建sparkContext于spark集群建立连接，然后向master申请资源，master来调度决定向worker的哪台机器上执行 ，在合适的worker（资源分配策略）上以子进程的方式启动excutor来执行 （Class.forName()会只在自己的进程里面执行，反射是在同一进程 ，启动java子进程的方法 是另一个进程，debug跳不到子进程中去，进程之间进行方法调用只有rpc才可以 ）此过程只是启动executor，最终任务的执行需要等到action触发，提交程序最终也是提交到executor上，在整个过程中，\n\n\n\n> driver提交一个application时就会在driver主机生成一个sparkSubmit进程来监控任务的执行情况。action触发程序运行后，executor只会通过driverClinet与driver交互，applicationMaster 不负责具体某个任务的进度 ,只负责资源的调度和监控。driver在action的时候才会把任务提交给executor。\n\n## master与worker交互\n\nworker和master以进程方式启动后 ，worker会向master进行注册,worker在启动的时候，会在spark env中指定worker的资源，如果没指定的话，会默认使用机器的所有核数，所有内存-1G的内存，预留1G给操作系统，然后worker会告知master自己拥有的资源，如核数cores，内存等。在之后的过程中，worker会实时向master发送心跳报活。\n\n\n\n## 资源分配策略\n\nspark任务资源分配有两种策略：尽量打散 和尽量集中\n\n以task1需要2core为例：\n\n尽量打散是尽可能的将任务分散到不同的worker来启动exector，机器1分配1core，机器2分配1core\n\n尽量集中是尽可能在更少的机器上来启动worker并启动exector，在分配的机器上尽可能多的分配资源，达到集中在更少的机器上的目的。如在机器1上分配2core（前提是机器1 剩余core数>=2）\n\n\n\n\n\n# RDD整个运行流程\n\n首先rdd产生过后，经过一系列的处理，构成相互依赖，在有宽依赖的情况下会划分stage，构成一个有向无环图也就是DAG，整个rdd的构建，完成在action触发之前，action触发就会将task提交到executor容器中运行\n\n\n\n\n\n## Driver与executor交互\n\n在任务提交的时候，会将driver信息封装，先告诉给master，master再告诉给worker，然后worker启动executor进程的时候，会将信息告诉给executor，故而最终exector能获取到driver的信息\n\n在executor创建成功后，就会和driver建立连接，而不再与Master通信，这样是为了减少Master压力，也不让Master成为性能瓶颈。\n\n由于executor有可能在很多台机器上启动，driver无法知晓在哪台机器上启动有exector，所以需要executor和driver进行rpc通信（netty或者akka）主动建立连接。建立通信后exector向driver汇报状态及其执行进度，driver向executor提交计算任务，然后在executor中执行，只有当RDD触发action的时候，driver才将taskset以stage的形式提交任务给executor，任务的最细粒度是task\n\n然后executor就跟driver进行通信，Rdd执行逻辑的时候就不再通过Master，这是为了减少Master压力，也不让Master成为性能瓶颈。\n\n\n\n# RDD的构建详细阶段\n\n\n\n![](https://ws2.sinaimg.cn/large/006tNbRwly1fuhtcjutpej30e007hmxg.jpg)\n\n\n\n## 第一阶段 rdd创建\n\nrdd创建都是在driver中执行，只有运行时有数据流向rdd才会提交到executor。如hdfs中读取数据，会产生很多的RDD….，RDD之间存在着依赖关系， RDD之间的**转换**都是由transformation产生的（故transformation返回对象为RDD），**在action触发后**，DAG就确定了RDD就形成了数据的流向hdfs->RDD1->RDD2->RDD3....->RDDn\n\n## 第二阶段 DAGScheduler\n\nstage的划分，也就是DAGScheduler的执行\n\nstage切分依据：有宽依赖（后面对宽窄依赖有说明）就会切分，更明确的说就是有shuffle的时候切分\n\n宽依赖：遇到shuffle就是宽依赖 \n窄依赖：没有shuffle就是窄依赖 \n\n把DAG切分成stage，然后以taskSet(流水线task的集合，所有的task业务逻辑都是一样的，只是计算的数据块不同，因为每一个task只计算一个分区，且taskSet集合中的任务会并行操作）\n\ntask由master决定在哪个executor上运行，之后由driver提交task到executor来执行\n\n> 前面的stage先提交，因为stage的划分意味着有shuffle，那么后面stage会依赖前面stage的数据，故此前面stage先提交。\n\n\n\nAction触发的时候，DAG就可以确定了，transformation的调用都是在driver中完成的，一旦调用了action，会提交这个任务。\n\n## 第三阶段 TaskScheduler\n\n**TaskScheduler**\n\n**cluster Manager** ：就是Master，启动work上的executor \n**stragling tasks**：例子：当100个任务，99都完成了，剩下了一个任务1，会再启动和剩下的一模一样的任务2，任务1和任务2谁先完成，就用谁的结果\n\n\n\n## 第四阶段 任务执行\n\nBlock Manager管理数据\n\n也就是action触发，数据开始流通，driver提交task到worker上的executor，执行真正的业务逻辑\n\n\n\n计算完成之后，若将这些结果数据collect聚合，则会将所有executor的部分结果聚合在一起，比如count、sum的，多个worker中每一个worker只会保存一部分数据，driver保存了所有数据，、都是在driver里面\n\n\n\n# task与读取hdfs数据的关系\n\ntask提交之后，如果向hdfs中拉取数据，driver已经将要获取数据的元数据都已经获取到了，task会和分区数挂钩，在executor上一个task读取一个分区。**task读取数据不是一下将所有数据加载到内存里面，是先构建一个迭代器，拿到一条处理一条**。处理完成之后会将数据保存在executor的内存中，如果内存放不下，就将数据持久化到磁盘上\n\n","slug":"Spark启动流程及一些小总结","published":1,"updated":"2018-08-23T16:50:45.753Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wr003q2tpbr0d1n4u2"},{"title":"Spark本地调试远程集群程序","date":"2018-08-07T08:09:43.314Z","toc":true,"_content":"\n\n\n[TOC]\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu562ur7pxj317o0s2qfq.jpg)\n\n由于在生产环境中进行调试spark程序需要进行打包和各种跳板机跳转，最好在本地搭一套集群来进行一些代码基础检查。\n\n<!-- more-->\n\n需要将 本地集群中**hdfs-site.xml core.site.xml **拷贝到本地工程的resource文件夹下，这样会应用这些配置，注意要在提交到生产环境的时候，替换成对应环境的配置文件\n\n```scala\nval array = Array(\n      (\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\"),\n      (\"spark.storage.memoryFraction\", \"0.3\"),\n      (\"spark.memory.useLegacyMode\", \"true\"),\n      (\"spark.shuffle.memoryFraction\", \"0.6\"),\n      (\"spark.shuffle.file.buffer\", \"128k\"),\n      (\"spark.reducer.maxSizeInFlight\", \"96m\"),\n      (\"spark.sql.shuffle.partitions\", \"500\"),\n      (\"spark.default.parallelism\", \"180\"),\n      (\"spark.dynamicAllocation.enabled\", \"false\")\n    )\n    val conf = new SparkConf().setAll(array)\n      .setJars(Array(\"your.jar\"))\n    val sparkSession: SparkSession = SparkSession\n      .builder\n      .appName(applicationName)\n      .enableHiveSupport()\n      .master(\"spark://master:7077\")\n      .config(conf)\n      .getOrCreate()\n    val sqlContext = sparkSession.sqlContext\n    val sparkContext: SparkContext = sparkSession.sparkContext\n```\n\n","source":"_posts/Spark本地调试远程集群程序.md","raw":"---\ntitle:  Spark本地调试远程集群程序\ndate: 2018年06月21日 22时15分52秒\ntags: [Spark]\ncategories: 大数据\ntoc: true\n---\n\n\n\n[TOC]\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu562ur7pxj317o0s2qfq.jpg)\n\n由于在生产环境中进行调试spark程序需要进行打包和各种跳板机跳转，最好在本地搭一套集群来进行一些代码基础检查。\n\n<!-- more-->\n\n需要将 本地集群中**hdfs-site.xml core.site.xml **拷贝到本地工程的resource文件夹下，这样会应用这些配置，注意要在提交到生产环境的时候，替换成对应环境的配置文件\n\n```scala\nval array = Array(\n      (\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\"),\n      (\"spark.storage.memoryFraction\", \"0.3\"),\n      (\"spark.memory.useLegacyMode\", \"true\"),\n      (\"spark.shuffle.memoryFraction\", \"0.6\"),\n      (\"spark.shuffle.file.buffer\", \"128k\"),\n      (\"spark.reducer.maxSizeInFlight\", \"96m\"),\n      (\"spark.sql.shuffle.partitions\", \"500\"),\n      (\"spark.default.parallelism\", \"180\"),\n      (\"spark.dynamicAllocation.enabled\", \"false\")\n    )\n    val conf = new SparkConf().setAll(array)\n      .setJars(Array(\"your.jar\"))\n    val sparkSession: SparkSession = SparkSession\n      .builder\n      .appName(applicationName)\n      .enableHiveSupport()\n      .master(\"spark://master:7077\")\n      .config(conf)\n      .getOrCreate()\n    val sqlContext = sparkSession.sqlContext\n    val sparkContext: SparkContext = sparkSession.sparkContext\n```\n\n","slug":"Spark本地调试远程集群程序","published":1,"updated":"2018-08-10T18:29:43.521Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wt003t2tpbvbwe0ob7"},{"title":"Spark算子案例","date":"2018-08-15T15:52:40.133Z","toc":true,"_content":"\n[TOC]\n\n# HelloWord？WorldCount\n\n```scala\nsc.textfile(\"hdfs://master:9000/wc\").flatMap(_.split(\"分隔符\")).map((_,1)).reduceByKey(_+_).saveAsTextFile(\"hdfs://master:9000/wcResult\")\n```\n\n\n数据最开始在Driver，计算的时候数据会流入worker\n当rdd形成过程中，worker的分区中只是预留了存放数据的位置，只有当action触发的时候，worker的分区中才会存在数据\n\nSpark的运算都是通过算子进行RDD的转换及运算，那我们对算子进行简单熟悉[参考RDD算子实例](http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html)\n\n<!-- more -->\n\n\n\n\n\nreduceByKey先进行一下combineer 移动计算\n\ngroupByKey不好\n\nreduceByKey会在局部先进行一下求和\n\ngroupByKey是会将所有的数据放在一个大集合里面，然后再求和 ，会消耗更多的网络带宽，不符合计算本地化\n\n\n\n\n\n一下一些RDD是给予rdd1来操作的\n\n```scala\nval rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)\n```\n\n\n\n## mapPartitions\n\nmap 是对 rdd 中的每一个元素进行操作，而 mapPartitions(foreachPartition) 则是对 rdd 中的每个分区的迭代器进行操作。如果在 map 过程中需要频繁创建额外的对象 (例如将 rdd 中的数据通过 jdbc 写入数据库, map 需要为每个元素创建一个链接而 mapPartition 为每个 partition 创建一个链接), 则 mapPartitions 效率比 map 高的多。\n\nSparkSql 或 DataFrame 默认会对程序进行 mapPartition 的优化。\n\n\n\n## mapPartitionsWithIndex\n\nmapPartitionWithIndex与mapPartition类似，只是会带上分区的序号\n\n把每个partition中的**分区号和对应的值**拿出来, 源码中方法的形式：\n\n```scala\nval func(index,Int,iter:Interator[(Int)]):Interator[String] = {\niter.toList.map(x => \"[partID:\" +  index + \", val: \" + x + \"]\").iterator\n}\n```\n\n会转换成函数 \n函数的形式\n\n```scala\nval func = (index: Int, iter: Iterator[(Int)]) => {\n  iter.toList.map(x => \"[partID:\" +  index + \", val: \" + x + \"]\").iterator\n}\nrdd1.mapPartitionsWithIndex(func).collect\n```\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fucjpo4afaj31ik056whp.jpg)\n\n## aggregate (action)\n\naggregate是一个action操作\n\n源码定义\n\n```scala\ndef aggregate[U](zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): U\n```\n\neqOp 操作会聚合各分区中的元素，然后 combOp 操作把所有分区的聚合结果再次聚合，两个操作的初始值都是 zeroValue.   seqOp 的操作是遍历分区中的所有元素 (T)，第一个 T 跟 zeroValue 做操作，结果再作为与第二个 T 做操作的 zeroValue，直到遍历完整个分区。combOp 操作是把各分区聚合的结果，再聚合。aggregate 函数返回一个跟 RDD 不同类型的值。因此，需要一个操作 seqOp 来把分区中的元素 T 合并成一个 U，另外一个操作 combOp 把所有 U 聚合。\n\n参考[理解 Spark RDD 中的 aggregate 函数](https://blog.csdn.net/qingyang0320/article/details/51603243)\n\n第一个参数：初始值（在进行操作的时候，会默认带入该值进行） \n第二个参数:   是两个函数[每个函数都是2个参数(第一个函数:先对各个分区进行合并, 第二个函数:对各个分区合并后的结果再进行合并)] \n\n最后得到返回值\n\n\n\n> rdd1为上面的rdd1分区函数的结果\n\n```scala\nrdd1.aggregate(0)(_+_, _+_)\n```\n\n**0 + (0+1+2+3+4 + 0+5+6+7+8+9)**\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fuck6fl2chj30kg02ut96.jpg)\n\n```scala\nrdd1.aggregate(7)(_+_, _+_)\n```\n\n**7 + (7+1+2+3+4 + 7+5+6+7+8+9)**\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fuck87fxhij30ju02gt92.jpg)\n\n\n\n```scala\nrdd1.aggregate(0)(math.max(_, _), _ + _)\n```\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fuck95wu88j30qo02o74x.jpg)\n\n\n\n```Scala\nrdd1.aggregate(5)(math.max(_, _), _ + _)\n```\n\n**5和1比, 得5再和234比得5 --> 5和6789比,得9 --> 5 + (5+9)**\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fucouprhcjj30s402yjs5.jpg)\n\n\n\n\n\n```scala\nval rdd2 = sc.parallelize(List(\"q\",\"w\",\"e\",\"r\",\"t\",\"y\",\"u\",\"i\",\"o\",\"p\"),2)\n```\n\n可以用更加直接的方式验证操作\n\n```Scala\ndef func2(index: Int, iter: Iterator[(String)]) : Iterator[String] = {\n  iter.toList.map(x => \"[partID:\" +  index + \", val: \" + x + \"]\").iterator\n}\n```\n\n```Scala\nrdd2.aggregate(\"\")(_ + _, _ + _)\nrdd2.aggregate(\"=\")(_ + _, _ + _)\nrdd2.aggregate(\"|\")(_ + _, _ + _)\n```\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fucpd6axxej30os034wf7.jpg)\n\n\n\n\n\n```scala\nval rdd3 = sc.parallelize(List(\"qazqqw7\",\"jishhrwe9\",\"sdfwezsddf12\",\"12esdww8\"),2)\nrdd3.aggregate(\"\")((x,y) => math.max(x.length, y.length).toString, (x,y) => x + y)\n```\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fucpvxoentj31gw03276d.jpg)\n\n\n\n```scala\nval rdd4 = sc.parallelize(List(\"qazqqw7\",\"jishhrwe9\",\"sdfwezsddf12\",\"\"),2)\nrdd4.aggregate(\"\")((x,y) => math.min(x.length, y.length).toString, (x,y) => x + y)\n```\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fucpwdi27nj31g8032abq.jpg)\n\n\n\n\n\n## aggregateByKey\n\n对每个分区进行计算\n\n```scala\nval pairRDD = sc.parallelize(List( (\"a\",1), (\"a\", 12), (\"b\", 4),(\"c\", 17), (\"c\", 12), (\"b\", 2)), 2)\ndef func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = {\n  iter.toList.map(x => \"[partID:\" +  index + \", val: \" + x + \"]\").iterator\n}\npairRDD.mapPartitionsWithIndex(func2).collect\n\n```\n\n## combineByKey\n\nreduceByKey aggregateByKey底层都是依赖的combineByKey，combineByKey比较底层的算子 \n和reduceByKey是相同的效果\n\n**combineByKey有三个参数**\n\n第一个参数x: **原封不动取出来**  第二个参数:**是函数, 局部运算**, 第三个:是函数, **对局部运算后的结果再做运算**\n\n\n\n```scala\nval rdd4 = sc.parallelize(List(\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\"),2)\nval rdd5 = sc.parallelize(List(1,1,2,2,2,1,2,2,2),2)\nval rdd6 = rdd5.zip(rdd4)\nval rdd7 = rdd6.combineByKey(List(_),(x:List[String],y:String)=>x:+y,(m:List[String],n:List[String])=>m ++ n)\nrdd7.collect\n```\n\n\n\n## reduceByKey\n\nreduceByKey 用于对每个 key 对应的多个 value 进行 merge 操作，最重要的是它能够在本地先进行 merge 操作，并且 merge 操作可以通过函数自定义。\n\n## groupByKey\n\ngroupByKey 也是对每个 key 进行操作，但只生成一个 sequence。不会再进行\n\n需要特别注意 “Note” 中的话，它告诉我们：如果需要对 sequence 进行 aggregation 操作（注意，groupByKey 本身不能自定义操作函数），那么，选择 reduceByKey/aggregateByKey 更好。这是因为 groupByKey 不能自定义函数，我们需要先用 groupByKey 生成 RDD，然后才能对此 RDD 通过 map 进行自定义函数操作。\n\n\n\n## checkpoint\n\n将rdd内容持久化\n\n```scala\nsc.setCheckpointDir(\"hdfs://master:9000/ck\")\nval rdd = sc.textFile(\"hdfs://master:9000/wc\").flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_+_)\nrdd.checkpoint\nrdd.isCheckpointed\nrdd.count\nrdd.isCheckpointed\nrdd.getCheckpointFile\n\n```\n\n## coalesce, repartition\n\n有时候需要重新设置 Rdd 的分区数量，比如 Rdd 的分区中，Rdd 分区比较多，但是每个 Rdd 的数据量比较小，需要设置一个比较合理的分区。或者需要把 Rdd 的分区数量调大。还有就是通过设置一个 Rdd 的分区来达到设置生成的文件的数量。\n\n如果分区的数量发生激烈的变化，如设置 numPartitions = 1，这可能会造成运行计算的节点比你想象的要少，为了避免这个情况，可以设置 shuffle=true，\n\n那么这会增加 shuffle 操作。\n\n关于这个分区的激烈的变化情况，比如分区数量从父 Rdd 的几千个分区设置成几个，有可能会遇到这么一个错误。\n\n```\njava.io.IOException: Unable to acquire 16777216 bytes of memory\n```\n\n这个错误只要把 shuffle 设置成 true 即可解决。\n\n当把父 Rdd 的分区数量增大时，比如 Rdd 的分区是 100，设置成 1000，如果 shuffle 为 false，并不会起作用。\n\n这时候就需要设置 shuffle 为 true 了，那么 Rdd 将在 shuffle 之后返回一个 1000 个分区的 Rdd，数据分区方式默认是采用 hash partitioner。\n\n最后来看看 repartition() 方法的源码：\n\n\n\n\n\ncoalesce() 方法的作用是返回指定一个新的指定分区的 Rdd。\n\n\n\n```scala\nval rdd1 = sc.parallelize(1 to 10, 10)\nval rdd2 = rdd1.coalesce(2, false)\nrdd2.partitions.length\n\n```\n\n[](https://www.cnblogs.com/fillPv/p/5392186.html)\n\n## collectAsMap \n\n将其他集合保存为map结构\n\n```scala\nval rdd = sc.parallelize(List((\"a\", 1), (\"b\", 2)))\nrdd.collectAsMap\n得到结果\nMap(b -> 2, a -> 1)\n```\n\n\n\n## countByKey\n\n```scala\nval rdd1 = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"b\", 2), (\"c\", 2), (\"c\", 1)))\nrdd1.countByKey \n统计Key出现的次数\n结果 Map(b -> 2, a -> 1, c -> 2) \n```\n\n\n\n## countByValue\n\n```scala\nval rdd1 = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"b\", 2), (\"c\", 2), (\"c\", 1)))\nrdd1.countByValue\n结果 (将整个元组作为key)\nMap((b,2) -> 2, (c,2) -> 1, (a,1) -> 1, (c,1) -> 1)\n```\n\n\n\n\n\n## filterByRange\n\n```scala\nval rdd1 = sc.parallelize(List((\"a\", 5), (\"b\", 3), (\"c\", 4), (\"d\", 2), (\"e\", 1)))\nval rdd2 = rdd1.filterByRange(\"b\", \"d\")\nrdd2.collect\n\nArray[(String, Int)] = Array((b,3), (c,4), (d,2))\n\n```\n\n## flatMapValues \n\n压平\n\n```scala\nval rdd3 = sc.parallelize(List((\"a\", \"1 2\"), (\"b\", \"3 4\")))\nval rdd4 = rdd3.flatMapValues(_.split(\" \"))\nrdd4.collect\n\nArray[(String, String)] = Array((a,1), (a,2), (b,3), (b,4))\n\n```\n\n## foldByKey\n\n```scala\nval rdd1 = sc.parallelize(List(\"a22\", \"b232\", \"c\", \"d\"), 2)\nval rdd2 = rdd1.map(x => (x.length, x))\nrdd2.collect\n结果： Array[(Int, String)] = Array((3,a22), (4,b232), (1,c), (1,d))\n\nval rdd3 = rdd2.foldByKey(\"\")(_+_)\nrdd3.collect\n\n\n\n\n结果：将相同key的元组合并在一起，\nArray[(Int, String)] = Array((4,b232), (1,cd), (3,a22))\n\n```\n\n## foreach\n\nforeach是针对于每一个元素， \nforeachPartition是针对每一个分区， \nforeachPartition是写入数据库时，可以将在foreachPartition时获得一个数据库连接，通过map方法来将每个分区的全部元素写入到数据库\n\n## foreachPartition\n\n3个分区\n\n```scala\nval rdd1 = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)\nrdd1.foreachPartition(x => println(x.reduce(_ + _)))\n\n```\n\n## keyBy\n\n以传入的参数做key\n\n```scala\nval rdd1 = sc.parallelize(List(\"dog\", \"salmon\", \"salmon\", \"rat\", \"elephant\"), 3)\nval rdd2 = rdd1.keyBy(_.length)\nrdd2.collect\n结果 Array((3,dog), (6,salmon), (6,salmon), (3,rat), (8,elephant))\n\n```\n\n## keys values\n\n```scala\nval rdd1 = sc.parallelize(List(\"dog\", \"tiger\", \"lion\", \"cat\", \"panther\", \"eagle\"), 2)\nval rdd2 = rdd1.map(x => (x.length, x))\nrdd2.keys.collect\nrdd2.values.collect\n```\n\n\n\n\n\n","source":"_posts/Spark算子.md","raw":"---\ntitle: Spark算子案例\ndate: 2018年08月06日 22时15分52秒\ntags: [Spark]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n# HelloWord？WorldCount\n\n```scala\nsc.textfile(\"hdfs://master:9000/wc\").flatMap(_.split(\"分隔符\")).map((_,1)).reduceByKey(_+_).saveAsTextFile(\"hdfs://master:9000/wcResult\")\n```\n\n\n数据最开始在Driver，计算的时候数据会流入worker\n当rdd形成过程中，worker的分区中只是预留了存放数据的位置，只有当action触发的时候，worker的分区中才会存在数据\n\nSpark的运算都是通过算子进行RDD的转换及运算，那我们对算子进行简单熟悉[参考RDD算子实例](http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html)\n\n<!-- more -->\n\n\n\n\n\nreduceByKey先进行一下combineer 移动计算\n\ngroupByKey不好\n\nreduceByKey会在局部先进行一下求和\n\ngroupByKey是会将所有的数据放在一个大集合里面，然后再求和 ，会消耗更多的网络带宽，不符合计算本地化\n\n\n\n\n\n一下一些RDD是给予rdd1来操作的\n\n```scala\nval rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)\n```\n\n\n\n## mapPartitions\n\nmap 是对 rdd 中的每一个元素进行操作，而 mapPartitions(foreachPartition) 则是对 rdd 中的每个分区的迭代器进行操作。如果在 map 过程中需要频繁创建额外的对象 (例如将 rdd 中的数据通过 jdbc 写入数据库, map 需要为每个元素创建一个链接而 mapPartition 为每个 partition 创建一个链接), 则 mapPartitions 效率比 map 高的多。\n\nSparkSql 或 DataFrame 默认会对程序进行 mapPartition 的优化。\n\n\n\n## mapPartitionsWithIndex\n\nmapPartitionWithIndex与mapPartition类似，只是会带上分区的序号\n\n把每个partition中的**分区号和对应的值**拿出来, 源码中方法的形式：\n\n```scala\nval func(index,Int,iter:Interator[(Int)]):Interator[String] = {\niter.toList.map(x => \"[partID:\" +  index + \", val: \" + x + \"]\").iterator\n}\n```\n\n会转换成函数 \n函数的形式\n\n```scala\nval func = (index: Int, iter: Iterator[(Int)]) => {\n  iter.toList.map(x => \"[partID:\" +  index + \", val: \" + x + \"]\").iterator\n}\nrdd1.mapPartitionsWithIndex(func).collect\n```\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fucjpo4afaj31ik056whp.jpg)\n\n## aggregate (action)\n\naggregate是一个action操作\n\n源码定义\n\n```scala\ndef aggregate[U](zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): U\n```\n\neqOp 操作会聚合各分区中的元素，然后 combOp 操作把所有分区的聚合结果再次聚合，两个操作的初始值都是 zeroValue.   seqOp 的操作是遍历分区中的所有元素 (T)，第一个 T 跟 zeroValue 做操作，结果再作为与第二个 T 做操作的 zeroValue，直到遍历完整个分区。combOp 操作是把各分区聚合的结果，再聚合。aggregate 函数返回一个跟 RDD 不同类型的值。因此，需要一个操作 seqOp 来把分区中的元素 T 合并成一个 U，另外一个操作 combOp 把所有 U 聚合。\n\n参考[理解 Spark RDD 中的 aggregate 函数](https://blog.csdn.net/qingyang0320/article/details/51603243)\n\n第一个参数：初始值（在进行操作的时候，会默认带入该值进行） \n第二个参数:   是两个函数[每个函数都是2个参数(第一个函数:先对各个分区进行合并, 第二个函数:对各个分区合并后的结果再进行合并)] \n\n最后得到返回值\n\n\n\n> rdd1为上面的rdd1分区函数的结果\n\n```scala\nrdd1.aggregate(0)(_+_, _+_)\n```\n\n**0 + (0+1+2+3+4 + 0+5+6+7+8+9)**\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fuck6fl2chj30kg02ut96.jpg)\n\n```scala\nrdd1.aggregate(7)(_+_, _+_)\n```\n\n**7 + (7+1+2+3+4 + 7+5+6+7+8+9)**\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fuck87fxhij30ju02gt92.jpg)\n\n\n\n```scala\nrdd1.aggregate(0)(math.max(_, _), _ + _)\n```\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fuck95wu88j30qo02o74x.jpg)\n\n\n\n```Scala\nrdd1.aggregate(5)(math.max(_, _), _ + _)\n```\n\n**5和1比, 得5再和234比得5 --> 5和6789比,得9 --> 5 + (5+9)**\n\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fucouprhcjj30s402yjs5.jpg)\n\n\n\n\n\n```scala\nval rdd2 = sc.parallelize(List(\"q\",\"w\",\"e\",\"r\",\"t\",\"y\",\"u\",\"i\",\"o\",\"p\"),2)\n```\n\n可以用更加直接的方式验证操作\n\n```Scala\ndef func2(index: Int, iter: Iterator[(String)]) : Iterator[String] = {\n  iter.toList.map(x => \"[partID:\" +  index + \", val: \" + x + \"]\").iterator\n}\n```\n\n```Scala\nrdd2.aggregate(\"\")(_ + _, _ + _)\nrdd2.aggregate(\"=\")(_ + _, _ + _)\nrdd2.aggregate(\"|\")(_ + _, _ + _)\n```\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fucpd6axxej30os034wf7.jpg)\n\n\n\n\n\n```scala\nval rdd3 = sc.parallelize(List(\"qazqqw7\",\"jishhrwe9\",\"sdfwezsddf12\",\"12esdww8\"),2)\nrdd3.aggregate(\"\")((x,y) => math.max(x.length, y.length).toString, (x,y) => x + y)\n```\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fucpvxoentj31gw03276d.jpg)\n\n\n\n```scala\nval rdd4 = sc.parallelize(List(\"qazqqw7\",\"jishhrwe9\",\"sdfwezsddf12\",\"\"),2)\nrdd4.aggregate(\"\")((x,y) => math.min(x.length, y.length).toString, (x,y) => x + y)\n```\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fucpwdi27nj31g8032abq.jpg)\n\n\n\n\n\n## aggregateByKey\n\n对每个分区进行计算\n\n```scala\nval pairRDD = sc.parallelize(List( (\"a\",1), (\"a\", 12), (\"b\", 4),(\"c\", 17), (\"c\", 12), (\"b\", 2)), 2)\ndef func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = {\n  iter.toList.map(x => \"[partID:\" +  index + \", val: \" + x + \"]\").iterator\n}\npairRDD.mapPartitionsWithIndex(func2).collect\n\n```\n\n## combineByKey\n\nreduceByKey aggregateByKey底层都是依赖的combineByKey，combineByKey比较底层的算子 \n和reduceByKey是相同的效果\n\n**combineByKey有三个参数**\n\n第一个参数x: **原封不动取出来**  第二个参数:**是函数, 局部运算**, 第三个:是函数, **对局部运算后的结果再做运算**\n\n\n\n```scala\nval rdd4 = sc.parallelize(List(\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\"),2)\nval rdd5 = sc.parallelize(List(1,1,2,2,2,1,2,2,2),2)\nval rdd6 = rdd5.zip(rdd4)\nval rdd7 = rdd6.combineByKey(List(_),(x:List[String],y:String)=>x:+y,(m:List[String],n:List[String])=>m ++ n)\nrdd7.collect\n```\n\n\n\n## reduceByKey\n\nreduceByKey 用于对每个 key 对应的多个 value 进行 merge 操作，最重要的是它能够在本地先进行 merge 操作，并且 merge 操作可以通过函数自定义。\n\n## groupByKey\n\ngroupByKey 也是对每个 key 进行操作，但只生成一个 sequence。不会再进行\n\n需要特别注意 “Note” 中的话，它告诉我们：如果需要对 sequence 进行 aggregation 操作（注意，groupByKey 本身不能自定义操作函数），那么，选择 reduceByKey/aggregateByKey 更好。这是因为 groupByKey 不能自定义函数，我们需要先用 groupByKey 生成 RDD，然后才能对此 RDD 通过 map 进行自定义函数操作。\n\n\n\n## checkpoint\n\n将rdd内容持久化\n\n```scala\nsc.setCheckpointDir(\"hdfs://master:9000/ck\")\nval rdd = sc.textFile(\"hdfs://master:9000/wc\").flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_+_)\nrdd.checkpoint\nrdd.isCheckpointed\nrdd.count\nrdd.isCheckpointed\nrdd.getCheckpointFile\n\n```\n\n## coalesce, repartition\n\n有时候需要重新设置 Rdd 的分区数量，比如 Rdd 的分区中，Rdd 分区比较多，但是每个 Rdd 的数据量比较小，需要设置一个比较合理的分区。或者需要把 Rdd 的分区数量调大。还有就是通过设置一个 Rdd 的分区来达到设置生成的文件的数量。\n\n如果分区的数量发生激烈的变化，如设置 numPartitions = 1，这可能会造成运行计算的节点比你想象的要少，为了避免这个情况，可以设置 shuffle=true，\n\n那么这会增加 shuffle 操作。\n\n关于这个分区的激烈的变化情况，比如分区数量从父 Rdd 的几千个分区设置成几个，有可能会遇到这么一个错误。\n\n```\njava.io.IOException: Unable to acquire 16777216 bytes of memory\n```\n\n这个错误只要把 shuffle 设置成 true 即可解决。\n\n当把父 Rdd 的分区数量增大时，比如 Rdd 的分区是 100，设置成 1000，如果 shuffle 为 false，并不会起作用。\n\n这时候就需要设置 shuffle 为 true 了，那么 Rdd 将在 shuffle 之后返回一个 1000 个分区的 Rdd，数据分区方式默认是采用 hash partitioner。\n\n最后来看看 repartition() 方法的源码：\n\n\n\n\n\ncoalesce() 方法的作用是返回指定一个新的指定分区的 Rdd。\n\n\n\n```scala\nval rdd1 = sc.parallelize(1 to 10, 10)\nval rdd2 = rdd1.coalesce(2, false)\nrdd2.partitions.length\n\n```\n\n[](https://www.cnblogs.com/fillPv/p/5392186.html)\n\n## collectAsMap \n\n将其他集合保存为map结构\n\n```scala\nval rdd = sc.parallelize(List((\"a\", 1), (\"b\", 2)))\nrdd.collectAsMap\n得到结果\nMap(b -> 2, a -> 1)\n```\n\n\n\n## countByKey\n\n```scala\nval rdd1 = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"b\", 2), (\"c\", 2), (\"c\", 1)))\nrdd1.countByKey \n统计Key出现的次数\n结果 Map(b -> 2, a -> 1, c -> 2) \n```\n\n\n\n## countByValue\n\n```scala\nval rdd1 = sc.parallelize(List((\"a\", 1), (\"b\", 2), (\"b\", 2), (\"c\", 2), (\"c\", 1)))\nrdd1.countByValue\n结果 (将整个元组作为key)\nMap((b,2) -> 2, (c,2) -> 1, (a,1) -> 1, (c,1) -> 1)\n```\n\n\n\n\n\n## filterByRange\n\n```scala\nval rdd1 = sc.parallelize(List((\"a\", 5), (\"b\", 3), (\"c\", 4), (\"d\", 2), (\"e\", 1)))\nval rdd2 = rdd1.filterByRange(\"b\", \"d\")\nrdd2.collect\n\nArray[(String, Int)] = Array((b,3), (c,4), (d,2))\n\n```\n\n## flatMapValues \n\n压平\n\n```scala\nval rdd3 = sc.parallelize(List((\"a\", \"1 2\"), (\"b\", \"3 4\")))\nval rdd4 = rdd3.flatMapValues(_.split(\" \"))\nrdd4.collect\n\nArray[(String, String)] = Array((a,1), (a,2), (b,3), (b,4))\n\n```\n\n## foldByKey\n\n```scala\nval rdd1 = sc.parallelize(List(\"a22\", \"b232\", \"c\", \"d\"), 2)\nval rdd2 = rdd1.map(x => (x.length, x))\nrdd2.collect\n结果： Array[(Int, String)] = Array((3,a22), (4,b232), (1,c), (1,d))\n\nval rdd3 = rdd2.foldByKey(\"\")(_+_)\nrdd3.collect\n\n\n\n\n结果：将相同key的元组合并在一起，\nArray[(Int, String)] = Array((4,b232), (1,cd), (3,a22))\n\n```\n\n## foreach\n\nforeach是针对于每一个元素， \nforeachPartition是针对每一个分区， \nforeachPartition是写入数据库时，可以将在foreachPartition时获得一个数据库连接，通过map方法来将每个分区的全部元素写入到数据库\n\n## foreachPartition\n\n3个分区\n\n```scala\nval rdd1 = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)\nrdd1.foreachPartition(x => println(x.reduce(_ + _)))\n\n```\n\n## keyBy\n\n以传入的参数做key\n\n```scala\nval rdd1 = sc.parallelize(List(\"dog\", \"salmon\", \"salmon\", \"rat\", \"elephant\"), 3)\nval rdd2 = rdd1.keyBy(_.length)\nrdd2.collect\n结果 Array((3,dog), (6,salmon), (6,salmon), (3,rat), (8,elephant))\n\n```\n\n## keys values\n\n```scala\nval rdd1 = sc.parallelize(List(\"dog\", \"tiger\", \"lion\", \"cat\", \"panther\", \"eagle\"), 2)\nval rdd2 = rdd1.map(x => (x.length, x))\nrdd2.keys.collect\nrdd2.values.collect\n```\n\n\n\n\n\n","slug":"Spark算子","published":1,"updated":"2018-08-20T01:44:42.481Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wu003y2tpbc6z9lhrq"},{"title":"Spark读取HBase解析json创建临时表录入到Hive表","date":"2018-08-10T16:09:46.585Z","toc":true,"_content":"\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/0069RVTdgy1fu81r6j5e5j30ji09c3z1.jpg)\n\n介绍：主要是读取通过mysql查到关联关系然后读取HBASE里面存放的Json，通过解析json将json数组对象里的元素拆分成单条json,再将json映射成临时表，查询临时表将数据落入到hive表中\n\n注意：查询HBASE的时候，HBase集群的HMaster，HRegionServer需要是正常运行\n\n主要将内容拆分成几块，spark读取HBase，spark解析json将json数组中每个元素拆成一条（比如json数组有10个元素，需要解析平铺成19个json，那么对应临时表中就是19条记录，对应查询插入到hive也就是19条记录）\n\nspark读取本地HBase\n\n<!-- more -->\n\n参考 [Spark读取HBase](http://www.gangtieguo.cn/2018/08/11/Spark读取Hbase/)\n\n# json样例\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu53tng462j31721e6afd.jpg)\n\n\n\n# 读取hbase\n\nhbase里面存放的是身份id作为rowkey来存放的数据\n\n> JSON、JSONObject类包是引用的com.alibaba.fastjson包下的\n\n```scala\n val hbaseJsonRdd: RDD[String] = hbaseRDD.mapPartitions( it=>{\n      it.map(x=>x._2).map(hbaseValue => {\n        var listBuffer = new ListBuffer[String]()\n        //对应的值\n        //获取key,也就是身份证号，通过身份证号在广播map中的值 也就是risk_request_id\n        val idNum = Bytes.toString(hbaseValue.getRow)\n        val json: String = Bytes.toString(hbaseValue.getValue(Bytes.toBytes(\"cf\"), Bytes.toBytes(s\"273468436_data\")))\n        if (null != json ) {\n          //********************************获取到json之后进行解析********************************\n          try {\n            val jSONObject: JSONObject = JSON.parseObject(json)\n            if (jSONObject != null) {\n                val contactRegion = repostData.getJSONArray(\"contact_region\")\n                if (contactRegion != null) {\n                  contactRegion.toArray().foreach(v => {\n                    val arrays = JSON.parseObject(v.toString)\n                    //val map = JSON.toJavaObject(arrays,classOf[util.Map[String,String]])\n                    val map: mutable.Map[String, String] = JsonUtils.jsonObj2Map(arrays)\n                    //将json 转为Map\n                    //将******************************** 日期和requestId request_id封装到 map里面********************************，再将map转为json\n                    map.put(\"region_id\", \"2\")\n                    map.put(\"request_id\", \"1\")\n                    map.put(\"region_create_at\", \"0000\")\n                    map.put(\"region_update_at\", \"0000\")\n                    listBuffer += (JsonUtils.map2Json(map))\n                  })\n                }\n              }\n            }\n          }catch {\n            case e: Exception => e.printStackTrace()\n          }\n        }\n        listBuffer\n      })\n    }).flatMap(r => r)\n```\n\n代码中的 jsonObj2Map,map2Json 方法参照 [Json与Scala类型的相互转换处理](http://www.gangtieguo.cn/2018/08/11/Json与Scala类型的一些互相转换处理/)\n\n这里拆分json数组每一个元素为一个json，存放在ListBuffer里面，通过flatMap压平rdd里面的内容。\n\n# 映射临时表\n\n最后将得到的json通过sparkSql创建成临时表\n\n```scala\n val dataFrame: DataFrame = sqlContext.read.json(hbaseJsonRdd)\ndataFrame.createOrReplaceTempView(\"tmp_hbase\")\n//// 测试\n\nprintln(\"++++++++++++++++++++++++++++++hbaseJsonRdd.....创建临时表 测试查询数据  ......++++++++++++++++++++++++++++++\")\nval df = sqlContext.sql(\"select * from tmp_hbase limit 1\")\ndf.show(1)\n```\n\n# 插入Hive\n\n```scala\nsqlContext.sql(\"insert into ods.ods_r_juxinli_region_n partition(dt='20180101') select region_id as juxinli_region_id,request_id as juxinli_request_id,\" +\n        \"region_loc as juxinli_rejion_loc ,region_uniq_num_cnt as juxinli_region_uniq_num_cnt ,\" +\n        \"region_call_out_time as juxinli_region_call_out_time,region_call_in_time as juxinli_region_call_in_time,region_call_out_cnt as juxinli_region_call_out_cnt,\" +\n        \"region_call_in_cnt as juxinli_region_call_in_cnt,region_avg_call_in_time as juxinli_region_avg_call_in_time,region_avg_call_out_time as juxinli_region_avg_call_out_time,\" +\n        \"region_call_in_time_pct as juxinli_region_call_in_time_pct,region_call_out_time_pct as juxinli_region_call_out_time_pct ,region_call_in_cnt_pct as juxinli_region_call_in_cnt_pct,\" +\n        \"region_call_out_cnt_pct as juxinli_region_call_out_cnt_pct,region_create_at as juxinli_region_create_at,region_update_at as juxinli_region_update_at from tmp_hbase\")\n    }\n```\n\n**关闭资源**\n\n```scala\nsparkContext.stop()\nsparkSession.close()\n```","source":"_posts/Spark读取HBase解析json创建临时表录入到Hive表.md","raw":"---\ntitle: Spark读取HBase解析json创建临时表录入到Hive表\ndate: 2018年08月06日 22时15分52秒\ntags: [Spark,SparkSQL]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/0069RVTdgy1fu81r6j5e5j30ji09c3z1.jpg)\n\n介绍：主要是读取通过mysql查到关联关系然后读取HBASE里面存放的Json，通过解析json将json数组对象里的元素拆分成单条json,再将json映射成临时表，查询临时表将数据落入到hive表中\n\n注意：查询HBASE的时候，HBase集群的HMaster，HRegionServer需要是正常运行\n\n主要将内容拆分成几块，spark读取HBase，spark解析json将json数组中每个元素拆成一条（比如json数组有10个元素，需要解析平铺成19个json，那么对应临时表中就是19条记录，对应查询插入到hive也就是19条记录）\n\nspark读取本地HBase\n\n<!-- more -->\n\n参考 [Spark读取HBase](http://www.gangtieguo.cn/2018/08/11/Spark读取Hbase/)\n\n# json样例\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu53tng462j31721e6afd.jpg)\n\n\n\n# 读取hbase\n\nhbase里面存放的是身份id作为rowkey来存放的数据\n\n> JSON、JSONObject类包是引用的com.alibaba.fastjson包下的\n\n```scala\n val hbaseJsonRdd: RDD[String] = hbaseRDD.mapPartitions( it=>{\n      it.map(x=>x._2).map(hbaseValue => {\n        var listBuffer = new ListBuffer[String]()\n        //对应的值\n        //获取key,也就是身份证号，通过身份证号在广播map中的值 也就是risk_request_id\n        val idNum = Bytes.toString(hbaseValue.getRow)\n        val json: String = Bytes.toString(hbaseValue.getValue(Bytes.toBytes(\"cf\"), Bytes.toBytes(s\"273468436_data\")))\n        if (null != json ) {\n          //********************************获取到json之后进行解析********************************\n          try {\n            val jSONObject: JSONObject = JSON.parseObject(json)\n            if (jSONObject != null) {\n                val contactRegion = repostData.getJSONArray(\"contact_region\")\n                if (contactRegion != null) {\n                  contactRegion.toArray().foreach(v => {\n                    val arrays = JSON.parseObject(v.toString)\n                    //val map = JSON.toJavaObject(arrays,classOf[util.Map[String,String]])\n                    val map: mutable.Map[String, String] = JsonUtils.jsonObj2Map(arrays)\n                    //将json 转为Map\n                    //将******************************** 日期和requestId request_id封装到 map里面********************************，再将map转为json\n                    map.put(\"region_id\", \"2\")\n                    map.put(\"request_id\", \"1\")\n                    map.put(\"region_create_at\", \"0000\")\n                    map.put(\"region_update_at\", \"0000\")\n                    listBuffer += (JsonUtils.map2Json(map))\n                  })\n                }\n              }\n            }\n          }catch {\n            case e: Exception => e.printStackTrace()\n          }\n        }\n        listBuffer\n      })\n    }).flatMap(r => r)\n```\n\n代码中的 jsonObj2Map,map2Json 方法参照 [Json与Scala类型的相互转换处理](http://www.gangtieguo.cn/2018/08/11/Json与Scala类型的一些互相转换处理/)\n\n这里拆分json数组每一个元素为一个json，存放在ListBuffer里面，通过flatMap压平rdd里面的内容。\n\n# 映射临时表\n\n最后将得到的json通过sparkSql创建成临时表\n\n```scala\n val dataFrame: DataFrame = sqlContext.read.json(hbaseJsonRdd)\ndataFrame.createOrReplaceTempView(\"tmp_hbase\")\n//// 测试\n\nprintln(\"++++++++++++++++++++++++++++++hbaseJsonRdd.....创建临时表 测试查询数据  ......++++++++++++++++++++++++++++++\")\nval df = sqlContext.sql(\"select * from tmp_hbase limit 1\")\ndf.show(1)\n```\n\n# 插入Hive\n\n```scala\nsqlContext.sql(\"insert into ods.ods_r_juxinli_region_n partition(dt='20180101') select region_id as juxinli_region_id,request_id as juxinli_request_id,\" +\n        \"region_loc as juxinli_rejion_loc ,region_uniq_num_cnt as juxinli_region_uniq_num_cnt ,\" +\n        \"region_call_out_time as juxinli_region_call_out_time,region_call_in_time as juxinli_region_call_in_time,region_call_out_cnt as juxinli_region_call_out_cnt,\" +\n        \"region_call_in_cnt as juxinli_region_call_in_cnt,region_avg_call_in_time as juxinli_region_avg_call_in_time,region_avg_call_out_time as juxinli_region_avg_call_out_time,\" +\n        \"region_call_in_time_pct as juxinli_region_call_in_time_pct,region_call_out_time_pct as juxinli_region_call_out_time_pct ,region_call_in_cnt_pct as juxinli_region_call_in_cnt_pct,\" +\n        \"region_call_out_cnt_pct as juxinli_region_call_out_cnt_pct,region_create_at as juxinli_region_create_at,region_update_at as juxinli_region_update_at from tmp_hbase\")\n    }\n```\n\n**关闭资源**\n\n```scala\nsparkContext.stop()\nsparkSession.close()\n```","slug":"Spark读取HBase解析json创建临时表录入到Hive表","published":1,"updated":"2018-08-13T06:16:38.124Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43ww00412tpbnktf4y54"},{"title":"Spark读取HBase","date":"2018-08-10T16:18:11.706Z","toc":true,"_content":"\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu552ugvkxj30ys0han0l.jpg)\n\nSpark读取Hbase\n\n<!-- more -->\n\n# spark配置\n\n首先spark的配置\n\n```scala\nval array = Array(\n      (\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\"),\n      (\"spark.storage.memoryFraction\", \"0.3\"),\n      (\"spark.memory.useLegacyMode\", \"true\"),\n      (\"spark.shuffle.memoryFraction\", \"0.6\"),\n      (\"spark.shuffle.file.buffer\", \"128k\"),\n      (\"spark.reducer.maxSizeInFlight\", \"96m\"),\n      (\"spark.sql.shuffle.partitions\", \"500\"),\n      (\"spark.default.parallelism\", \"180\"),\n      (\"spark.dynamicAllocation.enabled\", \"false\")\n    )\n    val conf = new SparkConf().setAll(array)\n      .setJars(Array(\"your.jar\"))\n    val sparkSession: SparkSession = SparkSession\n      .builder\n      .appName(applicationName)\n      .enableHiveSupport()\n      .master(\"spark://master:7077\")\n      .config(conf)\n      .getOrCreate()\n    val sqlContext = sparkSession.sqlContext\n    val sparkContext: SparkContext = sparkSession.sparkContext\n\n```\n\n# Hbase配置\n\n```scala\nval hBaseConf = HBaseConfiguration.create()\n\nvar scan = new Scan();\nscan.addFamily(Bytes.toBytes(\"cf\"));\nvar proto = ProtobufUtil.toScan(scan)\nvar scanToString = Base64.encodeBytes(proto.toByteArray())\n//以为全局扫描的方式\nhBaseConf.set(TableInputFormat.SCAN,scanToString)\n//如需要设置起止行的话\n//scan.setStartRow(Bytes.toBytes(\"1111111111111\"))\n//scan.setStopRow(Bytes.toBytes(\"999999999999999\"))\nhBaseConf.set(\"hbase.zookeeper.quorum\",\"zk1,zk2,zk3\")\nhBaseConf.set(\"phoenix.query.timeoutMs\",\"1800000\")\nhBaseConf.set(\"hbase.regionserver.lease.period\",\"1200000\")\nhBaseConf.set(\"hbase.rpc.timeout\",\"1200000\")\nhBaseConf.set(\"hbase.client.scanner.caching\",\"1000\")\nhBaseConf.set(\"hbase.client.scanner.timeout.period\",\"1200000\")\n//表名配置\nhBaseConf.set(TableInputFormat.INPUT_TABLE,\"beehive:a_up_rawdata\")\n// 从数据源获取数据\nval hbaseRDD = sparkContext.newAPIHadoopRDD(hBaseConf,classOf[TableInputFormat],classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],classOf[org.apache.hadoop.hbase.client.Result])\n//即可得到读取Hbase查询的RDD\n val hbaseJsonRdd: RDD[String] = hbaseRDD.filter(t =>\n        broadCast.value.contains(Bytes.toString(t._2.getRow))\n      //********************************操作每个分区的数据********************************\n    ).mapPartitions( it=>{\n      it.map(x=>x._2).map(hbaseValue => {\n        var listBuffer = new ListBuffer[String]()\n        //对应的值\n        val rowkey = Bytes.toString(hbaseValue.getRow)\n        val value: String = Bytes.toString(hbaseValue.getValue(Bytes.toBytes(\"cf\"), Bytes.toBytes(\"填写获取哪一列\")))\n        if (null != value ) {\n          //如果value不为空则再进行操作\n        }\n        listBuffer\n\n      })\n    }).flatMap(r => r)\n\n//注意map操作是需要函数内部有返回值的，如果只是打印的话，换成foreach算子\n    println(s\"hbaseJsonRdd.size为：${hbaseJsonRdd.count()}\")\n    sparkContext.stop()\n    sparkSession.close()\n    println(\"ALL 已经关闭，程序终止\")\n```\n\n\n\n\n\n","source":"_posts/Spark读取Hbase.md","raw":"---\ntitle: Spark读取HBase\ndate: 2018年08月06日 22时15分52秒\ntags: [Spark,HBase]\ncategories: 大数据\ntoc: true\n---\n\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu552ugvkxj30ys0han0l.jpg)\n\nSpark读取Hbase\n\n<!-- more -->\n\n# spark配置\n\n首先spark的配置\n\n```scala\nval array = Array(\n      (\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\"),\n      (\"spark.storage.memoryFraction\", \"0.3\"),\n      (\"spark.memory.useLegacyMode\", \"true\"),\n      (\"spark.shuffle.memoryFraction\", \"0.6\"),\n      (\"spark.shuffle.file.buffer\", \"128k\"),\n      (\"spark.reducer.maxSizeInFlight\", \"96m\"),\n      (\"spark.sql.shuffle.partitions\", \"500\"),\n      (\"spark.default.parallelism\", \"180\"),\n      (\"spark.dynamicAllocation.enabled\", \"false\")\n    )\n    val conf = new SparkConf().setAll(array)\n      .setJars(Array(\"your.jar\"))\n    val sparkSession: SparkSession = SparkSession\n      .builder\n      .appName(applicationName)\n      .enableHiveSupport()\n      .master(\"spark://master:7077\")\n      .config(conf)\n      .getOrCreate()\n    val sqlContext = sparkSession.sqlContext\n    val sparkContext: SparkContext = sparkSession.sparkContext\n\n```\n\n# Hbase配置\n\n```scala\nval hBaseConf = HBaseConfiguration.create()\n\nvar scan = new Scan();\nscan.addFamily(Bytes.toBytes(\"cf\"));\nvar proto = ProtobufUtil.toScan(scan)\nvar scanToString = Base64.encodeBytes(proto.toByteArray())\n//以为全局扫描的方式\nhBaseConf.set(TableInputFormat.SCAN,scanToString)\n//如需要设置起止行的话\n//scan.setStartRow(Bytes.toBytes(\"1111111111111\"))\n//scan.setStopRow(Bytes.toBytes(\"999999999999999\"))\nhBaseConf.set(\"hbase.zookeeper.quorum\",\"zk1,zk2,zk3\")\nhBaseConf.set(\"phoenix.query.timeoutMs\",\"1800000\")\nhBaseConf.set(\"hbase.regionserver.lease.period\",\"1200000\")\nhBaseConf.set(\"hbase.rpc.timeout\",\"1200000\")\nhBaseConf.set(\"hbase.client.scanner.caching\",\"1000\")\nhBaseConf.set(\"hbase.client.scanner.timeout.period\",\"1200000\")\n//表名配置\nhBaseConf.set(TableInputFormat.INPUT_TABLE,\"beehive:a_up_rawdata\")\n// 从数据源获取数据\nval hbaseRDD = sparkContext.newAPIHadoopRDD(hBaseConf,classOf[TableInputFormat],classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],classOf[org.apache.hadoop.hbase.client.Result])\n//即可得到读取Hbase查询的RDD\n val hbaseJsonRdd: RDD[String] = hbaseRDD.filter(t =>\n        broadCast.value.contains(Bytes.toString(t._2.getRow))\n      //********************************操作每个分区的数据********************************\n    ).mapPartitions( it=>{\n      it.map(x=>x._2).map(hbaseValue => {\n        var listBuffer = new ListBuffer[String]()\n        //对应的值\n        val rowkey = Bytes.toString(hbaseValue.getRow)\n        val value: String = Bytes.toString(hbaseValue.getValue(Bytes.toBytes(\"cf\"), Bytes.toBytes(\"填写获取哪一列\")))\n        if (null != value ) {\n          //如果value不为空则再进行操作\n        }\n        listBuffer\n\n      })\n    }).flatMap(r => r)\n\n//注意map操作是需要函数内部有返回值的，如果只是打印的话，换成foreach算子\n    println(s\"hbaseJsonRdd.size为：${hbaseJsonRdd.count()}\")\n    sparkContext.stop()\n    sparkSession.close()\n    println(\"ALL 已经关闭，程序终止\")\n```\n\n\n\n\n\n","slug":"Spark读取Hbase","published":1,"updated":"2018-08-10T17:56:47.563Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wy00462tpblnawe8ui"},{"title":"ELK容器的搭建","date":"2018-07-08T15:43:05.950Z","toc":true,"typora-copy-images-to":"ipic","_content":"\n[TOC]\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fu54yue2cxj31g20q875x.jpg)\n\n## 来源容器  elk\n\n新建容器，为减少工作量，引用的是有ssh服务的Docker镜像**kinogmt/centos-ssh:6.7**，生成容器os为基准。\n\n```\ndocker run -itd  --name elk --hostname elk kinogmt/centos-ssh:6.7 &> /dev/null\n```\n\n> 注意必须要以-d方式启动，不然sshd服务不会启动，这算是一个小bug\n\n<!--more-->\n\n在容器中下载需要的elk的源包。做解压就不赘述，很多案例教程。\n\n> 我是采用的下载到宿主机，解压后，用 \"docker cp 解压包目录  os:/usr/loca/\"来传到容器内，比在容器内下载速度更快\n\n\n\n\n## 设置Home\n `vim   ~/bashrc`\n\n```bash\nexport ES_HOME=/usr/es\nexport PATH=$ES_HOME/bin:$PATH\nexport KIBANA_HOME=/usr/kibana\nexport PATH=$KIBANA_HOME/bin:$PATH\nexport LOGSTASH_HOME=/usr/logstash\nexport PATH=$LOGSTASH_HOME/bin:$PATH\nexport NODE_HOME=/usr/node\nexport PATH=$NODE_HOME/bin:$PATH\nexport NODE_PATH=$NODE_HOME/lib/node_modules\n```\n`source ~/.bashrc`\n\n## 安装插件header\n\n### 安装nodejs\n\n一般预装的版本不对\n\n```\nyum erase nodejs npm -y   # 卸载旧版本的nodejs\nrpm -qa 'node|npm' | grep -v nodesource # 确认nodejs是否卸载干净\nyum install nodejs -y # 安装npm 安装的版本会有不对\n```\n\n下载合适版本\n\n```bash\ncd /usr\nwget https://npm.taobao.org/mirrors/node/latest-v4.x/node-v4.4.7-llinux-x64.tar.gz\ntar -zxvf node-v4.4.7-linux-x64.tar.gz\nmv node-v8.9.1-linux-x64 node\n```\n直接将node目录配置到home即可\n\n```\nexport NODE_HOME=/usr/node\nexport PATH=$NODE_HOME/bin:$PATH \n```\n\n\n\n### 下载 header，安装grunt\n\n（所有命令在hear的所在目录执行）\n\n`wget https://github.com/mobz/elasticsearch-head/archive/master.zip `\n\n`unzip master.zip`\n\n看当前 head 插件目录下有无 node_modules/grunt 目录： \n没有：执行命令创建：\n\n```\nnpm install grunt --save\n```\n\n安装 grunt： \ngrunt 是基于 Node.js 的项目构建工具，可以进行打包压缩、测试、执行等等的工作，head 插件就是通过 grunt 启动\n\n```\nnpm install -g grunt-cli\n```\n\n参考https://blog.csdn.net/ggwxk1990/article/details/78698648\n\n npm install 安装所下载的header包 \n\n```\nnpm install\n```\n\n\n\n## header启动\n\n在 elasticsearch-head-master 目录下\n\n```\ngrunt server  或者 npm run start\n```\n\n\n\n## els不能通过root启动，创建用户\n\n\tuseradd elk\n\tgroupadd elk\n\tusermod -a -G elk elk\n\t\n\techo elk | passwd --stdin elk\n\n将elk添加到sudoers\n\n\techo \"elk ALL = (root) NOPASSWD:ALL\" | tee /etc/sudoers.d/elk\n\tchmod 0440 /etc/sudoers.d/elk\n\n解决sudo: sorry, you must have a tty to run sudo问题，在/etc/sudoer注释掉 Default requiretty 一行\n\tsudo sed -i 's/Defaults requiretty/Defaults:elk !requiretty/' /etc/sudoers\n\n\n\n### 修改文件所有者\n`chown -R elk:elk /usr/es/`\n\n**设置资源参数**\n\n```\n  sudo vim  /etc/security/limits.d/90-nproc.conf\n```\n\n\n**添加**\n\t elk        soft    nproc     4096\n   <!-- 更改docker-machine的资源 -->\n```\ndocker-machine ssh\nsysctl -w vm.max_map_count=655360\n```\n\n### es启动脚本\n\n    单机 su elk -c \"$ES_HOME/bin/elasticsearch -d\"\n    ssh elk@elk1 \" $ES_HOME/bin/elasticsearch -d\"\n    ssh root@elk1 \" su elk -c  $ES_HOME/bin/elasticsearch \"\n\n\n\n### 集群\n#### elasticSearch\n脚本 `vim  es-start.sh`\n\n```bash\n#!/bin/bash\nsed -i '6c node.name: es1 '\n$ES_HOME/config/elasticsearch.yml\nsu - elk -c  \"$ES_HOME/bin/elasticsearch -d\"\nssh root@elk2 \"sed -i '6c node.name: es2 ' $ES_HOME/config/elasticsearch.yml\"\nssh root@elk2 ' su - elk -c  \"$ES_HOME/bin/elasticsearch -d\" '\nssh root@elk3 \"sed -i '6c node.name: es3 ' $ES_HOME/config/elasticsearch.yml\"\nssh root@elk3 ' su - elk -c  \"$ES_HOME/bin/elasticsearch -d\" '\n```\n#### kibana\n启动单机（只需要启动单机） `bin/kibana`\n```bash\n#!/bin/bash\nsed -i '3c http://elk1:9200 '\n$KIBANA_HOME/config/kibana.yml\nnohup $KIBANA_HOME/bin/kibana  &\nssh root@elk2 \"sed -i '3c http://elk2:9200 ' $KIBANA_HOME/config/kibana.yml\"\nssh root@elk2 \"nohup $KIBANA_HOME/bin/kibana & \"\nssh root@elk3 \"sed -i '3c   http://elk3:9200 ' $KIBANA_HOME/config/kibana.yml\"\nssh root@elk3 \"nohup $KIBANA_HOME/bin/kibana & \"\n```\n#### logstash\n单机启动   `$LOGSTASH_HOME/bin/logstash -f logstash.conf`\n `$LOGSTASH_HOME/bin/logstash -f 配置文件的目录`\n\b\n\n集群启动脚本 **logstash-start.sh**\n```shell\n#!/bin/bash\nnohup $LOGSTASH_HOME/bin/logstash -f  $LOGSTASH_HOME/conf/$1  &\nssh root@elk2 \"nohup $LOGSTASH_HOME/bin/logstash -f  $LOGSTASH_HOME/conf/$1 & \"\nssh root@elk3 \"nohup $LOGSTASH_HOME/bin/logstash -f  $LOGSTASH_HOME/conf/$1 & \"\n```\n\n\n\n## 保存容器为镜像\n\n```bash\ndocker commit -m \"elk镜像\"  --author=\"yaosong\"  os  yaosong5/elk:1.0\n```\n\n\n\n## 生成elk 容器\n\n```shell\ndocker run -itd --net=br --name elk1 --hostname elk1 yaosong5/elk:1.0 &> /dev/null\ndocker run -itd --net=br --name elk2 --hostname elk2 yaosong5/elk:1.0 &> /dev/null\ndocker run -itd --net=br --name elk3 --hostname elk3 yaosong5/elk:1.0 &> /dev/null\n```\n\n\n\n### 停止/删除elk 容器\n\n```shell\ndocker stop elk1\ndocker stop elk2\ndocker stop elk3\n\ndocker rm elk1\ndocker rm elk2\ndocker rm elk3\n```\n## 参考\n## elk 操作命令\n### es操作命令\nhttp://www.yfshare.vip/2017/11/04/%E9%83%A8%E7%BD%B2FileBeat-logstash-elasticsearch%E9%9B%86%E7%BE%A4-kibana/#%E9%85%8D%E7%BD%AE-filebeart\n\n\n\n## 其他\nyum erase nodejs npm -y   # 卸载旧版本的nodejs\nrpm -qa 'node|npm' | grep -v nodesource # 确认nodejs是否卸载干净\nyum install nodejs -y\n","source":"_posts/elk容器的搭建.md","raw":"---\ntitle:  ELK容器的搭建\ndate: 2018年08月06日 22时15分52秒\ntags:  [ELK,Docker]\ncategories: 安装部署\ntoc: true\ntypora-copy-images-to: ipic\n---\n\n[TOC]\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fu54yue2cxj31g20q875x.jpg)\n\n## 来源容器  elk\n\n新建容器，为减少工作量，引用的是有ssh服务的Docker镜像**kinogmt/centos-ssh:6.7**，生成容器os为基准。\n\n```\ndocker run -itd  --name elk --hostname elk kinogmt/centos-ssh:6.7 &> /dev/null\n```\n\n> 注意必须要以-d方式启动，不然sshd服务不会启动，这算是一个小bug\n\n<!--more-->\n\n在容器中下载需要的elk的源包。做解压就不赘述，很多案例教程。\n\n> 我是采用的下载到宿主机，解压后，用 \"docker cp 解压包目录  os:/usr/loca/\"来传到容器内，比在容器内下载速度更快\n\n\n\n\n## 设置Home\n `vim   ~/bashrc`\n\n```bash\nexport ES_HOME=/usr/es\nexport PATH=$ES_HOME/bin:$PATH\nexport KIBANA_HOME=/usr/kibana\nexport PATH=$KIBANA_HOME/bin:$PATH\nexport LOGSTASH_HOME=/usr/logstash\nexport PATH=$LOGSTASH_HOME/bin:$PATH\nexport NODE_HOME=/usr/node\nexport PATH=$NODE_HOME/bin:$PATH\nexport NODE_PATH=$NODE_HOME/lib/node_modules\n```\n`source ~/.bashrc`\n\n## 安装插件header\n\n### 安装nodejs\n\n一般预装的版本不对\n\n```\nyum erase nodejs npm -y   # 卸载旧版本的nodejs\nrpm -qa 'node|npm' | grep -v nodesource # 确认nodejs是否卸载干净\nyum install nodejs -y # 安装npm 安装的版本会有不对\n```\n\n下载合适版本\n\n```bash\ncd /usr\nwget https://npm.taobao.org/mirrors/node/latest-v4.x/node-v4.4.7-llinux-x64.tar.gz\ntar -zxvf node-v4.4.7-linux-x64.tar.gz\nmv node-v8.9.1-linux-x64 node\n```\n直接将node目录配置到home即可\n\n```\nexport NODE_HOME=/usr/node\nexport PATH=$NODE_HOME/bin:$PATH \n```\n\n\n\n### 下载 header，安装grunt\n\n（所有命令在hear的所在目录执行）\n\n`wget https://github.com/mobz/elasticsearch-head/archive/master.zip `\n\n`unzip master.zip`\n\n看当前 head 插件目录下有无 node_modules/grunt 目录： \n没有：执行命令创建：\n\n```\nnpm install grunt --save\n```\n\n安装 grunt： \ngrunt 是基于 Node.js 的项目构建工具，可以进行打包压缩、测试、执行等等的工作，head 插件就是通过 grunt 启动\n\n```\nnpm install -g grunt-cli\n```\n\n参考https://blog.csdn.net/ggwxk1990/article/details/78698648\n\n npm install 安装所下载的header包 \n\n```\nnpm install\n```\n\n\n\n## header启动\n\n在 elasticsearch-head-master 目录下\n\n```\ngrunt server  或者 npm run start\n```\n\n\n\n## els不能通过root启动，创建用户\n\n\tuseradd elk\n\tgroupadd elk\n\tusermod -a -G elk elk\n\t\n\techo elk | passwd --stdin elk\n\n将elk添加到sudoers\n\n\techo \"elk ALL = (root) NOPASSWD:ALL\" | tee /etc/sudoers.d/elk\n\tchmod 0440 /etc/sudoers.d/elk\n\n解决sudo: sorry, you must have a tty to run sudo问题，在/etc/sudoer注释掉 Default requiretty 一行\n\tsudo sed -i 's/Defaults requiretty/Defaults:elk !requiretty/' /etc/sudoers\n\n\n\n### 修改文件所有者\n`chown -R elk:elk /usr/es/`\n\n**设置资源参数**\n\n```\n  sudo vim  /etc/security/limits.d/90-nproc.conf\n```\n\n\n**添加**\n\t elk        soft    nproc     4096\n   <!-- 更改docker-machine的资源 -->\n```\ndocker-machine ssh\nsysctl -w vm.max_map_count=655360\n```\n\n### es启动脚本\n\n    单机 su elk -c \"$ES_HOME/bin/elasticsearch -d\"\n    ssh elk@elk1 \" $ES_HOME/bin/elasticsearch -d\"\n    ssh root@elk1 \" su elk -c  $ES_HOME/bin/elasticsearch \"\n\n\n\n### 集群\n#### elasticSearch\n脚本 `vim  es-start.sh`\n\n```bash\n#!/bin/bash\nsed -i '6c node.name: es1 '\n$ES_HOME/config/elasticsearch.yml\nsu - elk -c  \"$ES_HOME/bin/elasticsearch -d\"\nssh root@elk2 \"sed -i '6c node.name: es2 ' $ES_HOME/config/elasticsearch.yml\"\nssh root@elk2 ' su - elk -c  \"$ES_HOME/bin/elasticsearch -d\" '\nssh root@elk3 \"sed -i '6c node.name: es3 ' $ES_HOME/config/elasticsearch.yml\"\nssh root@elk3 ' su - elk -c  \"$ES_HOME/bin/elasticsearch -d\" '\n```\n#### kibana\n启动单机（只需要启动单机） `bin/kibana`\n```bash\n#!/bin/bash\nsed -i '3c http://elk1:9200 '\n$KIBANA_HOME/config/kibana.yml\nnohup $KIBANA_HOME/bin/kibana  &\nssh root@elk2 \"sed -i '3c http://elk2:9200 ' $KIBANA_HOME/config/kibana.yml\"\nssh root@elk2 \"nohup $KIBANA_HOME/bin/kibana & \"\nssh root@elk3 \"sed -i '3c   http://elk3:9200 ' $KIBANA_HOME/config/kibana.yml\"\nssh root@elk3 \"nohup $KIBANA_HOME/bin/kibana & \"\n```\n#### logstash\n单机启动   `$LOGSTASH_HOME/bin/logstash -f logstash.conf`\n `$LOGSTASH_HOME/bin/logstash -f 配置文件的目录`\n\b\n\n集群启动脚本 **logstash-start.sh**\n```shell\n#!/bin/bash\nnohup $LOGSTASH_HOME/bin/logstash -f  $LOGSTASH_HOME/conf/$1  &\nssh root@elk2 \"nohup $LOGSTASH_HOME/bin/logstash -f  $LOGSTASH_HOME/conf/$1 & \"\nssh root@elk3 \"nohup $LOGSTASH_HOME/bin/logstash -f  $LOGSTASH_HOME/conf/$1 & \"\n```\n\n\n\n## 保存容器为镜像\n\n```bash\ndocker commit -m \"elk镜像\"  --author=\"yaosong\"  os  yaosong5/elk:1.0\n```\n\n\n\n## 生成elk 容器\n\n```shell\ndocker run -itd --net=br --name elk1 --hostname elk1 yaosong5/elk:1.0 &> /dev/null\ndocker run -itd --net=br --name elk2 --hostname elk2 yaosong5/elk:1.0 &> /dev/null\ndocker run -itd --net=br --name elk3 --hostname elk3 yaosong5/elk:1.0 &> /dev/null\n```\n\n\n\n### 停止/删除elk 容器\n\n```shell\ndocker stop elk1\ndocker stop elk2\ndocker stop elk3\n\ndocker rm elk1\ndocker rm elk2\ndocker rm elk3\n```\n## 参考\n## elk 操作命令\n### es操作命令\nhttp://www.yfshare.vip/2017/11/04/%E9%83%A8%E7%BD%B2FileBeat-logstash-elasticsearch%E9%9B%86%E7%BE%A4-kibana/#%E9%85%8D%E7%BD%AE-filebeart\n\n\n\n## 其他\nyum erase nodejs npm -y   # 卸载旧版本的nodejs\nrpm -qa 'node|npm' | grep -v nodesource # 确认nodejs是否卸载干净\nyum install nodejs -y\n","slug":"elk容器的搭建","published":1,"updated":"2018-08-10T17:50:32.279Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43wz00492tpbj4tun92m"},{"title":"ES测试命令","date":"2018-07-09T16:11:03.158Z","toc":true,"typora-copy-images-to":"ipic","_content":"\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu55ayb4i7j31kw0c73yz.jpg)\n\n简单命令测试和展示es的功能\n\n<!--more -->\n\n插入记录\n\n```\ncurl  -H \"Content-Type: application/json\"  -XPUT 'http://localhost:9200/store/books/1' -d '{\n\n\"title\": \"Elasticsearch: The Definitive Guide\",\n\"name\" : {\n    \"first\" : \"Zachary\",\n    \"last\" : \"Tong\"\n},\n\"publish_date\":\"2015-02-06\",\n\"price\":\"49.99\"\n\n}'\n\n\n```\n\n在添加一个书的信息\n```\ncurl  -H \"Content-Type: application/json\"  -XPUT 'http://elk1:9200/store/books/2' -d '{\n\"title\": \"Elasticsearch Blueprints\",\n\"name\" : {\n    \"first\" : \"Vineeth\",\n    \"last\" : \"Mohan\"\n},\n\"publish_date\":\"2015-06-06\",\n\"price\":\"35.99\"\n}'\n```\n\n通过ID获得文档信息\n\n```\ncurl   -H \"Content-Type: application/json\"  -XGET 'http://elk1:9200/store/books/1'\n```\n\n\n\n```\ncurl  -H \"Content-Type: application/json\"  -XGET 'http://elk1:9200/store/books/_search' -d '{\n\"query\" : {\n\n\"filtered\" : {\n    \"query\" : {\n        \"match_all\" : {}\n        },\n        \"filter\" : {\n            \"term\" : {\n                \"price\" : 35.99\n            }\n        }\n    }\n}\n\n}'\n```\n\n\n\n在浏览\n\n \t\n\n```\ncurl -H \"Content-Type: application/json\"  -XPUT 'http://elk1:9200/store/books/1' -d '{\n\n\"title\": \"Elasticsearch: The Definitive Guide\",\n\"name\" : {\n    \"first\" : \"Zachary\",\n    \"last\" : \"Tong\"\n},\n\"publish_date\":\"2015-02-06\",\n\"price\":\"49.99\"\n\n}'\n```\n\n\n\n```\ncurl -H \"Content-Type: application/json\" -XPUT 'http://127.0.0.1:9200/kc22k2_test’ -d ‘\n```\n\n\n\n```\ncurl -XPUT elk1:9200/test\n```\n\n\n\n```\ncurl -XGET 'http://elk1:9200/_cluster/state?pretty'\n{\n  \"error\" : {\n\n\"root_cause\" : [\n  {\n    \"type\" : \"master_not_discovered_exception\",\n    \"reason\" : null\n  }\n],\n\"type\" : \"master_not_discovered_exception\",\n\"reason\" : null\n\n  },\n  \"status\" : 503\n}\n```\n\n","source":"_posts/es测试命令.md","raw":"---\ntitle:  ES测试命令\ndate: 2018年08月06日 22时15分52秒\ntags:  [ELK,Docker,es]\ncategories: 安装部署\ntoc: true\ntypora-copy-images-to: ipic\n---\n\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu55ayb4i7j31kw0c73yz.jpg)\n\n简单命令测试和展示es的功能\n\n<!--more -->\n\n插入记录\n\n```\ncurl  -H \"Content-Type: application/json\"  -XPUT 'http://localhost:9200/store/books/1' -d '{\n\n\"title\": \"Elasticsearch: The Definitive Guide\",\n\"name\" : {\n    \"first\" : \"Zachary\",\n    \"last\" : \"Tong\"\n},\n\"publish_date\":\"2015-02-06\",\n\"price\":\"49.99\"\n\n}'\n\n\n```\n\n在添加一个书的信息\n```\ncurl  -H \"Content-Type: application/json\"  -XPUT 'http://elk1:9200/store/books/2' -d '{\n\"title\": \"Elasticsearch Blueprints\",\n\"name\" : {\n    \"first\" : \"Vineeth\",\n    \"last\" : \"Mohan\"\n},\n\"publish_date\":\"2015-06-06\",\n\"price\":\"35.99\"\n}'\n```\n\n通过ID获得文档信息\n\n```\ncurl   -H \"Content-Type: application/json\"  -XGET 'http://elk1:9200/store/books/1'\n```\n\n\n\n```\ncurl  -H \"Content-Type: application/json\"  -XGET 'http://elk1:9200/store/books/_search' -d '{\n\"query\" : {\n\n\"filtered\" : {\n    \"query\" : {\n        \"match_all\" : {}\n        },\n        \"filter\" : {\n            \"term\" : {\n                \"price\" : 35.99\n            }\n        }\n    }\n}\n\n}'\n```\n\n\n\n在浏览\n\n \t\n\n```\ncurl -H \"Content-Type: application/json\"  -XPUT 'http://elk1:9200/store/books/1' -d '{\n\n\"title\": \"Elasticsearch: The Definitive Guide\",\n\"name\" : {\n    \"first\" : \"Zachary\",\n    \"last\" : \"Tong\"\n},\n\"publish_date\":\"2015-02-06\",\n\"price\":\"49.99\"\n\n}'\n```\n\n\n\n```\ncurl -H \"Content-Type: application/json\" -XPUT 'http://127.0.0.1:9200/kc22k2_test’ -d ‘\n```\n\n\n\n```\ncurl -XPUT elk1:9200/test\n```\n\n\n\n```\ncurl -XGET 'http://elk1:9200/_cluster/state?pretty'\n{\n  \"error\" : {\n\n\"root_cause\" : [\n  {\n    \"type\" : \"master_not_discovered_exception\",\n    \"reason\" : null\n  }\n],\n\"type\" : \"master_not_discovered_exception\",\n\"reason\" : null\n\n  },\n  \"status\" : 503\n}\n```\n\n","slug":"es测试命令","published":1,"updated":"2018-08-10T18:02:13.602Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43x0004d2tpbfis1bujj"},{"title":"FLINK容器的搭建","date":"2018-07-08T16:00:22.635Z","toc":true,"typora-copy-images-to":"ipic","_content":"\n[TOC]\n\n\n\n\n\n# 来源容器  flk\n\n新建容器，为减少工作量，引用的是有ssh服务的Docker镜像**kinogmt/centos-ssh:6.7**，生成容器os为基准。\n\n```\ndocker run -itd  --name flk --hostname flk kinogmt/centos-ssh:6.7 &> /dev/null\n```\n\n> 注意必须要以-d方式启动，不然sshd服务不会启动，这算是一个小bug\n\n<!--more-->\n\n在容器中下载需要的elk的源包。做解压就不赘述，很多案例教程。\n\n> 我是采用的下载到宿主机，解压后，用 \"docker cp 解压包目录  os:/usr/loca/\"来传到容器内，比在容器内下载速度更快\n>\n> ## 复制源包\n>\n> ```\n> docker cp /Users/yaosong/Yao/hadoop  flk:/usr/\n> docker cp /Users/yaosong/Yao/flink  flk:/usr/\n> ```\n\n\n\n\n\n# 配置home\n\n\n\texport JAVA_HOME=/usr/java/jdk1.8.0_144/\n\texport PATH=$JAVA_HOME/bin:$PATH\n\texport FLINK_HOME=/usr/flink\n\texport HADOOP_HOME=/usr/hadoop\n\texport HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoop\n\texport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n\texport PATH=$PATH:$HADOOP_HOME/bin\n\texport PATH=$PATH:$HADOOP_HOME/sbin\n\texport PATH=$PATH:$FLINK_HOME/bin\n\n**配置hadoop**  参考：\n\n[]: http://gangtieguo.cn/2018/07/20/Docker%E4%B8%ADhadoop%20spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/\t\"Docker 中 hadoop，spark 镜像搭建\"\n\n\n\n**flink.yml**\n\n```\n high-availability: zookeeper\n\n# The path where metadata for master recovery is persisted. While ZooKeeper stores\n# the small ground truth for checkpoint and leader election, this location stores\n# the larger objects, like persisted dataflow graphs.\n# \n# Must be a durable file system that is accessible from all nodes\n# (like HDFS, S3, Ceph, nfs, ...) \n#\n high-availability.storageDir: hdfs:///flink/ha/\n\n# The list of ZooKeeper quorum peers that coordinate the high-availability\n# setup. This must be a list of the form:\n# \"host1:clientPort,host2:clientPort,...\" (default clientPort: 2181)\n#\nhigh-availability.zookeeper.quorum: zk1:2181,zk2:2181,zk3:2181\n```\n\n**Master**\n\n```\nmaster:8081\n```\n\n**slave**\n\n```\nslave01\nslave02\n```\n\n**zoo.cfg**\n\n```\nserver.1=zk1:2888:3888\nserver.2=zk2:2888:3888\nserver.3=zk3:2888:3888\n```\n\n**需要在yarn-site.xml中配置**\n\n```xml\n<property>\n \t<name>yarn.resourcemanager.am.max-attempts</name>\n\t<value>4</value>\n</property>\n<property>\n\t<name>yarn.nodemanager.resource.cpu-vcores</name>\n\t<value>8</value>\n</property>\n```\n\n# 保存镜像\n\n```bash\ndocker commit -m \"bigdata:flink,hadoop\"  --author=\"yaosong\"  flk  yao/flinkonyarn:1.0\n```\n\n#   获得flk 容器\n\n```bash\n docker run -itd --net=br --name flk1 --hostname flk1 yao/flinkonyarn:1.0 &> /dev/null\n docker run -itd --net=br --name flk2 --hostname flk2 yao/flinkonyarn:1.0 &> /dev/null\n docker run -itd --net=br --name flk3 --hostname flk3 yao/flinkonyarn:1.0 &> /dev/null\n```\n\n# 停止/删除flk 容器\n\n```bash\ndocker stop flk1\ndocker stop flk2\ndocker stop flk3\ndocker rm flk1\ndocker rm flk2\ndocker rm flk3\n```\n\n# 官方：wordcount\n\n## flink测试命令\n\n由于在本地搭建，机器配置有限，故设置不同参数命令来运行官方wordcount\n\n```bash\nflink run -m yarn-cluster $FLINK_HOME/examples/batch/WordCount.jar\n\nflink run -m yarn-cluster -ynd 2 $FLINK_HOME/examples/batch/WordCount.jar\n\nflink run -m yarn-cluster -yn 4  $FLINK_HOME/examples/batch/WordCount.jar\n\nflink run -m yarn-cluster -yn 6  $FLINK_HOME/examples/batch/WordCount.jar\n\nflink run -m yarn-cluster -yn 8  $FLINK_HOME/examples/batch/WordCount.jar\n\nflink run -m yarn-cluster -yn 10  $FLINK_HOME/examples/batch/WordCount.jar\n```\n\n\n\n\n\n\n\n","source":"_posts/flink容器搭建.md","raw":"---\ntitle:  FLINK容器的搭建\ndate: 2018年08月06日 22时15分52秒\ntags:  [Docker,FLINK]\ncategories: 安装部署\ntoc: true\ntypora-copy-images-to: ipic\n---\n\n[TOC]\n\n\n\n\n\n# 来源容器  flk\n\n新建容器，为减少工作量，引用的是有ssh服务的Docker镜像**kinogmt/centos-ssh:6.7**，生成容器os为基准。\n\n```\ndocker run -itd  --name flk --hostname flk kinogmt/centos-ssh:6.7 &> /dev/null\n```\n\n> 注意必须要以-d方式启动，不然sshd服务不会启动，这算是一个小bug\n\n<!--more-->\n\n在容器中下载需要的elk的源包。做解压就不赘述，很多案例教程。\n\n> 我是采用的下载到宿主机，解压后，用 \"docker cp 解压包目录  os:/usr/loca/\"来传到容器内，比在容器内下载速度更快\n>\n> ## 复制源包\n>\n> ```\n> docker cp /Users/yaosong/Yao/hadoop  flk:/usr/\n> docker cp /Users/yaosong/Yao/flink  flk:/usr/\n> ```\n\n\n\n\n\n# 配置home\n\n\n\texport JAVA_HOME=/usr/java/jdk1.8.0_144/\n\texport PATH=$JAVA_HOME/bin:$PATH\n\texport FLINK_HOME=/usr/flink\n\texport HADOOP_HOME=/usr/hadoop\n\texport HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoop\n\texport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n\texport PATH=$PATH:$HADOOP_HOME/bin\n\texport PATH=$PATH:$HADOOP_HOME/sbin\n\texport PATH=$PATH:$FLINK_HOME/bin\n\n**配置hadoop**  参考：\n\n[]: http://gangtieguo.cn/2018/07/20/Docker%E4%B8%ADhadoop%20spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/\t\"Docker 中 hadoop，spark 镜像搭建\"\n\n\n\n**flink.yml**\n\n```\n high-availability: zookeeper\n\n# The path where metadata for master recovery is persisted. While ZooKeeper stores\n# the small ground truth for checkpoint and leader election, this location stores\n# the larger objects, like persisted dataflow graphs.\n# \n# Must be a durable file system that is accessible from all nodes\n# (like HDFS, S3, Ceph, nfs, ...) \n#\n high-availability.storageDir: hdfs:///flink/ha/\n\n# The list of ZooKeeper quorum peers that coordinate the high-availability\n# setup. This must be a list of the form:\n# \"host1:clientPort,host2:clientPort,...\" (default clientPort: 2181)\n#\nhigh-availability.zookeeper.quorum: zk1:2181,zk2:2181,zk3:2181\n```\n\n**Master**\n\n```\nmaster:8081\n```\n\n**slave**\n\n```\nslave01\nslave02\n```\n\n**zoo.cfg**\n\n```\nserver.1=zk1:2888:3888\nserver.2=zk2:2888:3888\nserver.3=zk3:2888:3888\n```\n\n**需要在yarn-site.xml中配置**\n\n```xml\n<property>\n \t<name>yarn.resourcemanager.am.max-attempts</name>\n\t<value>4</value>\n</property>\n<property>\n\t<name>yarn.nodemanager.resource.cpu-vcores</name>\n\t<value>8</value>\n</property>\n```\n\n# 保存镜像\n\n```bash\ndocker commit -m \"bigdata:flink,hadoop\"  --author=\"yaosong\"  flk  yao/flinkonyarn:1.0\n```\n\n#   获得flk 容器\n\n```bash\n docker run -itd --net=br --name flk1 --hostname flk1 yao/flinkonyarn:1.0 &> /dev/null\n docker run -itd --net=br --name flk2 --hostname flk2 yao/flinkonyarn:1.0 &> /dev/null\n docker run -itd --net=br --name flk3 --hostname flk3 yao/flinkonyarn:1.0 &> /dev/null\n```\n\n# 停止/删除flk 容器\n\n```bash\ndocker stop flk1\ndocker stop flk2\ndocker stop flk3\ndocker rm flk1\ndocker rm flk2\ndocker rm flk3\n```\n\n# 官方：wordcount\n\n## flink测试命令\n\n由于在本地搭建，机器配置有限，故设置不同参数命令来运行官方wordcount\n\n```bash\nflink run -m yarn-cluster $FLINK_HOME/examples/batch/WordCount.jar\n\nflink run -m yarn-cluster -ynd 2 $FLINK_HOME/examples/batch/WordCount.jar\n\nflink run -m yarn-cluster -yn 4  $FLINK_HOME/examples/batch/WordCount.jar\n\nflink run -m yarn-cluster -yn 6  $FLINK_HOME/examples/batch/WordCount.jar\n\nflink run -m yarn-cluster -yn 8  $FLINK_HOME/examples/batch/WordCount.jar\n\nflink run -m yarn-cluster -yn 10  $FLINK_HOME/examples/batch/WordCount.jar\n```\n\n\n\n\n\n\n\n","slug":"flink容器搭建","published":1,"updated":"2018-08-07T02:52:58.322Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43x2004g2tpb4g7n5e9i"},{"title":"git命令总结","date":"2018-05-21T17:43:07.025Z","toc":true,"_content":"\n\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu55a5sqb0j313s0cwmxj.jpg)\n\n## 提交\n\n>git add .\n>git commit -m \" \"\n>git push origin master\n>git push origin master -f\n## 拉取\ngit pull <远程主机名> <远程分支名>:<本地分支名>\n如拉取远程的 master 分支到本地 wy 分支：\ngit pull origin master:wy\n\n## 分支切换\n<!-- more -->\n查看分支：git branch\n创建分支：git branch <name>\n切换分支：git checkout <name>\n创建 + 切换分支：git checkout -b <name>\n合并某分支到当前分支：git merge <name>\n删除分支：git branch -d <name>\n","source":"_posts/git命令总结.md","raw":"---\ntitle:  git命令总结\ndate:\ntags:  [git]\ncategories: 工程框架\ntoc: true\n---\n\n\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu55a5sqb0j313s0cwmxj.jpg)\n\n## 提交\n\n>git add .\n>git commit -m \" \"\n>git push origin master\n>git push origin master -f\n## 拉取\ngit pull <远程主机名> <远程分支名>:<本地分支名>\n如拉取远程的 master 分支到本地 wy 分支：\ngit pull origin master:wy\n\n## 分支切换\n<!-- more -->\n查看分支：git branch\n创建分支：git branch <name>\n切换分支：git checkout <name>\n创建 + 切换分支：git checkout -b <name>\n合并某分支到当前分支：git merge <name>\n删除分支：git branch -d <name>\n","slug":"git命令总结","published":1,"updated":"2018-08-10T18:01:16.275Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43x3004k2tpbyx6tl0e2"},{"title":"Hadoop&Spark组合容器的搭建","date":"2018-07-08T17:06:20.114Z","toc":true,"typora-copy-images-to":"ipic","_content":"\n[TOC]\n\n**配置centos集群 hadoop spark组件**\n启动容器\n各组件版本对应\nhbase1.2  hive 版本 2.0.0  hbase1.x ZooKeeper 3.4.x is required as of HBase 1.0.0\n\n\n\n新建容器，为减少工作量，引用的是有ssh服务的Docker镜像**kinogmt/centos-ssh:6.7**，生成容器os为基准。\n\n```\ndocker run -itd  --name bigdata --hostname bigdata kinogmt/centos-ssh:6.7 &> /dev/null\n```\n\n> 注意必须要以-d方式启动，不然sshd服务不会启动，这算是一个小bug\n\n<!--more-->\n\n在容器中下载需要的elk的源包，做解压就不赘述，很多案例教程。\n\n 我是采用的下载到宿主机，解压后，用 \"docker cp 解压包目录  os:/usr/loca/\"来传到容器内，比在容器内下载速度更快\n\n # 拷贝文件到容器\n\n \n\n 命令格式`docker cp 本地文件路径   容器id或者容器名称:`\n 将所有组件下载解压并拷贝到容器\n\n ```bash\n docker cp /Users/yaosong/Downloads/hadoop-2.8.0.tar.gz   bigdata:/\n docker cp /Users/yaosong/Downloads/spark-2.2.0-bin-without-hadoop.tgz   bigdata:/\n docker cp /Users/yaosong/Downloads/jdk-8u144-linux-x64.rpm   bigdata:/\n docker cp /Users/yaosong/Downloads/spark-2.1.0-bin-hadoop2.6.tgz   bigdata:/\n docker cp  /Users/yaosong/Yao/spark源包/hive bigdata:/usr\n docker cp /Users/yaosong/Downloads/jdk-8u144-linux-x64.rpm  bigdata:/\n docker cp /Users/yaosong/Downloads/hadoop-2.8.0.tar.gz  bigdata:/\n docker cp /Users/yaosong/Downloads/spark-2.2.0-bin-without-hadoop.tgz  bigdata:/\n docker cp /Users/yaosong/Downloads/spark-2.1.0-bin-hadoop2.6.tgz  bigdata:/\n docker cp /Users/yaosong/Yao/spark源包/hbase  bigdata:/usr\n docker cp /Users/yaosong/Yao/spark源包/zk   bigdata:/usr\n docker cp  /Users/yaosong/Yao/ant    bigdata:/usr\n docker cp  /Users/yaosong/Yao/maven  bigdata:/usr\n docker cp  /Users/yaosong/Yao/hue4  bigdata:/usr\n 创建home\n\n vim /etc/profile\n\n mac: vim ~/.bashrc\n\n 添加以下内容\n\n     export JAVA_HOME=/usr/java/jdk\n     export PATH=$JAVA_HOME:$PATH\n     export SCALA_HOME=/usr/scala-2.12.3/\n     export HADOOP_HOME=/usr/hadoop\n     export HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoop\n     export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n     export PATH=$PATH:$HADOOP_HOME/bin\n     export PATH=$PATH:$HADOOP_HOME/sbin\n     export SPARK_DIST_CLASSPATH=$(hadoop classpath)\n     SPARK_MASTER_IP=master\n     SPARK_LOCAL_DIRS=/usr/spark\n     SPARK_DRIVER_MEMORY=1G\n     export SPARK_HOME=/usr/spark\n     export PATH=$SPARK_HOME/bin:$PATH\n     export PATH=$SPARK_HOME/sbin:$PATH\n     \n     MAVEN_HOME=/usr/maven\n     export MAVEN_HOME\n     export PATH=${PATH}:${MAVEN_HOME}/bin\n     ANT_HOME=/usr/ant\n     PATH=$JAVA_HOME/bin:$ANT_HOME/bin:$PATH\n     export ANT_HOME PATH\n     HUE_HOME=/usr/hue4\n     export ZK_HOME=/usr/zk\n     export HBASE_HOME=/usr/hbase\n     export PATH=$HBASE_HOME/bin:$PATH\n     export PATH=$ZK_HOME/bin:$PATH\n\n\n\n ```\n\n\n\n# 安装\n\n## 创建 hadoop 集群所需目录：\n\n在以下配置文件中会有以下目录\n\n```bash\ncd $HADOOP_HOME;\nmkdir tmp\nmkdir namenode\nmkdir datanode\ncd $HADOOP_CONFIG_HOME/\n```\n## 更改配置文件\n\n`cd $HADOOP_CONFIG_HOME/` or `cd $HADOOP_HOME/etc/hadoop`\n#### hdfs slaves\n\n```bash\nslave01\nslave02\n```\n\n#### core-site.xml：\n\n```xml\n<property>\n        <name>hadoop.tmp.dir</name>\n        <value>/usr/hadoop/tmp</value>\n        <description>A base for other temporary directories.</description>\n</property>\n<property>\n\t<name>fs.default.name</name>\n\t  <value>hdfs://master:9000</value>\n\t  <final>true</final>\n\t  <description>The name of the default file system.\n\t  A URI whose scheme and authority determine the\n\t  FileSystem implementation.\n\t  </description>\n  </property>\n  <!--hive的配置，参考https://blog.csdn.net/lblblblblzdx/article/details/79760959-->\n  <property>\n  \t<name>hive.server2.authentication</name>\n  \t<value>NONE</value>\n  </property>\n  <!--hive的配置hadoop代理用户  root用户提交的任务可以在任意机器上以任意组的所有用户的身份执行。-->\n  <property>\n \t<name>hadoop.proxyuser.root.hosts</name>\n  \t<value>*</value>\n  </property>\n  <property>\n\t<name>hadoop.proxyuser.root.groups</name>\n\t<value>*</value>\n  </property>\n\t <!--HUE 增加一个值开启 hdfs 的 web 交互-->\n\t<property>\n\t      <name>dfs.webhdfs.enabled</name>\n\t      <value>true</value>\n\t</property>\n <!--HUE 增加一个值开启 hdfs 的 web 交互-->\n```\n\n#### hdfs-site.xml：\n```xml\n<property>\n    <name>dfs.replication</name>\n    <value>2</value>\n    <final>true</final>\n    <description>Default block replication.\n    The actual number of replications can be specified when the file is created.\n    The default is used if replication is not specified in create time.\n    </description>\n</property>\n\n<property>\n    <name>dfs.namenode.name.dir</name>\n    <value>/usr/hadoop/namenode</value>\n    <final>true</final>\n</property>\n\n<property>\n    <name>dfs.datanode.data.dir</name>\n    <value>/usr/hadoop/datanode</value>\n    <final>true</final>\n</property>\n<!--《为了让 hue 能够访问 hdfs，需要在 hdfs-site.xml 里面配置一些内容-->\n<property>\n    <name>hadoop.proxyuser.hue.hosts</name>\n    <value>*</value>\n</property>\n<property>\n    <name>hadoop.proxyuser.hue.groups</name>\n    <value>*</value>\n</property>\n<!--《为了让 hue 能够访问 hdfs，需要在 hdfs-site.xml 里面配置一些内容-->\n```\n\n#### mapred-site.xml：\n```xml\n<property>\n    <name>mapred.job.tracker</name>\n    <value>master:9001</value>\n    <description>The host and port that the MapReduce job tracker runs\n    at.  If \"local\", then jobs are run in-process as a single map\n    and reduce task.\n    </description>\n</property>\n```\n####  yarn-site.xml：\n```xml\n<property>\n\t<name>yarn.nodemanager.pmem-check-enabled</name>\n\t<value>false</value>\n</property>\n<property>\n    <name>yarn.nodemanager.vmem-check-enabled</name>\n    <value>false</value>\n    <description>Whether virtual memory limits will be enforced for containers</description>\n</property>\n<property>\n  <name>yarn.scheduler.minimum-allocation-mb</name>\n  <value>256</value>\n</property>\n<property>\n\t<name>yarn.resourcemanager.address</name>\n\t<value>master:8032</value>\n</property>\n<property>\n\t<name>yarn.resourcemanager.scheduler.address</name>\n\t<value>master:8030</value>\n</property>\n<property>\n\t<name>yarn.resourcemanager.resource-tracker.address</name>\n\t<value>master:8031</value>\n</property>\n```\n\n\n\n如果是hadoop3以上版本，需要在**start-dfs.sh start-yarn.sh**中开头空白处分别配置一下内容\n\n```bash\nvim $HADOOP_HOME/sbin/start-dfs.sh\n\nHDFS_DATANODE_USER=root\nHADOOP_SECURE_DN_USER=hdfs\nHDFS_NAMENODE_USER=root\nHDFS_SECONDARYNAMENODE_USER=root\n\nvim $HADOOP_HOME/sbin/start-yarn.sh\n\nYARN_RESOURCEMANAGER_USER=root\nHADOOP_SECURE_DN_USER=root\nYARN_NODEMANAGER_USER=yarn\nYARN_PROXYSERVER_USER=root\n```\n\n## 格式化namenode\n\n```bash\n$HADOOP_HOME/bin/hadoop namenode -format\n```\n\n\n# 启动集群\n\n`$HADOOP_HOME/sbin/start-all.sh`\n\n测试\n\n```bash\nyarn 8088端口   http://yourip:8088\n```\n\n```bash\nhdfs 50070端口 hdfs3.0为9870   http://yourip:50070\n```\n\n## spark只需要在slaves中添加\n\n```bash\nslave01\nslave02\n\n```\n**sparkUI端口8080**\n\n# 测试spark集群\n\n启动spark \n\n```bash\n$HADOOP_HOME/bin/start-all.sh\n```\n\n## 官网命令\n\n```bash\n$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode cluster \\\n--driver-memory 512m \\\n--executor-memory 512m \\\n--executor-cores 1 \\\n$SPARK_HOME/examples/jars/spark-examples*.jar \\\n10\n```\n\n\n\n## 执行spark on yarn命令行模式\n\n```bash\nspark-shell --master yarn --deploy-mode client --driver-memory 1g --executor-memory 1g --executor-cores 1\n\nspark-shell --master yarn --deploy-mode client --driver-memory 512m --executor-memory 512m --executor-cores 1\n\nspark-shell --master yarn --deploy-mode client --driver-memory 475m --executor-memory 475m --executor-cores 1\n\nspark-shell --master yarn --deploy-mode client --driver-memory 350m --executor-memory 350m --executor-cores 1\n\nspark-shell --master yarn --deploy-mode client --driver-memory 650m --executor-memory 650m --executor-cores 1\n```\n\n\n\n# 创建镜像\n\n```bash\ndocker commit -m \"bigdata基础组件镜像\"  bigdata yaosong5/bigdata:2.0\n```\n\n\n\n# 创建容器\n\n```bash\ndocker run -itd  --net=br  --name master --hostname master yaosong5/bigdata:2.0 &> /dev/null\ndocker run -itd  --net=br  --name slave01 --hostname slave01 yaosong5/bigdata:2.0 &> /dev/null\ndocker run -itd  --net=br  --name slave02 --hostname slave02 yaosong5/bigdata:2.0 &> /dev/null\n```\n\n# 停止and 删除容器\n\n```bash\ndocker stop master\ndocker stop slave01\ndocker stop slave02\ndocker rm master\ndocker rm slave01\ndocker rm slave02\n```\n\n","source":"_posts/hadoop-spark集群搭建.md","raw":"---\ntitle:  Hadoop&Spark组合容器的搭建\ndate: 2018年08月06日 22时15分52秒\ntags:  [Docker,Spark,Hadoop]\ncategories: 安装部署\ntoc: true\ntypora-copy-images-to: ipic\n---\n\n[TOC]\n\n**配置centos集群 hadoop spark组件**\n启动容器\n各组件版本对应\nhbase1.2  hive 版本 2.0.0  hbase1.x ZooKeeper 3.4.x is required as of HBase 1.0.0\n\n\n\n新建容器，为减少工作量，引用的是有ssh服务的Docker镜像**kinogmt/centos-ssh:6.7**，生成容器os为基准。\n\n```\ndocker run -itd  --name bigdata --hostname bigdata kinogmt/centos-ssh:6.7 &> /dev/null\n```\n\n> 注意必须要以-d方式启动，不然sshd服务不会启动，这算是一个小bug\n\n<!--more-->\n\n在容器中下载需要的elk的源包，做解压就不赘述，很多案例教程。\n\n 我是采用的下载到宿主机，解压后，用 \"docker cp 解压包目录  os:/usr/loca/\"来传到容器内，比在容器内下载速度更快\n\n # 拷贝文件到容器\n\n \n\n 命令格式`docker cp 本地文件路径   容器id或者容器名称:`\n 将所有组件下载解压并拷贝到容器\n\n ```bash\n docker cp /Users/yaosong/Downloads/hadoop-2.8.0.tar.gz   bigdata:/\n docker cp /Users/yaosong/Downloads/spark-2.2.0-bin-without-hadoop.tgz   bigdata:/\n docker cp /Users/yaosong/Downloads/jdk-8u144-linux-x64.rpm   bigdata:/\n docker cp /Users/yaosong/Downloads/spark-2.1.0-bin-hadoop2.6.tgz   bigdata:/\n docker cp  /Users/yaosong/Yao/spark源包/hive bigdata:/usr\n docker cp /Users/yaosong/Downloads/jdk-8u144-linux-x64.rpm  bigdata:/\n docker cp /Users/yaosong/Downloads/hadoop-2.8.0.tar.gz  bigdata:/\n docker cp /Users/yaosong/Downloads/spark-2.2.0-bin-without-hadoop.tgz  bigdata:/\n docker cp /Users/yaosong/Downloads/spark-2.1.0-bin-hadoop2.6.tgz  bigdata:/\n docker cp /Users/yaosong/Yao/spark源包/hbase  bigdata:/usr\n docker cp /Users/yaosong/Yao/spark源包/zk   bigdata:/usr\n docker cp  /Users/yaosong/Yao/ant    bigdata:/usr\n docker cp  /Users/yaosong/Yao/maven  bigdata:/usr\n docker cp  /Users/yaosong/Yao/hue4  bigdata:/usr\n 创建home\n\n vim /etc/profile\n\n mac: vim ~/.bashrc\n\n 添加以下内容\n\n     export JAVA_HOME=/usr/java/jdk\n     export PATH=$JAVA_HOME:$PATH\n     export SCALA_HOME=/usr/scala-2.12.3/\n     export HADOOP_HOME=/usr/hadoop\n     export HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoop\n     export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n     export PATH=$PATH:$HADOOP_HOME/bin\n     export PATH=$PATH:$HADOOP_HOME/sbin\n     export SPARK_DIST_CLASSPATH=$(hadoop classpath)\n     SPARK_MASTER_IP=master\n     SPARK_LOCAL_DIRS=/usr/spark\n     SPARK_DRIVER_MEMORY=1G\n     export SPARK_HOME=/usr/spark\n     export PATH=$SPARK_HOME/bin:$PATH\n     export PATH=$SPARK_HOME/sbin:$PATH\n     \n     MAVEN_HOME=/usr/maven\n     export MAVEN_HOME\n     export PATH=${PATH}:${MAVEN_HOME}/bin\n     ANT_HOME=/usr/ant\n     PATH=$JAVA_HOME/bin:$ANT_HOME/bin:$PATH\n     export ANT_HOME PATH\n     HUE_HOME=/usr/hue4\n     export ZK_HOME=/usr/zk\n     export HBASE_HOME=/usr/hbase\n     export PATH=$HBASE_HOME/bin:$PATH\n     export PATH=$ZK_HOME/bin:$PATH\n\n\n\n ```\n\n\n\n# 安装\n\n## 创建 hadoop 集群所需目录：\n\n在以下配置文件中会有以下目录\n\n```bash\ncd $HADOOP_HOME;\nmkdir tmp\nmkdir namenode\nmkdir datanode\ncd $HADOOP_CONFIG_HOME/\n```\n## 更改配置文件\n\n`cd $HADOOP_CONFIG_HOME/` or `cd $HADOOP_HOME/etc/hadoop`\n#### hdfs slaves\n\n```bash\nslave01\nslave02\n```\n\n#### core-site.xml：\n\n```xml\n<property>\n        <name>hadoop.tmp.dir</name>\n        <value>/usr/hadoop/tmp</value>\n        <description>A base for other temporary directories.</description>\n</property>\n<property>\n\t<name>fs.default.name</name>\n\t  <value>hdfs://master:9000</value>\n\t  <final>true</final>\n\t  <description>The name of the default file system.\n\t  A URI whose scheme and authority determine the\n\t  FileSystem implementation.\n\t  </description>\n  </property>\n  <!--hive的配置，参考https://blog.csdn.net/lblblblblzdx/article/details/79760959-->\n  <property>\n  \t<name>hive.server2.authentication</name>\n  \t<value>NONE</value>\n  </property>\n  <!--hive的配置hadoop代理用户  root用户提交的任务可以在任意机器上以任意组的所有用户的身份执行。-->\n  <property>\n \t<name>hadoop.proxyuser.root.hosts</name>\n  \t<value>*</value>\n  </property>\n  <property>\n\t<name>hadoop.proxyuser.root.groups</name>\n\t<value>*</value>\n  </property>\n\t <!--HUE 增加一个值开启 hdfs 的 web 交互-->\n\t<property>\n\t      <name>dfs.webhdfs.enabled</name>\n\t      <value>true</value>\n\t</property>\n <!--HUE 增加一个值开启 hdfs 的 web 交互-->\n```\n\n#### hdfs-site.xml：\n```xml\n<property>\n    <name>dfs.replication</name>\n    <value>2</value>\n    <final>true</final>\n    <description>Default block replication.\n    The actual number of replications can be specified when the file is created.\n    The default is used if replication is not specified in create time.\n    </description>\n</property>\n\n<property>\n    <name>dfs.namenode.name.dir</name>\n    <value>/usr/hadoop/namenode</value>\n    <final>true</final>\n</property>\n\n<property>\n    <name>dfs.datanode.data.dir</name>\n    <value>/usr/hadoop/datanode</value>\n    <final>true</final>\n</property>\n<!--《为了让 hue 能够访问 hdfs，需要在 hdfs-site.xml 里面配置一些内容-->\n<property>\n    <name>hadoop.proxyuser.hue.hosts</name>\n    <value>*</value>\n</property>\n<property>\n    <name>hadoop.proxyuser.hue.groups</name>\n    <value>*</value>\n</property>\n<!--《为了让 hue 能够访问 hdfs，需要在 hdfs-site.xml 里面配置一些内容-->\n```\n\n#### mapred-site.xml：\n```xml\n<property>\n    <name>mapred.job.tracker</name>\n    <value>master:9001</value>\n    <description>The host and port that the MapReduce job tracker runs\n    at.  If \"local\", then jobs are run in-process as a single map\n    and reduce task.\n    </description>\n</property>\n```\n####  yarn-site.xml：\n```xml\n<property>\n\t<name>yarn.nodemanager.pmem-check-enabled</name>\n\t<value>false</value>\n</property>\n<property>\n    <name>yarn.nodemanager.vmem-check-enabled</name>\n    <value>false</value>\n    <description>Whether virtual memory limits will be enforced for containers</description>\n</property>\n<property>\n  <name>yarn.scheduler.minimum-allocation-mb</name>\n  <value>256</value>\n</property>\n<property>\n\t<name>yarn.resourcemanager.address</name>\n\t<value>master:8032</value>\n</property>\n<property>\n\t<name>yarn.resourcemanager.scheduler.address</name>\n\t<value>master:8030</value>\n</property>\n<property>\n\t<name>yarn.resourcemanager.resource-tracker.address</name>\n\t<value>master:8031</value>\n</property>\n```\n\n\n\n如果是hadoop3以上版本，需要在**start-dfs.sh start-yarn.sh**中开头空白处分别配置一下内容\n\n```bash\nvim $HADOOP_HOME/sbin/start-dfs.sh\n\nHDFS_DATANODE_USER=root\nHADOOP_SECURE_DN_USER=hdfs\nHDFS_NAMENODE_USER=root\nHDFS_SECONDARYNAMENODE_USER=root\n\nvim $HADOOP_HOME/sbin/start-yarn.sh\n\nYARN_RESOURCEMANAGER_USER=root\nHADOOP_SECURE_DN_USER=root\nYARN_NODEMANAGER_USER=yarn\nYARN_PROXYSERVER_USER=root\n```\n\n## 格式化namenode\n\n```bash\n$HADOOP_HOME/bin/hadoop namenode -format\n```\n\n\n# 启动集群\n\n`$HADOOP_HOME/sbin/start-all.sh`\n\n测试\n\n```bash\nyarn 8088端口   http://yourip:8088\n```\n\n```bash\nhdfs 50070端口 hdfs3.0为9870   http://yourip:50070\n```\n\n## spark只需要在slaves中添加\n\n```bash\nslave01\nslave02\n\n```\n**sparkUI端口8080**\n\n# 测试spark集群\n\n启动spark \n\n```bash\n$HADOOP_HOME/bin/start-all.sh\n```\n\n## 官网命令\n\n```bash\n$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode cluster \\\n--driver-memory 512m \\\n--executor-memory 512m \\\n--executor-cores 1 \\\n$SPARK_HOME/examples/jars/spark-examples*.jar \\\n10\n```\n\n\n\n## 执行spark on yarn命令行模式\n\n```bash\nspark-shell --master yarn --deploy-mode client --driver-memory 1g --executor-memory 1g --executor-cores 1\n\nspark-shell --master yarn --deploy-mode client --driver-memory 512m --executor-memory 512m --executor-cores 1\n\nspark-shell --master yarn --deploy-mode client --driver-memory 475m --executor-memory 475m --executor-cores 1\n\nspark-shell --master yarn --deploy-mode client --driver-memory 350m --executor-memory 350m --executor-cores 1\n\nspark-shell --master yarn --deploy-mode client --driver-memory 650m --executor-memory 650m --executor-cores 1\n```\n\n\n\n# 创建镜像\n\n```bash\ndocker commit -m \"bigdata基础组件镜像\"  bigdata yaosong5/bigdata:2.0\n```\n\n\n\n# 创建容器\n\n```bash\ndocker run -itd  --net=br  --name master --hostname master yaosong5/bigdata:2.0 &> /dev/null\ndocker run -itd  --net=br  --name slave01 --hostname slave01 yaosong5/bigdata:2.0 &> /dev/null\ndocker run -itd  --net=br  --name slave02 --hostname slave02 yaosong5/bigdata:2.0 &> /dev/null\n```\n\n# 停止and 删除容器\n\n```bash\ndocker stop master\ndocker stop slave01\ndocker stop slave02\ndocker rm master\ndocker rm slave01\ndocker rm slave02\n```\n\n","slug":"hadoop-spark集群搭建","published":1,"updated":"2018-08-10T16:07:47.069Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43x4004n2tpbq8i892dm"},{"title":"HBase容器的搭建","date":"2018-07-08T16:01:32.819Z","toc":true,"typora-copy-images-to":"ipic","_content":"\n\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu56b6k2qjj316y0s0jtg.jpg)\n\n# 创建hbase镜像\n\n##   拷贝源码\n\n\n```bash\ndocker cp /Users/yaosong/Yao/hbase 8019587d559b:/usr/\ndocker cp /Users/yaosong/Yao/zk   8019587d559b:/usr/\ndocker cp /Users/yaosong/Yao/hbasezkStart.sh 8019587d559b:/usr/\ndocker cp /Users/yaosong/Yao/hbase-start.sh 8019587d559b:/usr/\ndocker cp /Users/yaosong/Yao/zk-start.sh 8019587d559b:/usr/\n```\n\n参考https://www.cnblogs.com/netbloomy/p/6677883.html\n\n<!--more-->\n\n## 解压\n\n`tar -zxvf hbase-1.3.0-bin.tar.gz`\n进入 hbase 的配置目录，在 hbase-env.sh 文件里面加入 java 环境变量.\n\n 即：\n\n```bash\nvim  hbase-env.sh\nexport JAVA_HOME=JAVA_HOME=/usr/java/jdk\n```\n\n关闭 HBase 自带的 Zookeeper, 使用 Zookeeper 集群：\n\n```\nvim  hbase-env.sh\nexport  HBASE_MANAGES_ZK=false\n```\n\nhbase-site.xml\n\n```xml\n<configuration>\n    <property>\n        <name>hbase.rootdir</name>\n        <value>hdfs://master:9000/hbase</value>\n    </property>\n    <property>\n        <name>hbase.cluster.distributed</name>\n        <value>true</value>\n    </property>\n    <property>\n        <name>hbase.zookeeper.quorum</name>\n        <value>hbasezk1,hbasezk2,hbasezk3</value>\n    </property>\n    <property>\n        <name>hbase.zookeeper.property.dataDir</name>\n        <value>/usr/hbase/tmp/zk/data</value>\n    </property>\n    <!-- webui的配置 -->\n    <property>\n        <name>hbase.master.info.port</name>\n        <value>60010</value>\n    </property>\n    <!-- webui新增的配置 -->\n</configuration>\n```\n\n\n\n创建zk的datadir目录\n\n`mkdir -p /usr/hbase/tmp/zk/data`\n\n\n编辑配置目录下面的文件 regionservers. 命令：\n\n\tvim   $HBASE_HOME/config/regionservers\n\n\t加入如下内容：\n\thbasezk1\n\thbasezk2\n\thbasezk3\n\n\n\t把 Hbase 复制到其他机器scp\n\n\t开启 hbase 服务。命令如下： 哪台上运行哪台就为hmaster\n\n \t$HBASE_HOME/bin/start-hbase.sh\n\n\t在 hbasezk1,2,3 中的任意一台机器使用 $HBASE_HOME/bin/hbase shell\n\n\t进入 hbase 自带的 shell 环境，然后使用命令 version 等，进行查看 hbase 信息及建立表等操作。\n要配置 HBase 高可用的话，只需要启动两个 HMaster，让 Zookeeper 自己去选择一个 Master Acitve。\n","source":"_posts/hbasezk容器的搭建.md","raw":"---\ntitle:  HBase容器的搭建\ndate: 2018年08月06日 22时15分52秒\ntags:  [HBase,Docker]\ncategories: 安装部署\ntoc: true\ntypora-copy-images-to: ipic\n---\n\n\n[TOC]\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fu56b6k2qjj316y0s0jtg.jpg)\n\n# 创建hbase镜像\n\n##   拷贝源码\n\n\n```bash\ndocker cp /Users/yaosong/Yao/hbase 8019587d559b:/usr/\ndocker cp /Users/yaosong/Yao/zk   8019587d559b:/usr/\ndocker cp /Users/yaosong/Yao/hbasezkStart.sh 8019587d559b:/usr/\ndocker cp /Users/yaosong/Yao/hbase-start.sh 8019587d559b:/usr/\ndocker cp /Users/yaosong/Yao/zk-start.sh 8019587d559b:/usr/\n```\n\n参考https://www.cnblogs.com/netbloomy/p/6677883.html\n\n<!--more-->\n\n## 解压\n\n`tar -zxvf hbase-1.3.0-bin.tar.gz`\n进入 hbase 的配置目录，在 hbase-env.sh 文件里面加入 java 环境变量.\n\n 即：\n\n```bash\nvim  hbase-env.sh\nexport JAVA_HOME=JAVA_HOME=/usr/java/jdk\n```\n\n关闭 HBase 自带的 Zookeeper, 使用 Zookeeper 集群：\n\n```\nvim  hbase-env.sh\nexport  HBASE_MANAGES_ZK=false\n```\n\nhbase-site.xml\n\n```xml\n<configuration>\n    <property>\n        <name>hbase.rootdir</name>\n        <value>hdfs://master:9000/hbase</value>\n    </property>\n    <property>\n        <name>hbase.cluster.distributed</name>\n        <value>true</value>\n    </property>\n    <property>\n        <name>hbase.zookeeper.quorum</name>\n        <value>hbasezk1,hbasezk2,hbasezk3</value>\n    </property>\n    <property>\n        <name>hbase.zookeeper.property.dataDir</name>\n        <value>/usr/hbase/tmp/zk/data</value>\n    </property>\n    <!-- webui的配置 -->\n    <property>\n        <name>hbase.master.info.port</name>\n        <value>60010</value>\n    </property>\n    <!-- webui新增的配置 -->\n</configuration>\n```\n\n\n\n创建zk的datadir目录\n\n`mkdir -p /usr/hbase/tmp/zk/data`\n\n\n编辑配置目录下面的文件 regionservers. 命令：\n\n\tvim   $HBASE_HOME/config/regionservers\n\n\t加入如下内容：\n\thbasezk1\n\thbasezk2\n\thbasezk3\n\n\n\t把 Hbase 复制到其他机器scp\n\n\t开启 hbase 服务。命令如下： 哪台上运行哪台就为hmaster\n\n \t$HBASE_HOME/bin/start-hbase.sh\n\n\t在 hbasezk1,2,3 中的任意一台机器使用 $HBASE_HOME/bin/hbase shell\n\n\t进入 hbase 自带的 shell 环境，然后使用命令 version 等，进行查看 hbase 信息及建立表等操作。\n要配置 HBase 高可用的话，只需要启动两个 HMaster，让 Zookeeper 自己去选择一个 Master Acitve。\n","slug":"hbasezk容器的搭建","published":1,"updated":"2018-08-15T14:34:04.875Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43x5004r2tpbjrt5ctn3"},{"title":"Hue搭建","date":"2018-07-09T16:37:50.903Z","toc":true,"_content":"[TOC]\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu54s2yk2aj30z40dcmyl.jpg)\n# 本次采用的ant maven来编译hue\n\n## 启动一个基础容器\n`docker run -itd  --net=br  --name hue --hostname  hue yaosong5/centosbase:1.0 &> /dev/null`\n<!--more-->\n## 拷贝源包\n将ant、hue4.0.0、ant、maven等下载到本地结业后，再拷贝到容器（这样更快速）\n\n\tdocker cp  /Users/yaosong/Yao/ant   4115ea59088e:/\n\tdocker cp  /Users/yaosong/Yao/maven  4115ea59088e:/\n\tdocker cp  /Users/yaosong/Yao/hue4  4115ea59088e:/usr/\n\n## 配置HOME\n```bash\nvim ~/.bashrc\n加入\nMAVEN_HOME=/maven\nexport MAVEN_HOME\nexport PATH=${PATH}:${MAVEN_HOME}/bin\nANT_HOME=/ant\nPATH=$JAVA_HOME/bin:$ANT_HOME/bin:$PATH\nHUE_HOME=/hue4\n使其生效\nsource ~/.bashrc\n```\n## 安装依赖，编译hue需要安装一些依赖\n\n`yum install gmp-devel -y `\n> 参考 http://www.aizhuanji.com/a/0Vo0qEMW.html\n\n**若解决不了**\n```\nyum install asciidoc cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-plain gcc gcc-c++ krb5-devel libffi-devel libtidy libxml2-devel libxslt-devel make mysql-devel openldap-devel  sqlite-devel openssl-devel gmp-devel -y\n```\n\n> 参考链接：https://www.jianshu.com/p/417788238e3d\n\n## \b编译安装hue\n\n首先编译 Hue，并在要安装 Hue 的节点上创建 Hue 用户和 hue 组\n\n\n\n\n创建 Hue 用户\n```Bash\ngroupadd hue\nuseradd hue -g hue\ncd   $HUE_HOME\nchown -R hue:hue *\n```\n\n> 注：需要注意的是 hue 在编译时有两种方式:1.通过maven、ant编译 2.通过python编译（在centos6.5因为自身python为2.6.6版本和hue编译需要2.7版本会有一点小冲突，故采用1）两种方式都是在hue目录下 make apps，只是第一种方式要先配置maven、ant的环境而已\n\n```bash\ncd $HUE_HOME\nmake apps\n```\n> 参考 ：https://blog.csdn.net/u012802702/article/details/68071244\n\n如果报错\n\n```\n/usr/hue4/Makefile.vars:42: *** \"Error: must have python development packages for 2.6 or 2.7. Could not find Python.h. Please install python2.6-devel or python2.7-devel\".  Stop.\n```\n\n需执行\n\n```bash\n可以先查看一下含 python-devel 的包\nyum search python | grep python-devel\n64 位安装 python-devel.x86_64，32 位安装 python-devel.i686，我这里安装:\nsudo yum install python-devel.x86_64 -y\n```\n\n## 更改hue的配置文件\n\n`vim $HUE_HOME/desktop/conf/hue.ini`\n\n### mysql\n\n```Ini\n [[database]]\n  host=master\n  port=3306\n  engine=mysql\n  user=root\n  password=root\n  name=hue\n```\n\n### hive\n\n```ini\nhive_server_host=master\nhive_server_port=10000\nhive_conf_dir=$HIVE_HOME/conf\n```\n\n### hadoop-hdfs\n\n```ini\nfs_defaultfs=hdfs://master:9000\nlogical_name=master\nwebhdfs_url=http://master:50070/webhdfs/v1\nhadoop_hdfs_home=$HADOOP_HOME\nhadoop_conf_dir=$HADOOP_HOME/etc/hadoop\n```\n\n\n\n### hadoop-yarn\n\n在 [hadoop].[[yarn_clusters]].[[[default]]] 下\n\n```ini\nresourcemanager_host=master\nresourcemanager_port=8032\nresourcemanager_api_url=http://master:8088\nproxy_api_url=http://master:8088\n```\n\n### hbase\n\n\n在 [hbase] 节点下\n\n```ini\nhbase_clusters=(HBASE|master:9090)\nhbase_conf_dir=$HBASE_HOME/conf\nuse_doas=true\n```\n\n\n\n## 大数据各组件满足hue进行相应配置\n\n### 安装mysql\n\n由于需要hue需要存放一些元数据故安装mysql\n\n```bash\nyum install -y mysql-server\nservice mysqld start\nmysql -u root -p\nEnter password:           //默认密码为空，输入后回车即可\nset password for root@localhost=password('root'); 　　密码设置为root\n默认情况下Mysql只允许本地登录，所以只需配置root@localhost就好\nset password for root@%=password('root'); 　　　　　　密码设置为root （其实这一步可以不配）\nset password for root@master=password('root'); 　　密码设置为root （其实这一步可以不配）\n\nselect user,host,password from mysql.user;  　　查看密码是否设置成功\nGRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;\n\ncreate database hue;\n```\n\n### 报错DatabaseError\n\n> DatabaseError:(1146,\"Table 'hue.desktop_settings' doesn't exist\")-初始化mysql\n\n完成以上的这个配置，启动 Hue, 通过浏览器访问，会发生错误，原因是 mysql 数据库没有被初始化\n**DatabaseError: (1146,\"Table 'hue.desktop_settings' doesn't exist\")**\n执行以下指令对 hue 数据库进行初始化\n\n```bash\ncd $HUE_HOME/build/env/\nbin/hue syncdb\nbin/hue migrate\n```\n\n此外需要注意的是如果使用的是：\n`$HUE_HOME/build/env/bin/hue syncdb --noinput`\n\n则不会让输入初始的用户名和密码，只有在首次登录时才会让输入，作为超级管理员账户。\b\n\n### hdfs\n\n#### hdfs-site.xml\n增加一个值开启 hdfs 的 web 交互\n```xml\n <!--HUE 增加一个值开启 hdfs 的 web 交互-->\n<property>\n      <name>dfs.webhdfs.enabled</name>\n      <value>true</value>\n</property>\n <!--HUE 增加一个值开启 hdfs 的 web 交互-->\n```\n\n#### core-site.xml\n```xml\n<!--《为了让 hue 能够访问 hdfs，需要在 hdfs-site.xml 里面配置一些内容-->\n<property>\n    <name>hadoop.proxyuser.hue.hosts</name>\n    <value>*</value>\n</property>\n<property>\n    <name>hadoop.proxyuser.hue.groups</name>\n    <value>*</value>\n</property>\n<!--《为了让 hue 能够访问 hdfs，需要在 hdfs-site.xml 里面配置一些内容-->\n```\n\n### hbase\n\n**hbase-site.xml**\n\n```xml\n<!-- hue支持 -->\n<property>\n        <name>hbase.thrift.support.proxyuser</name>\n        <value>true</value>\n</property>\n<property>\n        <name>hbase.regionserver.thrift.http</name>\n        <value>true</value>\n</property>\n<!-- hue支持 -->\n```\n\nhue 访问 hbase 是用的 thriftserver，并且是 thrift1，不是 thrift2，所以要在 master 上面启动 thrif1\n\n```\n$HBASE_HOME/bin/hbase-daemon.sh start thrift\n```\n> 参考 https://blog.csdn.net/Dante_003/article/details/78889084\n\n### 读取hbase问题\n\n为解决访问\n**Failed to authenticate to HBase Thrift Server, check authentication configurations.**\n需要在hue的配置文件中配置\n\n```ini\nuse_doas=true\n```\n\n\n参考http://gethue.com/hbase-browsing-with-doas-impersonation-and-kerberos/\n\n若以上配置未能解决问题，还需要将core-site.xml拷贝到hbase/conf，并添加以下内容\n\n```xml\n<property>\n  <name>hadoop.proxyuser.hbase.hosts</name>\n  <value>*</value>\n</property>\n<property>\n  <name>hadoop.proxyuser.hbase.groups</name>\n  <value>*</value>\n</property>\n```\n\n\n\n> [参考]https://blog.csdn.net/u012802702/article/details/68071244\n\n### hive\n\nHue 与框架 Hive 的集成\n开启 Hive Remote MetaStore\n`nohup $HIVE_HOME/bin/hive --service metastore &`\n\nhive 只需启动 hiveserver2，thriftserver 的 10000 端口启动即可\n```bash\nnohup $HIVE_HOME/bin/hiveserver2 &\n或者\nnohup HIVE_HOME/bin/hive --service hiveserver2 &\n```\n\n### 解决 hue ui 界面查询中文乱码问题\n\n在 `[[[mysql]]] `节点下\n\n```ini\noptions={\"init_command\":\"SET NAMES'utf8'\"}\n```\n\n> [参考]\n> https://blog.csdn.net/u012802702/article/details/68071244\n\n\n\n## 依赖的组件启动\n\n### Mysql\n\n`service mysqld start`\n\n### hadoop\n\n`start-all.sh`\n\n### hive\n\n然后需要同时启动 hive 的 metastore 和 hiveserve2\n\n```bash\nnohup hive --service metastore &\nnohup hive --service hiveserver2 &\n```\n\n### hbase\n\nHue 需要读取 HBase 的数据是使用 thrift 的方式，默认 HBase 的 thrift 服务没有开启，所有需要手动额外开启 thrift 服务。\n\n启动 thrift service\n`$HBASE_HOME/bin/hbase-daemon.sh start thrift`\n\nthrift service 默认使用的是 9090 端口，使用如下命令查看端口是否被占用\n\n`netstat -nl|grep 9090`\n\n#### 依赖启动的脚本\n\n```bash\n#!/bin/bash\n#启动mysql\nservice mysqld start\n#启动hadoop\nsh /hadoop-start.sh\n#启动hive\nsh /hive-start-servers2.sh\n#启动 thrift service\n$HBASE_HOME/bin/hbase-daemon.sh start thrift\n#启动hue\nnohup $HUE_HOME/build/env/bin/supervisor &\n```\n\n\n## hue启动命令\n```bash\n$HUE_HOME/build/env/bin/supervisor &\n```\n\n(注：想要后台执行就是 **$HUE_HOME/build/env/bin/supervisor &** )\n\n或者\n\n```Bash\n$HUE_HOME/build/env/bin/hue runserver_plus 0.0.0.0:8888\n```\n\n>\n> 【参考】https://blog.csdn.net/hexinghua0126/article/details/80338779\n>\n\n*hue的\bweb服务端口：8888*\n\n\n\n## hue停止命令\n\n`pkill -U hue`\n\n# 报错\n\n1、如果修改配置文件后，启动后无法进人 hue 界面\n\n```\n可能是配置文件被锁住了\ncd $HUE_HOME/desktop/conf\nls –a\nrm –rf hue.ini.swp\n或者 hadoop、hive 等服务没有启动起来\n```\n\n2、在 hue\b界面异常，导致 hive 无法使用\n安装插件：\n`yum install cyrus-sasl-plain  cyrus-sasl-devel  cyrus-sasl-gssapi`\n\n# 操作镜像\n\n## 保存为镜像\n`docker commit -m \"hue\"  hue   yaosong5/hue4:1.0`\n\n## 创建容器\n`docker run -itd  --net=br  --name gethue --hostname gethue gethue/hue:latest &> /dev/null`\n\b映射宿主机的\b\bhosts文件及其hue的配置文件方式启动容器\n```\ndocker run --name=hue -d --net=br\n -v /etc/hosts/:/etc/hosts -v $PWD/pseudo-distributed.ini:/hue/desktop/conf/pseudo-distributed.ini \tyaosong5/hue4:1.0\n```\n\n--net=br为\b了宿主机和容器之前ip自由访问所搭建的网络模式，如有需求\b请参考\n\n**其他参考**\n\n```bash\ndocker run --name=hue -d --net=br -v /etc/hosts/:/etc/hosts -v $PWD/pseudo-distributed.ini:/hue/desktop/conf/pseudo-distributed.ini gethue/hue:latest\n```\n> 参考：https://blog.csdn.net/Dante_003/article/details/78889084\n","source":"_posts/hue搭建.md","raw":"---\ntitle:  Hue搭建\ndate: 2018年06月21日 22时15分52秒\ntags:  [Docker,Hue]\ncategories: 安装部署\ntoc: true\n---\n[TOC]\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu54s2yk2aj30z40dcmyl.jpg)\n# 本次采用的ant maven来编译hue\n\n## 启动一个基础容器\n`docker run -itd  --net=br  --name hue --hostname  hue yaosong5/centosbase:1.0 &> /dev/null`\n<!--more-->\n## 拷贝源包\n将ant、hue4.0.0、ant、maven等下载到本地结业后，再拷贝到容器（这样更快速）\n\n\tdocker cp  /Users/yaosong/Yao/ant   4115ea59088e:/\n\tdocker cp  /Users/yaosong/Yao/maven  4115ea59088e:/\n\tdocker cp  /Users/yaosong/Yao/hue4  4115ea59088e:/usr/\n\n## 配置HOME\n```bash\nvim ~/.bashrc\n加入\nMAVEN_HOME=/maven\nexport MAVEN_HOME\nexport PATH=${PATH}:${MAVEN_HOME}/bin\nANT_HOME=/ant\nPATH=$JAVA_HOME/bin:$ANT_HOME/bin:$PATH\nHUE_HOME=/hue4\n使其生效\nsource ~/.bashrc\n```\n## 安装依赖，编译hue需要安装一些依赖\n\n`yum install gmp-devel -y `\n> 参考 http://www.aizhuanji.com/a/0Vo0qEMW.html\n\n**若解决不了**\n```\nyum install asciidoc cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-plain gcc gcc-c++ krb5-devel libffi-devel libtidy libxml2-devel libxslt-devel make mysql-devel openldap-devel  sqlite-devel openssl-devel gmp-devel -y\n```\n\n> 参考链接：https://www.jianshu.com/p/417788238e3d\n\n## \b编译安装hue\n\n首先编译 Hue，并在要安装 Hue 的节点上创建 Hue 用户和 hue 组\n\n\n\n\n创建 Hue 用户\n```Bash\ngroupadd hue\nuseradd hue -g hue\ncd   $HUE_HOME\nchown -R hue:hue *\n```\n\n> 注：需要注意的是 hue 在编译时有两种方式:1.通过maven、ant编译 2.通过python编译（在centos6.5因为自身python为2.6.6版本和hue编译需要2.7版本会有一点小冲突，故采用1）两种方式都是在hue目录下 make apps，只是第一种方式要先配置maven、ant的环境而已\n\n```bash\ncd $HUE_HOME\nmake apps\n```\n> 参考 ：https://blog.csdn.net/u012802702/article/details/68071244\n\n如果报错\n\n```\n/usr/hue4/Makefile.vars:42: *** \"Error: must have python development packages for 2.6 or 2.7. Could not find Python.h. Please install python2.6-devel or python2.7-devel\".  Stop.\n```\n\n需执行\n\n```bash\n可以先查看一下含 python-devel 的包\nyum search python | grep python-devel\n64 位安装 python-devel.x86_64，32 位安装 python-devel.i686，我这里安装:\nsudo yum install python-devel.x86_64 -y\n```\n\n## 更改hue的配置文件\n\n`vim $HUE_HOME/desktop/conf/hue.ini`\n\n### mysql\n\n```Ini\n [[database]]\n  host=master\n  port=3306\n  engine=mysql\n  user=root\n  password=root\n  name=hue\n```\n\n### hive\n\n```ini\nhive_server_host=master\nhive_server_port=10000\nhive_conf_dir=$HIVE_HOME/conf\n```\n\n### hadoop-hdfs\n\n```ini\nfs_defaultfs=hdfs://master:9000\nlogical_name=master\nwebhdfs_url=http://master:50070/webhdfs/v1\nhadoop_hdfs_home=$HADOOP_HOME\nhadoop_conf_dir=$HADOOP_HOME/etc/hadoop\n```\n\n\n\n### hadoop-yarn\n\n在 [hadoop].[[yarn_clusters]].[[[default]]] 下\n\n```ini\nresourcemanager_host=master\nresourcemanager_port=8032\nresourcemanager_api_url=http://master:8088\nproxy_api_url=http://master:8088\n```\n\n### hbase\n\n\n在 [hbase] 节点下\n\n```ini\nhbase_clusters=(HBASE|master:9090)\nhbase_conf_dir=$HBASE_HOME/conf\nuse_doas=true\n```\n\n\n\n## 大数据各组件满足hue进行相应配置\n\n### 安装mysql\n\n由于需要hue需要存放一些元数据故安装mysql\n\n```bash\nyum install -y mysql-server\nservice mysqld start\nmysql -u root -p\nEnter password:           //默认密码为空，输入后回车即可\nset password for root@localhost=password('root'); 　　密码设置为root\n默认情况下Mysql只允许本地登录，所以只需配置root@localhost就好\nset password for root@%=password('root'); 　　　　　　密码设置为root （其实这一步可以不配）\nset password for root@master=password('root'); 　　密码设置为root （其实这一步可以不配）\n\nselect user,host,password from mysql.user;  　　查看密码是否设置成功\nGRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;\n\ncreate database hue;\n```\n\n### 报错DatabaseError\n\n> DatabaseError:(1146,\"Table 'hue.desktop_settings' doesn't exist\")-初始化mysql\n\n完成以上的这个配置，启动 Hue, 通过浏览器访问，会发生错误，原因是 mysql 数据库没有被初始化\n**DatabaseError: (1146,\"Table 'hue.desktop_settings' doesn't exist\")**\n执行以下指令对 hue 数据库进行初始化\n\n```bash\ncd $HUE_HOME/build/env/\nbin/hue syncdb\nbin/hue migrate\n```\n\n此外需要注意的是如果使用的是：\n`$HUE_HOME/build/env/bin/hue syncdb --noinput`\n\n则不会让输入初始的用户名和密码，只有在首次登录时才会让输入，作为超级管理员账户。\b\n\n### hdfs\n\n#### hdfs-site.xml\n增加一个值开启 hdfs 的 web 交互\n```xml\n <!--HUE 增加一个值开启 hdfs 的 web 交互-->\n<property>\n      <name>dfs.webhdfs.enabled</name>\n      <value>true</value>\n</property>\n <!--HUE 增加一个值开启 hdfs 的 web 交互-->\n```\n\n#### core-site.xml\n```xml\n<!--《为了让 hue 能够访问 hdfs，需要在 hdfs-site.xml 里面配置一些内容-->\n<property>\n    <name>hadoop.proxyuser.hue.hosts</name>\n    <value>*</value>\n</property>\n<property>\n    <name>hadoop.proxyuser.hue.groups</name>\n    <value>*</value>\n</property>\n<!--《为了让 hue 能够访问 hdfs，需要在 hdfs-site.xml 里面配置一些内容-->\n```\n\n### hbase\n\n**hbase-site.xml**\n\n```xml\n<!-- hue支持 -->\n<property>\n        <name>hbase.thrift.support.proxyuser</name>\n        <value>true</value>\n</property>\n<property>\n        <name>hbase.regionserver.thrift.http</name>\n        <value>true</value>\n</property>\n<!-- hue支持 -->\n```\n\nhue 访问 hbase 是用的 thriftserver，并且是 thrift1，不是 thrift2，所以要在 master 上面启动 thrif1\n\n```\n$HBASE_HOME/bin/hbase-daemon.sh start thrift\n```\n> 参考 https://blog.csdn.net/Dante_003/article/details/78889084\n\n### 读取hbase问题\n\n为解决访问\n**Failed to authenticate to HBase Thrift Server, check authentication configurations.**\n需要在hue的配置文件中配置\n\n```ini\nuse_doas=true\n```\n\n\n参考http://gethue.com/hbase-browsing-with-doas-impersonation-and-kerberos/\n\n若以上配置未能解决问题，还需要将core-site.xml拷贝到hbase/conf，并添加以下内容\n\n```xml\n<property>\n  <name>hadoop.proxyuser.hbase.hosts</name>\n  <value>*</value>\n</property>\n<property>\n  <name>hadoop.proxyuser.hbase.groups</name>\n  <value>*</value>\n</property>\n```\n\n\n\n> [参考]https://blog.csdn.net/u012802702/article/details/68071244\n\n### hive\n\nHue 与框架 Hive 的集成\n开启 Hive Remote MetaStore\n`nohup $HIVE_HOME/bin/hive --service metastore &`\n\nhive 只需启动 hiveserver2，thriftserver 的 10000 端口启动即可\n```bash\nnohup $HIVE_HOME/bin/hiveserver2 &\n或者\nnohup HIVE_HOME/bin/hive --service hiveserver2 &\n```\n\n### 解决 hue ui 界面查询中文乱码问题\n\n在 `[[[mysql]]] `节点下\n\n```ini\noptions={\"init_command\":\"SET NAMES'utf8'\"}\n```\n\n> [参考]\n> https://blog.csdn.net/u012802702/article/details/68071244\n\n\n\n## 依赖的组件启动\n\n### Mysql\n\n`service mysqld start`\n\n### hadoop\n\n`start-all.sh`\n\n### hive\n\n然后需要同时启动 hive 的 metastore 和 hiveserve2\n\n```bash\nnohup hive --service metastore &\nnohup hive --service hiveserver2 &\n```\n\n### hbase\n\nHue 需要读取 HBase 的数据是使用 thrift 的方式，默认 HBase 的 thrift 服务没有开启，所有需要手动额外开启 thrift 服务。\n\n启动 thrift service\n`$HBASE_HOME/bin/hbase-daemon.sh start thrift`\n\nthrift service 默认使用的是 9090 端口，使用如下命令查看端口是否被占用\n\n`netstat -nl|grep 9090`\n\n#### 依赖启动的脚本\n\n```bash\n#!/bin/bash\n#启动mysql\nservice mysqld start\n#启动hadoop\nsh /hadoop-start.sh\n#启动hive\nsh /hive-start-servers2.sh\n#启动 thrift service\n$HBASE_HOME/bin/hbase-daemon.sh start thrift\n#启动hue\nnohup $HUE_HOME/build/env/bin/supervisor &\n```\n\n\n## hue启动命令\n```bash\n$HUE_HOME/build/env/bin/supervisor &\n```\n\n(注：想要后台执行就是 **$HUE_HOME/build/env/bin/supervisor &** )\n\n或者\n\n```Bash\n$HUE_HOME/build/env/bin/hue runserver_plus 0.0.0.0:8888\n```\n\n>\n> 【参考】https://blog.csdn.net/hexinghua0126/article/details/80338779\n>\n\n*hue的\bweb服务端口：8888*\n\n\n\n## hue停止命令\n\n`pkill -U hue`\n\n# 报错\n\n1、如果修改配置文件后，启动后无法进人 hue 界面\n\n```\n可能是配置文件被锁住了\ncd $HUE_HOME/desktop/conf\nls –a\nrm –rf hue.ini.swp\n或者 hadoop、hive 等服务没有启动起来\n```\n\n2、在 hue\b界面异常，导致 hive 无法使用\n安装插件：\n`yum install cyrus-sasl-plain  cyrus-sasl-devel  cyrus-sasl-gssapi`\n\n# 操作镜像\n\n## 保存为镜像\n`docker commit -m \"hue\"  hue   yaosong5/hue4:1.0`\n\n## 创建容器\n`docker run -itd  --net=br  --name gethue --hostname gethue gethue/hue:latest &> /dev/null`\n\b映射宿主机的\b\bhosts文件及其hue的配置文件方式启动容器\n```\ndocker run --name=hue -d --net=br\n -v /etc/hosts/:/etc/hosts -v $PWD/pseudo-distributed.ini:/hue/desktop/conf/pseudo-distributed.ini \tyaosong5/hue4:1.0\n```\n\n--net=br为\b了宿主机和容器之前ip自由访问所搭建的网络模式，如有需求\b请参考\n\n**其他参考**\n\n```bash\ndocker run --name=hue -d --net=br -v /etc/hosts/:/etc/hosts -v $PWD/pseudo-distributed.ini:/hue/desktop/conf/pseudo-distributed.ini gethue/hue:latest\n```\n> 参考：https://blog.csdn.net/Dante_003/article/details/78889084\n","slug":"hue搭建","published":1,"updated":"2018-08-10T17:44:19.068Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43x7004v2tpbycdcdukg"},{"title":"jsontool使用","date":"2018-05-30T01:36:02.864Z","toc":true,"_content":"\njson-tool使用：`java -jar json-tool.jar \"json文件目录\" \"jsonPath路径\"`\n示例：\n\n```\njava -jar /Users/yaosong/Documents/json-tool.jar \"/Users/yaosong/tmp/access_report_data_by_token.json\"  \"$.report_data.behavior_check[?(@.check_point_cn == '朋友圈在哪里')].evidence\"\n```\n![示例截图](https://www.github.com/yaosong5/tuchuang/raw/master/mdtc/2018/5/30/1527644586687.jpg)","source":"_posts/json-tool使用.md","raw":"---\ntitle:  jsontool使用\ndate: 2018年06月21日 22时15分52秒\ntags:  [命令]\ncategories: tool\ntoc: true\n---\n\njson-tool使用：`java -jar json-tool.jar \"json文件目录\" \"jsonPath路径\"`\n示例：\n\n```\njava -jar /Users/yaosong/Documents/json-tool.jar \"/Users/yaosong/tmp/access_report_data_by_token.json\"  \"$.report_data.behavior_check[?(@.check_point_cn == '朋友圈在哪里')].evidence\"\n```\n![示例截图](https://www.github.com/yaosong5/tuchuang/raw/master/mdtc/2018/5/30/1527644586687.jpg)","slug":"json-tool使用","published":1,"updated":"2018-08-10T17:46:56.174Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43x8004y2tpbudzsj4bv"},{"title":"kafka的启动及常用命令","date":"2018-07-08T15:58:00.558Z","toc":true,"_content":"# kafka的启动脚本\n\n \tkafka-startall.sh\n\n```bash\n#!/bin/bash\nsed -e '1c borker.id=0' $KAFKA_HOME/config/server.properties\n$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties\nssh root@slave01 \"sed -i '1c borker.id=1 ' $KAFKA_HOME/config/server.properties\"\nssh root@slave01 \"sed -i '5c host.name=slave01 ' $KAFKA_HOME/config/server.properties\"\nssh root@slave01 \"sed -i '6c advertised.host.name=slave01 ' $KAFKA_HOME/config/server.properties\"\nssh root@slave01 \"$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties\"\nssh root@slave02 \"sed -i '1c borker.id=2 ' $KAFKA_HOME/config/server.properties\"\nssh root@slave02 \"sed -i '5c host.name=slave02 ' $KAFKA_HOME/config/server.properties\"\nssh root@slave02 \"sed -i '6c advertised.host.name=slave02 ' $KAFKA_HOME/config/server.properties\"\nssh root@slave02 \"$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties\"\n```\n<!--more -->\n\n# kafka的shell命令\n\njps -ml 查看kafka的运行情况\n\n## 启动\n\n```bash\nnohup  $KAFKA_HOME/bin/kafka-server-start.sh  $KAFKA_HOME/config/server.properties > /dev/null 2>&1 &\n/usr/kafka/bin/kafka-server-start.sh -daemon /usr/kafka/config/server.properties\n```\n\n## 创建 topic\n\n```bash\n$KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper zk1:2181 --replication-factor 2 --partitions 3 --topic shuaige\n```\n\n## 查看消费位置\n\n```bash\nsh  $KAFKA_HOME/bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper zk1:2181 --group testGroup\n\n```\n\n## 查看某个 Topic 的详情\n\n```bash\nsh  $KAFKA_HOME/bin/kafka-topics.sh --topic test --describe --zookeeper zk1:2181\n\n```\n\n## 创建生产者\n\n```bash\n$KAFKA_HOME/bin/kafka-console-producer.sh --broker-list kafka1:9092 --topic shuaige\n```\n\n## 创建消费者\n\n```bash\n$KAFKA_HOME/bin/kafka-console-consumer.sh --zookeeper zk1:2181 --topic shuaige --from-beginning\n\n```\n\n## 查看所有 topic\n\nzk 表示 zk 的地址 地址表示 zookeeper 的地址\n\n```Bash\n$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper  zk1:2181\n```\n\n## 删除 topic\n\n```bash\nsh $KAFKA_HOME/bin/kafka-topics.sh --delete --zookeeper zk1:2181 --topic test\n```","source":"_posts/kafka启动脚本及命令.md","raw":"---\ntitle:  kafka的启动及常用命令\ndate: 2018年06月21日 22时15分52秒\ntags:  [Kafka]\ncategories: 大数据\ntoc: true\n---\n# kafka的启动脚本\n\n \tkafka-startall.sh\n\n```bash\n#!/bin/bash\nsed -e '1c borker.id=0' $KAFKA_HOME/config/server.properties\n$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties\nssh root@slave01 \"sed -i '1c borker.id=1 ' $KAFKA_HOME/config/server.properties\"\nssh root@slave01 \"sed -i '5c host.name=slave01 ' $KAFKA_HOME/config/server.properties\"\nssh root@slave01 \"sed -i '6c advertised.host.name=slave01 ' $KAFKA_HOME/config/server.properties\"\nssh root@slave01 \"$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties\"\nssh root@slave02 \"sed -i '1c borker.id=2 ' $KAFKA_HOME/config/server.properties\"\nssh root@slave02 \"sed -i '5c host.name=slave02 ' $KAFKA_HOME/config/server.properties\"\nssh root@slave02 \"sed -i '6c advertised.host.name=slave02 ' $KAFKA_HOME/config/server.properties\"\nssh root@slave02 \"$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties\"\n```\n<!--more -->\n\n# kafka的shell命令\n\njps -ml 查看kafka的运行情况\n\n## 启动\n\n```bash\nnohup  $KAFKA_HOME/bin/kafka-server-start.sh  $KAFKA_HOME/config/server.properties > /dev/null 2>&1 &\n/usr/kafka/bin/kafka-server-start.sh -daemon /usr/kafka/config/server.properties\n```\n\n## 创建 topic\n\n```bash\n$KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper zk1:2181 --replication-factor 2 --partitions 3 --topic shuaige\n```\n\n## 查看消费位置\n\n```bash\nsh  $KAFKA_HOME/bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper zk1:2181 --group testGroup\n\n```\n\n## 查看某个 Topic 的详情\n\n```bash\nsh  $KAFKA_HOME/bin/kafka-topics.sh --topic test --describe --zookeeper zk1:2181\n\n```\n\n## 创建生产者\n\n```bash\n$KAFKA_HOME/bin/kafka-console-producer.sh --broker-list kafka1:9092 --topic shuaige\n```\n\n## 创建消费者\n\n```bash\n$KAFKA_HOME/bin/kafka-console-consumer.sh --zookeeper zk1:2181 --topic shuaige --from-beginning\n\n```\n\n## 查看所有 topic\n\nzk 表示 zk 的地址 地址表示 zookeeper 的地址\n\n```Bash\n$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper  zk1:2181\n```\n\n## 删除 topic\n\n```bash\nsh $KAFKA_HOME/bin/kafka-topics.sh --delete --zookeeper zk1:2181 --topic test\n```","slug":"kafka启动脚本及命令","published":1,"updated":"2018-08-14T11:36:16.094Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43xa00522tpbfuxqfg3a"},{"title":"spring集成权限校验","date":"2018-05-08T10:34:55.000Z","toc":true,"_content":"# shiro简介  \nshiro是权限控制的一个框架\t\t\n是一个强大易用的Java安全框架，提供了认证、授权、加密和会话管理功能，可为任何应用提供安全保障 - 从命令行应用、移动应用到大型网络及企业应用。\n<!-- more -->\n\n### **权限控制的方式**   \n权限有四种实现方式   \n注解(基于代理),url拦截(基于过滤器),shiro标签库(基于标签),编写代码(及其不推荐)   \n**不论哪种方式:都需要引入spring用于整合shiro的过滤器  **   \nweb.xml中:DelegatingFilterProxy=>spring整合shiro\n配置spring提供的用于整合shiro框架的过滤器\n```xml\n  <filter>\n    <filter-name>shiroFilter</filter-name>\n    <filter-class>org.springframework.web.filter.DelegatingFilterProxy\n   </fileter>\n```  \nfilet-name需要和**spring配置文件**中的一个BEAN对象的id保持一致**非常重要**  \n\n###  配置   \nI. 注解方式,注解是利用生成的代理对象来完成权限校验:   \nspring框架会为当前action对象(加注解的action)创建一个代理对象,如果有权限,就执行这个方法,不然就会报**异常**\n(将spring,Strust配置文件丰富:添加权限的注解,struts添加捕获异常,跳转页面)  \n 1. 需要在spring配置文件中进行配置开启注解**DefaultAdvisorAutoProxyCreator**,\n  并配置成cjlib方式的注解   \n```xml\n<property name=\"proxyTargetClass\" value=\"true\">\\</property>\n```\n注解实现权限当为jdk模式的时候    \n方法注解实现权限过滤  \n抛异常的原因:因为如果是jdk方式的话,实现的接口modelDriven只有一个getModel方法      \n所以不能进行对除该方法外其他方法进行注解\n\n 2.  定义切面类**AuthorizationAttributeSourceAdvisor**\n ```xml\n <bean id=\"authorizationAttributeSourceAdvisor\" class=\"org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor\"></bean>\n ```\n\n 3. 在需要权限才能访问的方法上添加注解\n   ```java\n  @RequiresPermissions(\"relo_delete这是权限名称\")   \n  ```  \n\nII.  url拦截(springxml)\n  基于过滤器或者拦截器实现   \n```xml\n<bean id=\"shiroFilter\" class=\"org.apache.shiro.spring.web.ShiroFilterFactoryBean\">\n\t\t<property name=\"securityManager\" ref=\"securityManager\"/>\n\t\t<property name=\"loginUrl\" value=\"/login.jsp\"/>\n\t\t<property name=\"unauthorizedUrl\" value=\"/unauthorized.jsp\"/>\n\t\t<property name=\"filterChainDefinitions\">\n\t\t\t<value>\n\t\t\t\t/css/** = anon\n\t\t\t\t/js/** = anon\n\t\t\t\t/images/** = anon\n\t\t\t\t/validatecode.jsp* = anon\n\t\t\t\t/login.jsp* = anon\n\t\t\t\t/userAction_login.action = anon\n\t\t\t\t/page_base_staff.action = perms[\"staff\"]\n\t\t\t\t/** = authc\n\t\t\t\t<!--/** = authc-->\n\t\t\t</value>\n\t\t</property>\n\t</bean>\n\t<!--开启自动代理,并且将代理代理模式设置为cjlib-->\n\t<bean id=\"defaultAdvisorAutoProxyCreator\" class=\"org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator\">\n\t\t<!--设置成cglib方式-->\n\t\t<property name=\"proxyTargetClass\" value=\"true\"></property>\n\t</bean>\n```\n![shiro各种过滤器简写](http://img.blog.csdn.net/20170109205450918?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzQ1MTM0Nw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n\n\n#  shiro的使用 \t\t\n\n1. 在web.xml中引入用于创建shiro框架的过滤器\nweb.xml中:DelegatingFilterProxy=>spring整合shiro   \n`注意引入的位置:要在struts核心过滤器的前面,StrutsPrepareAndExcutFilter,不然,所有请求会通过struts过滤器获直接访问得到,shiro的过滤器将不会起到作用`    \n\n2. 在Spring中整合shiro   \n  2.1).  shiro框架过滤器:**ShiroFilterFactoryBean** 需要声明那些过滤器,那些资源需要匹配那些过滤器,采用url拦截方式进行的路径对应的拦截器    \n  2.2).  配置安全管理器:**DefaultWebSecurityManager** 需要注入 自定义的Realm bean对象     \n\n  ```xml\n  <!--配置一个shiro框架的过滤器工厂bean,用于创建shiro框架的过滤器-->\n\t<bean id=\"shiroFilter\" class=\"org.apache.shiro.spring.web.ShiroFilterFactoryBean\">\n\t\t<property name=\"securityManager\" ref=\"securityManager\"/>\n\t\t<property name=\"loginUrl\" value=\"/login.jsp\"/>\n\t\t<property name=\"unauthorizedUrl\" value=\"/unauthorized.jsp\"/>\n\t\t<property name=\"filterChainDefinitions\">\n\t\t\t<value>\n\t\t\t\t/css/** = anon\n\t\t\t\t/js/** = anon\n\t\t\t\t/images/** = anon\n\t\t\t\t/validatecode.jsp* = anon\n\t\t\t\t/login.jsp* = anon\n\t\t\t\t/userAction_login.action = anon\n\t\t\t\t/page_base_staff.action = perms[\"staff\"]\n\t\t\t\t/** = authc\n\t\t\t\t<!--/** 表示所有/下所有路径,包括下面的所有路径-->\n        <!--/validatecode.jsp*\n         表示所有除了validatecode.jsp,还包括jsp后追加其他内容的.如validatecode.jsp?'+Math.random();防止验证码读取缓存\n\t\t\t</value>\n\t\t</property>\n\t</bean>\n\t<!--开启自动代理,并且将代理代理模式设置为cjlib\n  动态代理分为两类\n  基于jdk 创建的类必须要实现一个接口,这是面向接口的动态代理   \n  基于cjlib 创建的类不能用final修饰\n-->\n\t<bean id=\"defaultAdvisorAutoProxyCreator\" class=\"org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator\">\n\t\t<!--设置成cglib方式-->\n\t\t<property name=\"proxyTargetClass\" value=\"true\"></property>\n\t</bean>\n\t<!--定义aop通知+切入点-->\n\t<bean id=\"authorizationAttributeSourceAdvisor\" class=\"org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor\"></bean>\n\n\t<!--注入安全管理器-->\n\t<bean id=\"securityManager\" class=\"org.apache.shiro.web.mgt.DefaultWebSecurityManager\">\n\t\t<property name=\"realm\" ref=\"bosRealm\"></property>\n\t\t<property name=\"cacheManager\" ref=\"ehCacheManager\"></property>\n\t</bean>\n  ```\n\n3. 在登陆认证的方法中加入subject `controller中的login方法`\n```java\npublic String login(){\nSubject subject = SecurityUtils.getSubject();\n  //创建一个用户名密码令牌\n  AuthenticationToken token = new UsernamePasswordToken(getModel().getUsername(), MD5Utils.md5(\n          getModel().getPassword()));\n  try {\n    //认证\n      subject.login(token);\n  } catch (Exception e) {\n      this.addActionError(\"用户名或者密码错误\");\n      return LOGIN;\n  }\n  /*当通过认证,跳入主页*/\n  User user = (User) subject.getPrincipal();\n  /*将用户信息存入session*/\n  ServletActionContext.getRequest().getSession().setAttribute(\"currentUser\", user);\n  /*返回主页*/\n  return \"\";\n}\n```  \n\n4. 自定义Realm(用于权限的具体实施,即认证和授权)一般实现Realm接口的 **AuthorizingRealm** 实例    \n4.1实现认证 重写doGetAuthenticationInfo方法\n必须继承*AuthorizingRealm*    \n在需要交付给spring生成,并需要在安全注册管理器中注入属性Realm\n\n\n```java\nprotected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException {\n\nUsernamePasswordToken mytoken = (UsernamePasswordToken) token;\n       String username = mytoken.getUsername();\n       DetachedCriteria dc = DetachedCriteria.forClass(User.class);\n       dc.add(Restrictions.eq(\"username\",username));\n       List<User> list = userDao.findByCriteria(dc);\n       if(list != null && list.size() >0){\n           User user = list.get(0);\n           String dbPassword = user.getPassword();\n           AuthenticationInfo info = new SimpleAuthenticationInfo(user,dbPassword,this.getName());\n           return info;\n       }else{\n           return null;\n       }\n   }\n```\n4.2实现授权 重写doGetAuthorizationInfo方法\n\n\n\n```java\nprotected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) {\n/*获的简单授权对象,用于授权的*/\n  SimpleAuthorizationInfo info = new SimpleAuthorizationInfo();\n    /*授权staff权限*/\n    //info.addStringPermission(\"staff\");\n    //步骤获得授权对象,获得当前用户,获得当前用户的权限(若为admin即授予所有权限),当前用户授权\n    //获得对象\n    User user = (User)principals.getPrimaryPrincipal();\n    List<Function> fList = null;\n    //获得权限\n    if(user.getUsername().equals(\"admin\")){\n        fList = functionDao.findAll();\n    }else{\n        fList = functionDao.findFunctionByUserId(user.getId());\n    }\n    //授予权限\n    for(Function f : fList){\n        info.addStringPermission(f.getCode());\n}\n```\t\t\t\n\n## 关于Shiro中使用 **encache**     \n1.引入包   \n`在spring配置文件中配置以下`  \n2.配置文件ehcache.xml   \n```xml\n<ehcache xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"../config/ehcache.xsd\">\n    <defaultCache\n            maxElementsInMemory=\"10000\"\n            eternal=\"false\"\n            timeToIdleSeconds=\"120\"\n            timeToLiveSeconds=\"120\"\n            overflowToDisk=\"true\"\n            maxElementsOnDisk=\"10000000\"\n            diskPersistent=\"false\"\n            diskExpiryThreadIntervalSeconds=\"120\"\n            memoryStoreEvictionPolicy=\"LRU\"\n            />\n</ehcache>\n <!--eternal是否永久有效-->\n```\n3.引入缓存管理器**EhCacheManager**(shiro包中的),并设置配置文件;     \n4.将缓存管理器注入安全管理器**DefaultWebSecurityManager**\n```xml\n<!--注册安全管理器-->\n<bean id=\"securityManager\" class=\"org.apache.shiro.web.mgt.DefaultWebSecurityManager\">\n\t\t<property name=\"realm\" ref=\"bosRealm\"></property>\n\t\t<property name=\"cacheManager\" ref=\"ehCacheManager\"></property>\n\t</bean>\n\t<bean id=\"bosRealm\" class=\"org.yao.bos.web.action.realm.BOSRealm\"></bean>\n\t<!--注入缓存管理器-->\n\t<bean id=\"ehCacheManager\" class=\"org.apache.shiro.cache.ehcache.EhCacheManager\">\n\t\t<property name=\"cacheManagerConfigFile\" value=\"classpath:ehcache.xml\"></property>\n\t</bean>\n```\n\n","source":"_posts/spring集成权限校验.md","raw":"---\ntitle: spring集成权限校验\ndate: 2018-05-08 18:34:55\ntags: [技术,开发,Java]\ncategories: [Spring]\ntoc: true\n---\n# shiro简介  \nshiro是权限控制的一个框架\t\t\n是一个强大易用的Java安全框架，提供了认证、授权、加密和会话管理功能，可为任何应用提供安全保障 - 从命令行应用、移动应用到大型网络及企业应用。\n<!-- more -->\n\n### **权限控制的方式**   \n权限有四种实现方式   \n注解(基于代理),url拦截(基于过滤器),shiro标签库(基于标签),编写代码(及其不推荐)   \n**不论哪种方式:都需要引入spring用于整合shiro的过滤器  **   \nweb.xml中:DelegatingFilterProxy=>spring整合shiro\n配置spring提供的用于整合shiro框架的过滤器\n```xml\n  <filter>\n    <filter-name>shiroFilter</filter-name>\n    <filter-class>org.springframework.web.filter.DelegatingFilterProxy\n   </fileter>\n```  \nfilet-name需要和**spring配置文件**中的一个BEAN对象的id保持一致**非常重要**  \n\n###  配置   \nI. 注解方式,注解是利用生成的代理对象来完成权限校验:   \nspring框架会为当前action对象(加注解的action)创建一个代理对象,如果有权限,就执行这个方法,不然就会报**异常**\n(将spring,Strust配置文件丰富:添加权限的注解,struts添加捕获异常,跳转页面)  \n 1. 需要在spring配置文件中进行配置开启注解**DefaultAdvisorAutoProxyCreator**,\n  并配置成cjlib方式的注解   \n```xml\n<property name=\"proxyTargetClass\" value=\"true\">\\</property>\n```\n注解实现权限当为jdk模式的时候    \n方法注解实现权限过滤  \n抛异常的原因:因为如果是jdk方式的话,实现的接口modelDriven只有一个getModel方法      \n所以不能进行对除该方法外其他方法进行注解\n\n 2.  定义切面类**AuthorizationAttributeSourceAdvisor**\n ```xml\n <bean id=\"authorizationAttributeSourceAdvisor\" class=\"org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor\"></bean>\n ```\n\n 3. 在需要权限才能访问的方法上添加注解\n   ```java\n  @RequiresPermissions(\"relo_delete这是权限名称\")   \n  ```  \n\nII.  url拦截(springxml)\n  基于过滤器或者拦截器实现   \n```xml\n<bean id=\"shiroFilter\" class=\"org.apache.shiro.spring.web.ShiroFilterFactoryBean\">\n\t\t<property name=\"securityManager\" ref=\"securityManager\"/>\n\t\t<property name=\"loginUrl\" value=\"/login.jsp\"/>\n\t\t<property name=\"unauthorizedUrl\" value=\"/unauthorized.jsp\"/>\n\t\t<property name=\"filterChainDefinitions\">\n\t\t\t<value>\n\t\t\t\t/css/** = anon\n\t\t\t\t/js/** = anon\n\t\t\t\t/images/** = anon\n\t\t\t\t/validatecode.jsp* = anon\n\t\t\t\t/login.jsp* = anon\n\t\t\t\t/userAction_login.action = anon\n\t\t\t\t/page_base_staff.action = perms[\"staff\"]\n\t\t\t\t/** = authc\n\t\t\t\t<!--/** = authc-->\n\t\t\t</value>\n\t\t</property>\n\t</bean>\n\t<!--开启自动代理,并且将代理代理模式设置为cjlib-->\n\t<bean id=\"defaultAdvisorAutoProxyCreator\" class=\"org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator\">\n\t\t<!--设置成cglib方式-->\n\t\t<property name=\"proxyTargetClass\" value=\"true\"></property>\n\t</bean>\n```\n![shiro各种过滤器简写](http://img.blog.csdn.net/20170109205450918?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzQ1MTM0Nw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n\n\n#  shiro的使用 \t\t\n\n1. 在web.xml中引入用于创建shiro框架的过滤器\nweb.xml中:DelegatingFilterProxy=>spring整合shiro   \n`注意引入的位置:要在struts核心过滤器的前面,StrutsPrepareAndExcutFilter,不然,所有请求会通过struts过滤器获直接访问得到,shiro的过滤器将不会起到作用`    \n\n2. 在Spring中整合shiro   \n  2.1).  shiro框架过滤器:**ShiroFilterFactoryBean** 需要声明那些过滤器,那些资源需要匹配那些过滤器,采用url拦截方式进行的路径对应的拦截器    \n  2.2).  配置安全管理器:**DefaultWebSecurityManager** 需要注入 自定义的Realm bean对象     \n\n  ```xml\n  <!--配置一个shiro框架的过滤器工厂bean,用于创建shiro框架的过滤器-->\n\t<bean id=\"shiroFilter\" class=\"org.apache.shiro.spring.web.ShiroFilterFactoryBean\">\n\t\t<property name=\"securityManager\" ref=\"securityManager\"/>\n\t\t<property name=\"loginUrl\" value=\"/login.jsp\"/>\n\t\t<property name=\"unauthorizedUrl\" value=\"/unauthorized.jsp\"/>\n\t\t<property name=\"filterChainDefinitions\">\n\t\t\t<value>\n\t\t\t\t/css/** = anon\n\t\t\t\t/js/** = anon\n\t\t\t\t/images/** = anon\n\t\t\t\t/validatecode.jsp* = anon\n\t\t\t\t/login.jsp* = anon\n\t\t\t\t/userAction_login.action = anon\n\t\t\t\t/page_base_staff.action = perms[\"staff\"]\n\t\t\t\t/** = authc\n\t\t\t\t<!--/** 表示所有/下所有路径,包括下面的所有路径-->\n        <!--/validatecode.jsp*\n         表示所有除了validatecode.jsp,还包括jsp后追加其他内容的.如validatecode.jsp?'+Math.random();防止验证码读取缓存\n\t\t\t</value>\n\t\t</property>\n\t</bean>\n\t<!--开启自动代理,并且将代理代理模式设置为cjlib\n  动态代理分为两类\n  基于jdk 创建的类必须要实现一个接口,这是面向接口的动态代理   \n  基于cjlib 创建的类不能用final修饰\n-->\n\t<bean id=\"defaultAdvisorAutoProxyCreator\" class=\"org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator\">\n\t\t<!--设置成cglib方式-->\n\t\t<property name=\"proxyTargetClass\" value=\"true\"></property>\n\t</bean>\n\t<!--定义aop通知+切入点-->\n\t<bean id=\"authorizationAttributeSourceAdvisor\" class=\"org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor\"></bean>\n\n\t<!--注入安全管理器-->\n\t<bean id=\"securityManager\" class=\"org.apache.shiro.web.mgt.DefaultWebSecurityManager\">\n\t\t<property name=\"realm\" ref=\"bosRealm\"></property>\n\t\t<property name=\"cacheManager\" ref=\"ehCacheManager\"></property>\n\t</bean>\n  ```\n\n3. 在登陆认证的方法中加入subject `controller中的login方法`\n```java\npublic String login(){\nSubject subject = SecurityUtils.getSubject();\n  //创建一个用户名密码令牌\n  AuthenticationToken token = new UsernamePasswordToken(getModel().getUsername(), MD5Utils.md5(\n          getModel().getPassword()));\n  try {\n    //认证\n      subject.login(token);\n  } catch (Exception e) {\n      this.addActionError(\"用户名或者密码错误\");\n      return LOGIN;\n  }\n  /*当通过认证,跳入主页*/\n  User user = (User) subject.getPrincipal();\n  /*将用户信息存入session*/\n  ServletActionContext.getRequest().getSession().setAttribute(\"currentUser\", user);\n  /*返回主页*/\n  return \"\";\n}\n```  \n\n4. 自定义Realm(用于权限的具体实施,即认证和授权)一般实现Realm接口的 **AuthorizingRealm** 实例    \n4.1实现认证 重写doGetAuthenticationInfo方法\n必须继承*AuthorizingRealm*    \n在需要交付给spring生成,并需要在安全注册管理器中注入属性Realm\n\n\n```java\nprotected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException {\n\nUsernamePasswordToken mytoken = (UsernamePasswordToken) token;\n       String username = mytoken.getUsername();\n       DetachedCriteria dc = DetachedCriteria.forClass(User.class);\n       dc.add(Restrictions.eq(\"username\",username));\n       List<User> list = userDao.findByCriteria(dc);\n       if(list != null && list.size() >0){\n           User user = list.get(0);\n           String dbPassword = user.getPassword();\n           AuthenticationInfo info = new SimpleAuthenticationInfo(user,dbPassword,this.getName());\n           return info;\n       }else{\n           return null;\n       }\n   }\n```\n4.2实现授权 重写doGetAuthorizationInfo方法\n\n\n\n```java\nprotected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) {\n/*获的简单授权对象,用于授权的*/\n  SimpleAuthorizationInfo info = new SimpleAuthorizationInfo();\n    /*授权staff权限*/\n    //info.addStringPermission(\"staff\");\n    //步骤获得授权对象,获得当前用户,获得当前用户的权限(若为admin即授予所有权限),当前用户授权\n    //获得对象\n    User user = (User)principals.getPrimaryPrincipal();\n    List<Function> fList = null;\n    //获得权限\n    if(user.getUsername().equals(\"admin\")){\n        fList = functionDao.findAll();\n    }else{\n        fList = functionDao.findFunctionByUserId(user.getId());\n    }\n    //授予权限\n    for(Function f : fList){\n        info.addStringPermission(f.getCode());\n}\n```\t\t\t\n\n## 关于Shiro中使用 **encache**     \n1.引入包   \n`在spring配置文件中配置以下`  \n2.配置文件ehcache.xml   \n```xml\n<ehcache xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"../config/ehcache.xsd\">\n    <defaultCache\n            maxElementsInMemory=\"10000\"\n            eternal=\"false\"\n            timeToIdleSeconds=\"120\"\n            timeToLiveSeconds=\"120\"\n            overflowToDisk=\"true\"\n            maxElementsOnDisk=\"10000000\"\n            diskPersistent=\"false\"\n            diskExpiryThreadIntervalSeconds=\"120\"\n            memoryStoreEvictionPolicy=\"LRU\"\n            />\n</ehcache>\n <!--eternal是否永久有效-->\n```\n3.引入缓存管理器**EhCacheManager**(shiro包中的),并设置配置文件;     \n4.将缓存管理器注入安全管理器**DefaultWebSecurityManager**\n```xml\n<!--注册安全管理器-->\n<bean id=\"securityManager\" class=\"org.apache.shiro.web.mgt.DefaultWebSecurityManager\">\n\t\t<property name=\"realm\" ref=\"bosRealm\"></property>\n\t\t<property name=\"cacheManager\" ref=\"ehCacheManager\"></property>\n\t</bean>\n\t<bean id=\"bosRealm\" class=\"org.yao.bos.web.action.realm.BOSRealm\"></bean>\n\t<!--注入缓存管理器-->\n\t<bean id=\"ehCacheManager\" class=\"org.apache.shiro.cache.ehcache.EhCacheManager\">\n\t\t<property name=\"cacheManagerConfigFile\" value=\"classpath:ehcache.xml\"></property>\n\t</bean>\n```\n\n","slug":"spring集成权限校验","published":1,"updated":"2018-05-17T12:21:22.941Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43xb00562tpbeiwaah4f"},{"title":"Zookeeper的配置容器的搭建","date":"2018-08-06T19:06:03.170Z","toc":true,"typora-copy-images-to":"ipic","_content":"\n[TOC]\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu554uqh0pj31660fwta8.jpg)\n\n在usr目录下下载zk包，并且解压到/usr/目录，改名为zk，所以$ZK_HOME为/usr/zk\n\n# 创建目录\n\n```\nmkdir -p /usr/zk/data\nmkdir -p /usr/zk/logs\ntouch /usr/zk/data/myid\n```\n\n<!--more -->\n\n# 更改配置文件\n\n`vim  $ZK_HOME/conf/zoo.cfg`\n\n```\ndataDir=/usr/zk/data\ndataLogDir=/usr/zk/logs\nserver.1=zk1:2888:3888\nserver.2=zk2:2888:3888\nserver.3=zk3:2888:3888\n```\n\nzookeeper需要全部左右节点都启动才会选举leader，follower\n\n所有节点启动的脚本\n\n```bash\n# !/bin/bash\necho 1 > $ZK_HOME/data/myid\n$ZK_HOME/bin/zkServer.sh start\nssh root@zk2 \"echo 2 > /usr/zk/data/myid\"\nssh root@zk2 \"/usr/zk/bin/zkServer.sh start\"\nssh root@zk3 \"echo 3 > /usr/zk/data/myid\"\nssh root@zk3 \"/usr/zk/bin/zkServer.sh start\"\n```\n\n\n\n\n\n# zk问题\n\n1、由于 zk 运行一段时间后，会产生大量的日志文件，把磁盘空间占满，导致整个机器进程都不能活动了，所以需要定期清理这些日志文件，方法如下：\n\n1）、写一个脚本文件 cleanup.sh 内容如下：\n\n```\n java -cp zookeeper.jar:lib/slf4j-api-1.6.1.jar:lib/slf4j-log4j12-1.6.1.jar:lib/log4j-1.2.15.jar:conf org.apache.zookeeper.server.PurgeTxnLog <dataDir> <snapDir> -n <count>\n 其中：\n\n　　dataDir：即上面配置的 dataDir 的目录\n　　\n　　snapDir：即上面配置的 dataLogDir 的目录\n\n　　count：保留前几个日志文件，默认为 3\n\n\n```\n\n2）、通过 crontab 写定时任务，来完成定时清理日志的需求\n\n```\ncrontab -e 0 0 * *  /opt/zookeeper-3.4.10/bin/cleanup.sh\nHBase Master 高可用（HA）（http://www.cnblogs.com/captainlucky/p/4710642.html）\nHMaster 没有单点问题，HBase 中可以启动多个 HMaster，通过 Zookeeper 的 Master Election 机制保证总有一个 Master 运行。\n```\n\n所以这里要配置 HBase 高可用的话，只需要启动两个 HMaster，让 Zookeeper 自己去选择一个 Master Acitve。","source":"_posts/zookeeper配置.md","raw":"---\ntitle:  Zookeeper的配置容器的搭建\ndate: 2018年08月06日 22时15分52秒\ntags:  [zk,Docker]\ncategories: 安装部署\ntoc: true\ntypora-copy-images-to: ipic\n---\n\n[TOC]\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu554uqh0pj31660fwta8.jpg)\n\n在usr目录下下载zk包，并且解压到/usr/目录，改名为zk，所以$ZK_HOME为/usr/zk\n\n# 创建目录\n\n```\nmkdir -p /usr/zk/data\nmkdir -p /usr/zk/logs\ntouch /usr/zk/data/myid\n```\n\n<!--more -->\n\n# 更改配置文件\n\n`vim  $ZK_HOME/conf/zoo.cfg`\n\n```\ndataDir=/usr/zk/data\ndataLogDir=/usr/zk/logs\nserver.1=zk1:2888:3888\nserver.2=zk2:2888:3888\nserver.3=zk3:2888:3888\n```\n\nzookeeper需要全部左右节点都启动才会选举leader，follower\n\n所有节点启动的脚本\n\n```bash\n# !/bin/bash\necho 1 > $ZK_HOME/data/myid\n$ZK_HOME/bin/zkServer.sh start\nssh root@zk2 \"echo 2 > /usr/zk/data/myid\"\nssh root@zk2 \"/usr/zk/bin/zkServer.sh start\"\nssh root@zk3 \"echo 3 > /usr/zk/data/myid\"\nssh root@zk3 \"/usr/zk/bin/zkServer.sh start\"\n```\n\n\n\n\n\n# zk问题\n\n1、由于 zk 运行一段时间后，会产生大量的日志文件，把磁盘空间占满，导致整个机器进程都不能活动了，所以需要定期清理这些日志文件，方法如下：\n\n1）、写一个脚本文件 cleanup.sh 内容如下：\n\n```\n java -cp zookeeper.jar:lib/slf4j-api-1.6.1.jar:lib/slf4j-log4j12-1.6.1.jar:lib/log4j-1.2.15.jar:conf org.apache.zookeeper.server.PurgeTxnLog <dataDir> <snapDir> -n <count>\n 其中：\n\n　　dataDir：即上面配置的 dataDir 的目录\n　　\n　　snapDir：即上面配置的 dataLogDir 的目录\n\n　　count：保留前几个日志文件，默认为 3\n\n\n```\n\n2）、通过 crontab 写定时任务，来完成定时清理日志的需求\n\n```\ncrontab -e 0 0 * *  /opt/zookeeper-3.4.10/bin/cleanup.sh\nHBase Master 高可用（HA）（http://www.cnblogs.com/captainlucky/p/4710642.html）\nHMaster 没有单点问题，HBase 中可以启动多个 HMaster，通过 Zookeeper 的 Master Election 机制保证总有一个 Master 运行。\n```\n\n所以这里要配置 HBase 高可用的话，只需要启动两个 HMaster，让 Zookeeper 自己去选择一个 Master Acitve。","slug":"zookeeper配置","published":1,"updated":"2018-08-10T17:58:15.529Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43xd005a2tpbk1q3b7b5"},{"title":"源文件提交到仓库","date":"2018-05-21T17:43:07.025Z","toc":true,"_content":"将博客源文件加入到仓库\n>git add .\ngit commit -m \" \"\ngit push origin master\ngit push origin master -f\n\n<!-- more -->","source":"_posts/博客源文件提交到仓库.md","raw":"---\ntitle:  源文件提交到仓库\ndate: \ntags:  [Hexo,git]\ncategories: 博客\ntoc: true\n---\n将博客源文件加入到仓库\n>git add .\ngit commit -m \" \"\ngit push origin master\ngit push origin master -f\n\n<!-- more -->","slug":"博客源文件提交到仓库","published":1,"updated":"2018-05-21T17:44:03.947Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43xe005d2tpbbemw7qnn"},{"title":"博客备份.md","date":"2018-05-08T10:34:55.000Z","toc":true,"_content":"\n```\n<% if (!is_post()) { %>\n    <% if (site.tags.length){ %>\n        <div class=\"widget tag\">\n          <h3 class=\"title\"><%= __('标签 :') %></h3>\n            <%- list_categories(site.tags) %>\n            </div>\n        <% } %>\n        <% } %>\n\n    <% if (!is_post()) { %>\n    <% if (site.categories.length){ %>\n        <div class=\"widget tag\">\n          <h2 class=\"title\"><%= __('分类 :') %></h2>\n           <h4> <%- list_categories(site.categories) %></h4>\n            </div>\n        <% } %>\n        <% } %>\n\t```\n\t\n---\n\n```\n\t<% if (!index && post.toc) { %>\n                <div id=\"toc\" class=\"toc-article\">\n                <strong class=\"toc-title\"><%= __('') %></strong>\n                    <%- toc(post.content) %>\n                </div>\n            <% } %>\n\n```","source":"_posts/博客修改备份.md","raw":"---\ntitle: 博客备份.md\ndate: 2018-05-08 18:34:55\ntags: [Hexo,Other]\ncategories: [Hexo]\ntoc: true\n---\n\n```\n<% if (!is_post()) { %>\n    <% if (site.tags.length){ %>\n        <div class=\"widget tag\">\n          <h3 class=\"title\"><%= __('标签 :') %></h3>\n            <%- list_categories(site.tags) %>\n            </div>\n        <% } %>\n        <% } %>\n\n    <% if (!is_post()) { %>\n    <% if (site.categories.length){ %>\n        <div class=\"widget tag\">\n          <h2 class=\"title\"><%= __('分类 :') %></h2>\n           <h4> <%- list_categories(site.categories) %></h4>\n            </div>\n        <% } %>\n        <% } %>\n\t```\n\t\n---\n\n```\n\t<% if (!index && post.toc) { %>\n                <div id=\"toc\" class=\"toc-article\">\n                <strong class=\"toc-title\"><%= __('') %></strong>\n                    <%- toc(post.content) %>\n                </div>\n            <% } %>\n\n```","slug":"博客修改备份","published":1,"updated":"2018-05-17T16:18:46.428Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43xf005i2tpbwd4pgv0e"},{"title":"各种快捷键","date":"2018-06-21T15:40:13.306Z","toc":true,"_content":"\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu565l9on8j30ps0iewev.jpg)\n\n一些常用快捷键让人事半功倍\n\n<!-- more -->\n​\t\n\n-----------\n|iterm|\n----------\n\n\tCommand + Shift + h \titerms2复制历史\n\n\t分屏\n\tcommand + option + 方向键 command + [ 或 command + ]   分屏切换屏幕\n\tControl + a        到行首\n\tControl + u        清除当前行\n\tControl + e\t   到行尾\n\tControl + p        / !! 上一条命令\n\tControl + k        从光标处删至命令行尾 (本来 Control + u 是删至命令行首，但iterm中是删掉整行)\n\tControl + w        A + d 从光标处删至字首/尾\n\tControl + k\t删除到文本末尾\n\tControl + h         删掉光标前的自负\n\tControl + d\t    删掉光标后的自负\n\tControl + r        搜索命令历史，这个较常用\n\n-------\n|Alfred|\n-------\n\tCommand + Option + C \tafrend剪切板历史 \n\tCommand + Option + / \tafrend路径历史 \n\tCommand + Option + \\\tafrend对搜索的路径进行操作 如复制等等\n\n\n​\t\n​\t\n​\t\n---------\n|sublime|\n---------\n\tCommand + Shift + d \t复制一行\n\tCommand + Option + f\t查找并替换\n\tCTRL + - 上个打开的文件\n\n------\n|idea|\n------\n\n\n\tCommand + Shift + F12   编栏全屏\t其实就是 Hide All Tool Windows (隐藏所有工具窗口) 这个操作的快捷键。\n\tCommand + Option + Space\t类名或接口名提示\n\tControl + ; \t\t是什么 代替鼠标\n\tCommand + l \t\t跳到指定行\n\tCommand + w \t\t关闭标签页 \n\tOption + 上 \t\t和windows的ctrl+w相同 递进选中代码块\n\tAlt + Insert Command + N \t\t\t\t代码自动生成，如生成对象的 set / get 方法，构造函数，toString() 等\n\tAlt + 前方向键 \tControl + 前方向键 \t\t\t当前光标跳转到当前文件的前一个方法名位置\n\tCtrl + Alt + Enter \tCommand + Option + Enter \t光标所在行上空出一行，光标定位到新行\n\tCtrl + Alt + 左方向键 \tCommand + Option + 左方向键 \t退回到上一个操作的地方\n\tCtrl + Alt + 右方向键 \tCommand + Option + 右方向键 \t前进到上一个操作的地方\n\tCommand + Option + T \t\t包围代码\n\tCommand + Shift +v \t\t历史 \n\tFind usage \t\t查看变量方法的类的直接使用情况\n\tShift + Enter \t\t开始新的一行\n\tCommand + P \t\t方法参数提示\n\tCommand + U \t\t前往父类\n\tCommand + +/- \t\t展开/折叠代码\n\tAlt + 1,2,3...9 \tCommand + 1,2,3...9 \t\t显示对应数值的选项卡，其中 1 是 Project 用得最多\n\tControl + Option + O \t优化导入的类，可以对当前文件和整个包目录使用\n\tCtrl + Alt + T \t对选中的代码弹出环绕选项弹出层\n\t\n\tControl + Option + H\t继承关系\n\tControl + H\t\t接口到实现类\n\n\n\tControl + Shift + J \t智能的将代码拼接成一行\n\tCommand+Alt+V \t引入变量，自动导入变量\n\tOption + F7 \t\t查询所选对象/变量被引用 \n\t\n\t4、类、方法、文件定位\n\t查找类 ctr + N\n \t查找文件 Ctrl + Shift + N\n \t符号定位 Ctrl + Alt + Shift + N\n \t查看文件结构 ctrl + F12\n \t最近打开的文件 ctr + E\n \t定位下一个方法 alt + down\n \t定位上一个方法 alt + up\n \t查看方法参数信息 ctr + p\n \t查看方法、类的 doc ctr + Q\n \t行数 Command + l\n\t5、类、方法的结构查看、定位\n \t跳到类或方法的声明 ctr + B\n \t定位到类的父类、接口 ctr + U 画图 ommand+Option+u\n \t查看类的继承结构 ctr + H\n \t查看方法的继承结构 ctr + Shift + H\n \t查看类或方法被调用情况 ctr + alt +H \n \t原地参看类、方法的声明 Ctrl + Shift + I\n​\t\n\tControl + H \t\t显示当前类的层次结构 继承\n\tCommand + Shift + H \t显示方法层次结构\n\tControl + Option + H \t显示调用层次结构\n\tCommand + L \t\t在当前文件跳转到某一行的指定处\n\t\n\tCommand + B / Command + 鼠标点击 进入光标所在的方法/变量的接口或是定义处\n\tCommand + Option + B \t跳转到实现处\n\tCommand + G \t\t查找模式下，向下查找\n\tCommand + Shift + U \t大小写切换\n\n-----\n|Mac|\n-----\n\n\n​\t\n\tShift+Command+G \t\t跳转打开文件夹 \n\tCommand + Shift + . \t\t显示隐藏文件 \n\tCommand + Control + 空格\t\temoji表情 \n\tShift + Option）+ K \t\tApple logo 「 」\n\tCommand+i \t\t\t简介 \n\tShift+Control+d \t\t搜狗表情包\n\n\n\tShift + Command + 空格 历史文件\n\tControl + Command +F 全屏模式\n\t\n\tCommand + z \t\t\t打开safari下最近关闭tab页面\n\t \t\t\n\tCommand + Option + Shift + Esc \t\t强制退出活跃的\n\tCommand + Option + Esc \t\t\t强制退出\n\tCommand + ` \t\t\t\t切换同一应用的窗口\n\n\n\tShift + Command + d alt + cmd + space \t快速打开finder\n\tcmd + Option + Shift + v \t\t去格式粘贴（亲测大部分软件都可以）\n\tCommand + Tab \t\t\t\t切换应用的时候，可以松开Tab，然后按Q退出选中的应用。\n\t\n\t三指点击网页链接，可以预览链接网页。\n\t按住右上角的新建选项卡按钮能快速浏览并选择最近关闭的窗口 \n\n--------\n|finder|\n--------\n\tCommand +  1，2，3\t\t图标，列表，分栏显示文件夹\n\tCommand + Control + P\t\t复制路径 自己配置\n\tCommand + Option + B\t\t快捷键标记 自己配置\n\tCommand + O\t\t\t打开文件夹\n\tCommand + 下 \t\t\t进入文件夹 \n\tCommand + 上\t \t\t返回文件夹\n\tCommand ＋［ \t\t\t返回\n\tCommand ＋ ］ \t\t\t前进\n\n--------\n| mac命令|\n--------\n\n\n\t根据 asker 提示 作补充：\n\tcommand + fn + 左/右，可以调整到文件开头 / 结尾。\n\t\n\tfn + 左/右相当于home/end在      网页和多数文档中适用。\n\n\n​\t\n\tdefaults write com.apple.finder QuitMenuItem -bool YES  设置finder可以关闭\n\n\topen -n /Applications/WizNote.app   多次打开一个应用\n\n\tmac 没有声音\n \tsudo kill -9 `ps ax|grep 'coreaudio[a-z]' |awk '{print $1}'`\n​\t\n \tsudo killall coreaudiod\n​\t\n​\t\n\t使用后的效果，可以说是非常明显了，再也不会有在「挤牙膏」的感觉。让它回到最开始的状态：\n\t\tdefaults delete com.apple.dock; killall Dock\n \t\tdefaults write com.apple.Dock autohide-delay -float 0 && killall Dock \n\t打开开机声音\n \t\tsudo nvram -d SystemAudioVolume\n\t双屏分任务工作！只要按住窗口左上角的绿色＋即可\n\n\t去掉资源库文件夹的隐藏属性\n \t\tchflags nohidden ~/Library/\n\t打开隐藏属性\n \t\tchflags hidden ~/Library/\n\t调节音量的同时按住\n \t\tOption + Shift键\n\t显示“隐藏文件” Command + Shift + .\n\t\tdefaults write com.apple.finder AppleShowAllFiles -bool true;killall Finder\n\t隐藏\n\t\tdefaults write com.apple.finder AppleShowAllFiles -bool true;killall Finder\n\t关闭开机声音\n\t\tsudo nvram SystemAudioVolume=%80，\n\t省略号 1、依次按 ⌃ ⌘ 空格\n \t2、⇧ 数字6\n\tOption 点击 Dock 图标，按住 Option 点击 Dock 中的图标，则会在桌面显示该应用所有窗口 \n\tOption + 左：向左移动一个单词\n\tOption + 右：向右移动一个单词\n\tOption + Delete：删除左边一个单词\n\tOption + Fn + Delete：删除右边一个单词\n\n\n\t设置 dock 显示时间命令\n\t打开终端输入以下命令\n\t#先修改停留时间（后面数字为停留时间）如：\n\tdefaults write com.apple.dock autohide-delay -int 0      ##（时间设为最短）\n\tdefaults write com.apple.dock autohide-delay -int 0.5    ##（时间设为 0.5s）\n\tdefaults write com.apple.dock autohide-delay -int 10     ##（时间设为 10s）\n\t#使设置生效\n\tkillall Dock\n\n\n​\t\n-----\n|推荐|\n-----\n\t再推荐个人 池健强 《人生元编程》作者 他的博客和微信上有很多干货\n\n​\t\n​\t\n","source":"_posts/各种快捷键.md","raw":"---\ntitle:  各种快捷键\ndate: 2018年06月21日 23时40分28秒\ntags:  [快捷键,Mac,Idea,Finder]\ncategories: 快捷键\ntoc: true\n---\n\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu565l9on8j30ps0iewev.jpg)\n\n一些常用快捷键让人事半功倍\n\n<!-- more -->\n​\t\n\n-----------\n|iterm|\n----------\n\n\tCommand + Shift + h \titerms2复制历史\n\n\t分屏\n\tcommand + option + 方向键 command + [ 或 command + ]   分屏切换屏幕\n\tControl + a        到行首\n\tControl + u        清除当前行\n\tControl + e\t   到行尾\n\tControl + p        / !! 上一条命令\n\tControl + k        从光标处删至命令行尾 (本来 Control + u 是删至命令行首，但iterm中是删掉整行)\n\tControl + w        A + d 从光标处删至字首/尾\n\tControl + k\t删除到文本末尾\n\tControl + h         删掉光标前的自负\n\tControl + d\t    删掉光标后的自负\n\tControl + r        搜索命令历史，这个较常用\n\n-------\n|Alfred|\n-------\n\tCommand + Option + C \tafrend剪切板历史 \n\tCommand + Option + / \tafrend路径历史 \n\tCommand + Option + \\\tafrend对搜索的路径进行操作 如复制等等\n\n\n​\t\n​\t\n​\t\n---------\n|sublime|\n---------\n\tCommand + Shift + d \t复制一行\n\tCommand + Option + f\t查找并替换\n\tCTRL + - 上个打开的文件\n\n------\n|idea|\n------\n\n\n\tCommand + Shift + F12   编栏全屏\t其实就是 Hide All Tool Windows (隐藏所有工具窗口) 这个操作的快捷键。\n\tCommand + Option + Space\t类名或接口名提示\n\tControl + ; \t\t是什么 代替鼠标\n\tCommand + l \t\t跳到指定行\n\tCommand + w \t\t关闭标签页 \n\tOption + 上 \t\t和windows的ctrl+w相同 递进选中代码块\n\tAlt + Insert Command + N \t\t\t\t代码自动生成，如生成对象的 set / get 方法，构造函数，toString() 等\n\tAlt + 前方向键 \tControl + 前方向键 \t\t\t当前光标跳转到当前文件的前一个方法名位置\n\tCtrl + Alt + Enter \tCommand + Option + Enter \t光标所在行上空出一行，光标定位到新行\n\tCtrl + Alt + 左方向键 \tCommand + Option + 左方向键 \t退回到上一个操作的地方\n\tCtrl + Alt + 右方向键 \tCommand + Option + 右方向键 \t前进到上一个操作的地方\n\tCommand + Option + T \t\t包围代码\n\tCommand + Shift +v \t\t历史 \n\tFind usage \t\t查看变量方法的类的直接使用情况\n\tShift + Enter \t\t开始新的一行\n\tCommand + P \t\t方法参数提示\n\tCommand + U \t\t前往父类\n\tCommand + +/- \t\t展开/折叠代码\n\tAlt + 1,2,3...9 \tCommand + 1,2,3...9 \t\t显示对应数值的选项卡，其中 1 是 Project 用得最多\n\tControl + Option + O \t优化导入的类，可以对当前文件和整个包目录使用\n\tCtrl + Alt + T \t对选中的代码弹出环绕选项弹出层\n\t\n\tControl + Option + H\t继承关系\n\tControl + H\t\t接口到实现类\n\n\n\tControl + Shift + J \t智能的将代码拼接成一行\n\tCommand+Alt+V \t引入变量，自动导入变量\n\tOption + F7 \t\t查询所选对象/变量被引用 \n\t\n\t4、类、方法、文件定位\n\t查找类 ctr + N\n \t查找文件 Ctrl + Shift + N\n \t符号定位 Ctrl + Alt + Shift + N\n \t查看文件结构 ctrl + F12\n \t最近打开的文件 ctr + E\n \t定位下一个方法 alt + down\n \t定位上一个方法 alt + up\n \t查看方法参数信息 ctr + p\n \t查看方法、类的 doc ctr + Q\n \t行数 Command + l\n\t5、类、方法的结构查看、定位\n \t跳到类或方法的声明 ctr + B\n \t定位到类的父类、接口 ctr + U 画图 ommand+Option+u\n \t查看类的继承结构 ctr + H\n \t查看方法的继承结构 ctr + Shift + H\n \t查看类或方法被调用情况 ctr + alt +H \n \t原地参看类、方法的声明 Ctrl + Shift + I\n​\t\n\tControl + H \t\t显示当前类的层次结构 继承\n\tCommand + Shift + H \t显示方法层次结构\n\tControl + Option + H \t显示调用层次结构\n\tCommand + L \t\t在当前文件跳转到某一行的指定处\n\t\n\tCommand + B / Command + 鼠标点击 进入光标所在的方法/变量的接口或是定义处\n\tCommand + Option + B \t跳转到实现处\n\tCommand + G \t\t查找模式下，向下查找\n\tCommand + Shift + U \t大小写切换\n\n-----\n|Mac|\n-----\n\n\n​\t\n\tShift+Command+G \t\t跳转打开文件夹 \n\tCommand + Shift + . \t\t显示隐藏文件 \n\tCommand + Control + 空格\t\temoji表情 \n\tShift + Option）+ K \t\tApple logo 「 」\n\tCommand+i \t\t\t简介 \n\tShift+Control+d \t\t搜狗表情包\n\n\n\tShift + Command + 空格 历史文件\n\tControl + Command +F 全屏模式\n\t\n\tCommand + z \t\t\t打开safari下最近关闭tab页面\n\t \t\t\n\tCommand + Option + Shift + Esc \t\t强制退出活跃的\n\tCommand + Option + Esc \t\t\t强制退出\n\tCommand + ` \t\t\t\t切换同一应用的窗口\n\n\n\tShift + Command + d alt + cmd + space \t快速打开finder\n\tcmd + Option + Shift + v \t\t去格式粘贴（亲测大部分软件都可以）\n\tCommand + Tab \t\t\t\t切换应用的时候，可以松开Tab，然后按Q退出选中的应用。\n\t\n\t三指点击网页链接，可以预览链接网页。\n\t按住右上角的新建选项卡按钮能快速浏览并选择最近关闭的窗口 \n\n--------\n|finder|\n--------\n\tCommand +  1，2，3\t\t图标，列表，分栏显示文件夹\n\tCommand + Control + P\t\t复制路径 自己配置\n\tCommand + Option + B\t\t快捷键标记 自己配置\n\tCommand + O\t\t\t打开文件夹\n\tCommand + 下 \t\t\t进入文件夹 \n\tCommand + 上\t \t\t返回文件夹\n\tCommand ＋［ \t\t\t返回\n\tCommand ＋ ］ \t\t\t前进\n\n--------\n| mac命令|\n--------\n\n\n\t根据 asker 提示 作补充：\n\tcommand + fn + 左/右，可以调整到文件开头 / 结尾。\n\t\n\tfn + 左/右相当于home/end在      网页和多数文档中适用。\n\n\n​\t\n\tdefaults write com.apple.finder QuitMenuItem -bool YES  设置finder可以关闭\n\n\topen -n /Applications/WizNote.app   多次打开一个应用\n\n\tmac 没有声音\n \tsudo kill -9 `ps ax|grep 'coreaudio[a-z]' |awk '{print $1}'`\n​\t\n \tsudo killall coreaudiod\n​\t\n​\t\n\t使用后的效果，可以说是非常明显了，再也不会有在「挤牙膏」的感觉。让它回到最开始的状态：\n\t\tdefaults delete com.apple.dock; killall Dock\n \t\tdefaults write com.apple.Dock autohide-delay -float 0 && killall Dock \n\t打开开机声音\n \t\tsudo nvram -d SystemAudioVolume\n\t双屏分任务工作！只要按住窗口左上角的绿色＋即可\n\n\t去掉资源库文件夹的隐藏属性\n \t\tchflags nohidden ~/Library/\n\t打开隐藏属性\n \t\tchflags hidden ~/Library/\n\t调节音量的同时按住\n \t\tOption + Shift键\n\t显示“隐藏文件” Command + Shift + .\n\t\tdefaults write com.apple.finder AppleShowAllFiles -bool true;killall Finder\n\t隐藏\n\t\tdefaults write com.apple.finder AppleShowAllFiles -bool true;killall Finder\n\t关闭开机声音\n\t\tsudo nvram SystemAudioVolume=%80，\n\t省略号 1、依次按 ⌃ ⌘ 空格\n \t2、⇧ 数字6\n\tOption 点击 Dock 图标，按住 Option 点击 Dock 中的图标，则会在桌面显示该应用所有窗口 \n\tOption + 左：向左移动一个单词\n\tOption + 右：向右移动一个单词\n\tOption + Delete：删除左边一个单词\n\tOption + Fn + Delete：删除右边一个单词\n\n\n\t设置 dock 显示时间命令\n\t打开终端输入以下命令\n\t#先修改停留时间（后面数字为停留时间）如：\n\tdefaults write com.apple.dock autohide-delay -int 0      ##（时间设为最短）\n\tdefaults write com.apple.dock autohide-delay -int 0.5    ##（时间设为 0.5s）\n\tdefaults write com.apple.dock autohide-delay -int 10     ##（时间设为 10s）\n\t#使设置生效\n\tkillall Dock\n\n\n​\t\n-----\n|推荐|\n-----\n\t再推荐个人 池健强 《人生元编程》作者 他的博客和微信上有很多干货\n\n​\t\n​\t\n","slug":"各种快捷键","published":1,"updated":"2018-08-10T18:40:41.685Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43xg005l2tpbv696xgh0"},{"title":"命令积累","date":"2018-05-31T06:10:40.710Z","toc":true,"_content":"\n\n>bin /h dfs oev -i edits -o edits.xml 查看元数据\n>![示例](https://www.github.com/yaosong5/tuchuang/raw/master/mdtc/2018/5/31/1527747042953.jpg)\n\n<!-- more -->","source":"_posts/大数据命令积累.md","raw":"---\ntitle:  命令积累\ndate: \ntags:  [大数据,命令]\ncategories: 碎片知识\ntoc: true\n---\n\n\n>bin /h dfs oev -i edits -o edits.xml 查看元数据\n>![示例](https://www.github.com/yaosong5/tuchuang/raw/master/mdtc/2018/5/31/1527747042953.jpg)\n\n<!-- more -->","slug":"大数据命令积累","published":1,"updated":"2018-08-06T18:20:09.751Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43xi005q2tpbv9pku4x8"},{"title":"部署博客到云服务器","date":"2018-05-20T16:56:17.069Z","toc":true,"_content":"\n\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu566e9ybjj30kg05q0t0.jpg)\n\n简单记录转移到博客到云服务器\n<!-- more -->\n\n#  原理及准备\n>我们在自己的电脑上写好博客, 使用 git 发布到代码仓库进行备份, git 仓库接收到 push 请求后, 使用 webhook 配合 nodejs 自动进行服务器端页面的更新.\n## 准备\n安装Git和NodeJS (CentOS 环境)\n\n``` \nyum install git\n```\n安装NodeJS\n\n``` vim\ncurl --silent --location https://rpm.nodesource.com/setup_5.x | bash -\n```\n\n# 服务器构建\n## webhook方式\n服务器端的” 钩子”\n我们借助一个 node 插件 github-webhook-handler 来快速完成配合 github webhook 的操作, 其他 git 平台也有相应的插件, 如配合 coding 的 coding-webhook-handler.\n\n### 监听脚本\n>我们借助一个 node 插件 *github webhook-handler*来快速完成配合 github webhook 的操作, 其他 git 平台也有相应的插件, 如配合 coding 的 coding-webhook-handler.\n\n使用 `npm install -g github-webhook-handler` 命令来安装到服务器端.\nconding则为`npm install -g coding-webhook-handler` \n\n\n>切换到服务器站点目录，如我的是 /root/blog,新建一个public目录，将你的github仓库中的master分支pull到该目录中，这个目录作为这个博客的根目录了\n\n```bash\ncd /root/blog\nmkdir public \ncd public \ngit init\ngit remote add origin https://github.com/yaosong5/yaosong5.github.io\ngit pull origin master\n```\n\n\n然后我们创建一个webhooks.js文件，将以下的内容粘贴，这相当于Node.js **服务器**的代码构建\n\n\n\n``` javascript\nvar http = require('http')\nvar createHandler = require('github-webhook-handler')\nvar handler = createHandler({ path: '/', secret: 'yao' })\n\nfunction run_cmd(cmd, args, callback) {\n  var spawn = require('child_process').spawn;\n  var child = spawn(cmd, args);\n  var resp = \"\";\n\n  child.stdout.on('data', function(buffer) { resp += buffer.toString(); });\n  child.stdout.on('end', function() { callback (resp) });\n}\n\nhttp.createServer(function (req, res) {\n  handler(req, res, function (err) {\n    res.statusCode = 404\n    res.end('no such location')\n  })\n}).listen(7777)\n\nhandler.on('error', function (err) {\n  console.error('Error:', err.message)\n})\n\nhandler.on('push', function (event) {\n  console.log('Received a push event for %s to %s',\n    event.payload.repository.name,\n    event.payload.ref);\n    run_cmd('sh', ['./deploy.sh',event.payload.repository.name], function(text){ console.log(text) });\n})\n```\n\n注意上段代码中第 3 行 { path: '/', secret: '改为你的secret' } 中 secret 可以改为你喜欢的口令, 这口令将在下面的步骤中起到作用 ,配置github webhooks的时候填入的口令, 请留意. 第 19 行 listen(7777) 中 7777 为监听程序需要使用的端口.\n\n### 执行脚本\n上面的 javascript 代码是用来捕捉 github 发来的信号并发起一个执行 ./deploy.sh 的脚本, 接下来我们还需要写 deploy.sh 的内容.\n\n``` bash\n#!/bin/bash\n\nWEB_PATH='/root/blog/public'\n\necho \"Start deployment\"\ncd $WEB_PATH\necho \"pulling source code...\"\ngit reset --hard origin/master\ngit clean -f\ngit pull\ngit checkout master\necho \"Finished.\"\n```\n\n将以上代码的第 3 行改为你服务器中的实际目录. 接下来只需要开启监听就可以了.\n\n\ntips: 在此之前你可以使用 node webhook.js 来测试一下监听程序是否能够正常运行.\n我在这里碰到了一个 node 环境变量的问题, 读取不到 github-webhook-handler 这个模块, 找了很多办法也没有解决, 后来我直接在项目根目录的上级目录安装了这个模块, 问题就解决了.\n\n>cd /root/blog\n>npm install github-webhook-handler\n>npm 会从当前目录依次向上寻找含有 node_modules 目录并访问该模块.\n\n### 普通方式运行 webhook.js\n利用 Linux 提供的 nohup 命令，让 webhooks.js 运行在后台\n\n```\nnohup node webhook.js > deploy.log &\n```\n###  Forever方式运行webhook.js\n>我在实际使用的时候发现，我的 Node 服务器时不时会自动停掉，具体原因我暂时还没有弄清楚。不过似乎很多人都遇到了这样的困扰，要解决这个问题，forever 是个不错的选择。借助 forever 这个库，它可以保证 Node 持续运行下去，一旦服务器挂了，它都会重启服务器。\n\n安装 forever：\n` npm install -g forever`\n运行：\n``` \ncd { 部署服务器的根目录 }\n nohup forever start webhook.js > deploy.log &\n```\n Ubuntu 中原本就有一个叫 node 的包。为了避免冲突，在 Ubuntu 上安装或使用 Node 得用 nodejs 这个名字。而 forever 默认是使用 node 作为执行脚本的程序名。所以为了处理 Ubuntu 存在的这种特殊情况，在启动 forever 时得另外添加一个参数：(其它则忽略)\n\n`forever start webhook.js -c nodejs`\n### Github配置webhooks\n配置好 Webhook 后，Github 会发送一个 ping 来测试这个地址。如果成功了，那么这个 Webhook 前就会加上一个绿色的勾；如果你得到的是一个红色的叉，那就好好检查一下哪儿出问题了吧！\n## git-hook方式\n可采用一种更为简单的部署方式\n>这种方式和webhook可二选一\n\n### 服务器上建立git裸库\n\n创建一个裸仓库，裸仓库就是只保存git信息的Repository, 首先切换到git用户确保git用户拥有仓库所有权\n一定要加 --bare，这样才是一个裸库。\n\n```\ncd \ngit init --bare blog.git\n```\n### 使用 git-hooks 同步网站根目录\n\n\n在这里我们使用的是 post-receive这个钩子，当git有收发的时候就会调用这个钩子。 在 ~/blog.git 裸库的 hooks文件夹中，\n新建post-receive文件。\n>vim ~/blog.git/hooks/post-receive\n\n填入以下内容\n\n``` bash\n#!/bin/sh\ngit --work-tree=/root/blog/public --git-dir=/root/blog.git checkout -f\n```\n\n*work-tree=/root/blog/public这个目录是网站的网页文件目录，--git-dir=/root/blog.git目录为裸库地址，裸库监听git提交会将文件提交到网页目录*\n保存后，要赋予这个文件可执行权限`chmod +x post-receive`\n### 配置博客根目录_config.yml\n完成自动化部署\n打开 _config.yml, 找到 deploy\n```\ndeploy:\n    type: git\n    repo: 用户名@SERVER名:/home/git/blog.git（裸库地址）    //<repository url>\n    branch: master            //这里填写分支   [branch]\n    message: 提交的信息         //自定义提交信息 (默认为 Site updated: {{ now('YYYY-MM-DD HH:mm:ss') }})\n```\n\n# Nginx服务\nnpm 安装nginx\n启动nginx \n\n``` bash\nservice nginx start\n```\n`nginx -t` 查看nginx配置文件\n若nginx服务启动，访问报403错误\n 则将首行 user nginx 改为user root\n\n``` \nvim /etc/nginx/nginx.conf\nserver {\n    listen          80;  # 监听端口\n    server_name     47.98.141.252:80 gangtieguo.cn wwww.gangtieguo.cn;  # 你的域名\n    location / {\n        root        /root/blog/public;\n        index       index.html;\n    }\n}\n```\n**重载 nginx，使配置生效**\n\n```nginx -s reload```\n\n参考\n[Hexo 静态博客搭建并实现自动部署到远程 vps](https://www.jianshu.com/p/b29a2e40501f)\n[将 Hexo 博客发布到自己的服务器上](https://blog.mutoe.com/2017/deploy-hexo-website-to-self-server/)\n[利用 Github 的 Webhook 功能和 Node.js 完成项目的自动部署](https://www.jianshu.com/p/e4cacd775e5b)\n[Webhook 实践 —— 自动部署](https://segmentfault.com/a/1190000003908244)\n[Hexo 快速搭建静态博客并实现远程 VPS 自动部署](http://imlianer.com/a/deploy-hexo-on-vps)\n[阿里云 VPS 搭建自己的的 Hexo 博客](https://segmentfault.com/a/1190000005723321)","source":"_posts/转移Github博客到云服务器.md","raw":"---\ntitle:  部署博客到云服务器\ndate: 2018年05月21日 00时54分48秒\ntags:  [Hexo]\ncategories: 博客\ntoc: true\n---\n\n\n\n![](https://ws2.sinaimg.cn/large/006tNbRwgy1fu566e9ybjj30kg05q0t0.jpg)\n\n简单记录转移到博客到云服务器\n<!-- more -->\n\n#  原理及准备\n>我们在自己的电脑上写好博客, 使用 git 发布到代码仓库进行备份, git 仓库接收到 push 请求后, 使用 webhook 配合 nodejs 自动进行服务器端页面的更新.\n## 准备\n安装Git和NodeJS (CentOS 环境)\n\n``` \nyum install git\n```\n安装NodeJS\n\n``` vim\ncurl --silent --location https://rpm.nodesource.com/setup_5.x | bash -\n```\n\n# 服务器构建\n## webhook方式\n服务器端的” 钩子”\n我们借助一个 node 插件 github-webhook-handler 来快速完成配合 github webhook 的操作, 其他 git 平台也有相应的插件, 如配合 coding 的 coding-webhook-handler.\n\n### 监听脚本\n>我们借助一个 node 插件 *github webhook-handler*来快速完成配合 github webhook 的操作, 其他 git 平台也有相应的插件, 如配合 coding 的 coding-webhook-handler.\n\n使用 `npm install -g github-webhook-handler` 命令来安装到服务器端.\nconding则为`npm install -g coding-webhook-handler` \n\n\n>切换到服务器站点目录，如我的是 /root/blog,新建一个public目录，将你的github仓库中的master分支pull到该目录中，这个目录作为这个博客的根目录了\n\n```bash\ncd /root/blog\nmkdir public \ncd public \ngit init\ngit remote add origin https://github.com/yaosong5/yaosong5.github.io\ngit pull origin master\n```\n\n\n然后我们创建一个webhooks.js文件，将以下的内容粘贴，这相当于Node.js **服务器**的代码构建\n\n\n\n``` javascript\nvar http = require('http')\nvar createHandler = require('github-webhook-handler')\nvar handler = createHandler({ path: '/', secret: 'yao' })\n\nfunction run_cmd(cmd, args, callback) {\n  var spawn = require('child_process').spawn;\n  var child = spawn(cmd, args);\n  var resp = \"\";\n\n  child.stdout.on('data', function(buffer) { resp += buffer.toString(); });\n  child.stdout.on('end', function() { callback (resp) });\n}\n\nhttp.createServer(function (req, res) {\n  handler(req, res, function (err) {\n    res.statusCode = 404\n    res.end('no such location')\n  })\n}).listen(7777)\n\nhandler.on('error', function (err) {\n  console.error('Error:', err.message)\n})\n\nhandler.on('push', function (event) {\n  console.log('Received a push event for %s to %s',\n    event.payload.repository.name,\n    event.payload.ref);\n    run_cmd('sh', ['./deploy.sh',event.payload.repository.name], function(text){ console.log(text) });\n})\n```\n\n注意上段代码中第 3 行 { path: '/', secret: '改为你的secret' } 中 secret 可以改为你喜欢的口令, 这口令将在下面的步骤中起到作用 ,配置github webhooks的时候填入的口令, 请留意. 第 19 行 listen(7777) 中 7777 为监听程序需要使用的端口.\n\n### 执行脚本\n上面的 javascript 代码是用来捕捉 github 发来的信号并发起一个执行 ./deploy.sh 的脚本, 接下来我们还需要写 deploy.sh 的内容.\n\n``` bash\n#!/bin/bash\n\nWEB_PATH='/root/blog/public'\n\necho \"Start deployment\"\ncd $WEB_PATH\necho \"pulling source code...\"\ngit reset --hard origin/master\ngit clean -f\ngit pull\ngit checkout master\necho \"Finished.\"\n```\n\n将以上代码的第 3 行改为你服务器中的实际目录. 接下来只需要开启监听就可以了.\n\n\ntips: 在此之前你可以使用 node webhook.js 来测试一下监听程序是否能够正常运行.\n我在这里碰到了一个 node 环境变量的问题, 读取不到 github-webhook-handler 这个模块, 找了很多办法也没有解决, 后来我直接在项目根目录的上级目录安装了这个模块, 问题就解决了.\n\n>cd /root/blog\n>npm install github-webhook-handler\n>npm 会从当前目录依次向上寻找含有 node_modules 目录并访问该模块.\n\n### 普通方式运行 webhook.js\n利用 Linux 提供的 nohup 命令，让 webhooks.js 运行在后台\n\n```\nnohup node webhook.js > deploy.log &\n```\n###  Forever方式运行webhook.js\n>我在实际使用的时候发现，我的 Node 服务器时不时会自动停掉，具体原因我暂时还没有弄清楚。不过似乎很多人都遇到了这样的困扰，要解决这个问题，forever 是个不错的选择。借助 forever 这个库，它可以保证 Node 持续运行下去，一旦服务器挂了，它都会重启服务器。\n\n安装 forever：\n` npm install -g forever`\n运行：\n``` \ncd { 部署服务器的根目录 }\n nohup forever start webhook.js > deploy.log &\n```\n Ubuntu 中原本就有一个叫 node 的包。为了避免冲突，在 Ubuntu 上安装或使用 Node 得用 nodejs 这个名字。而 forever 默认是使用 node 作为执行脚本的程序名。所以为了处理 Ubuntu 存在的这种特殊情况，在启动 forever 时得另外添加一个参数：(其它则忽略)\n\n`forever start webhook.js -c nodejs`\n### Github配置webhooks\n配置好 Webhook 后，Github 会发送一个 ping 来测试这个地址。如果成功了，那么这个 Webhook 前就会加上一个绿色的勾；如果你得到的是一个红色的叉，那就好好检查一下哪儿出问题了吧！\n## git-hook方式\n可采用一种更为简单的部署方式\n>这种方式和webhook可二选一\n\n### 服务器上建立git裸库\n\n创建一个裸仓库，裸仓库就是只保存git信息的Repository, 首先切换到git用户确保git用户拥有仓库所有权\n一定要加 --bare，这样才是一个裸库。\n\n```\ncd \ngit init --bare blog.git\n```\n### 使用 git-hooks 同步网站根目录\n\n\n在这里我们使用的是 post-receive这个钩子，当git有收发的时候就会调用这个钩子。 在 ~/blog.git 裸库的 hooks文件夹中，\n新建post-receive文件。\n>vim ~/blog.git/hooks/post-receive\n\n填入以下内容\n\n``` bash\n#!/bin/sh\ngit --work-tree=/root/blog/public --git-dir=/root/blog.git checkout -f\n```\n\n*work-tree=/root/blog/public这个目录是网站的网页文件目录，--git-dir=/root/blog.git目录为裸库地址，裸库监听git提交会将文件提交到网页目录*\n保存后，要赋予这个文件可执行权限`chmod +x post-receive`\n### 配置博客根目录_config.yml\n完成自动化部署\n打开 _config.yml, 找到 deploy\n```\ndeploy:\n    type: git\n    repo: 用户名@SERVER名:/home/git/blog.git（裸库地址）    //<repository url>\n    branch: master            //这里填写分支   [branch]\n    message: 提交的信息         //自定义提交信息 (默认为 Site updated: {{ now('YYYY-MM-DD HH:mm:ss') }})\n```\n\n# Nginx服务\nnpm 安装nginx\n启动nginx \n\n``` bash\nservice nginx start\n```\n`nginx -t` 查看nginx配置文件\n若nginx服务启动，访问报403错误\n 则将首行 user nginx 改为user root\n\n``` \nvim /etc/nginx/nginx.conf\nserver {\n    listen          80;  # 监听端口\n    server_name     47.98.141.252:80 gangtieguo.cn wwww.gangtieguo.cn;  # 你的域名\n    location / {\n        root        /root/blog/public;\n        index       index.html;\n    }\n}\n```\n**重载 nginx，使配置生效**\n\n```nginx -s reload```\n\n参考\n[Hexo 静态博客搭建并实现自动部署到远程 vps](https://www.jianshu.com/p/b29a2e40501f)\n[将 Hexo 博客发布到自己的服务器上](https://blog.mutoe.com/2017/deploy-hexo-website-to-self-server/)\n[利用 Github 的 Webhook 功能和 Node.js 完成项目的自动部署](https://www.jianshu.com/p/e4cacd775e5b)\n[Webhook 实践 —— 自动部署](https://segmentfault.com/a/1190000003908244)\n[Hexo 快速搭建静态博客并实现远程 VPS 自动部署](http://imlianer.com/a/deploy-hexo-on-vps)\n[阿里云 VPS 搭建自己的的 Hexo 博客](https://segmentfault.com/a/1190000005723321)","slug":"转移Github博客到云服务器","published":1,"updated":"2018-08-10T18:32:14.487Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlok43xj005t2tpbwjj8acoe"}],"PostAsset":[],"PostCategory":[{"post_id":"cjlok43v500032tpbaz8j4r72","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43vj000h2tpbc24bmd6v"},{"post_id":"cjlok43v800052tpbj3jeh386","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43vn000o2tpb052qulmd"},{"post_id":"cjlok43vc00092tpbqbncqomu","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43vp000u2tpb5cb1pqym"},{"post_id":"cjlok43vn000r2tpb0lbd5tnc","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43vr00102tpbsat4ayca"},{"post_id":"cjlok43vd000a2tpbh5flt34n","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43vs00142tpbdeeyl8g7"},{"post_id":"cjlok43vo000t2tpbjbd6bdyw","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43vt00172tpbjz7id9fr"},{"post_id":"cjlok43vp000y2tpb9113ca31","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43vu001b2tpbe3e10lws"},{"post_id":"cjlok43ve000b2tpbibqbm1nn","category_id":"cjlok43vp000v2tpb5801fyth","_id":"cjlok43vw001f2tpbdbxbcvwx"},{"post_id":"cjlok43vq000z2tpbfew2692s","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43vx001i2tpbtqniu62n"},{"post_id":"cjlok43vs00132tpbsgpkqvzt","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43vy001l2tpb9ins29k3"},{"post_id":"cjlok43vg000e2tpbuzdp9k6y","category_id":"cjlok43vp000v2tpb5801fyth","_id":"cjlok43vz001o2tpb6zuu55lv"},{"post_id":"cjlok43vt00162tpb76sn7zdy","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43w0001r2tpb6g7h1yqd"},{"post_id":"cjlok43vu001a2tpb8mbb5zaz","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43w1001u2tpb3khxjhfj"},{"post_id":"cjlok43vh000f2tpbbwnzdnh3","category_id":"cjlok43vu00182tpblgtu3x3s","_id":"cjlok43w2001y2tpb2kzs4h9c"},{"post_id":"cjlok43vv001e2tpb79933r5e","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43w300222tpb23xgua58"},{"post_id":"cjlok43vw001h2tpbnnqybm72","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43w400252tpbcgg01kuz"},{"post_id":"cjlok43vk000k2tpbobtqdc41","category_id":"cjlok43vp000v2tpb5801fyth","_id":"cjlok43w6002a2tpb54li656y"},{"post_id":"cjlok43vx001k2tpb20goysu0","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43w7002c2tpbkdyduodc"},{"post_id":"cjlok43vz001n2tpbewytjvjc","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43w8002f2tpbj7xldo1q"},{"post_id":"cjlok43vm000m2tpbjfpcdq2w","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43wa002j2tpbws149ekl"},{"post_id":"cjlok43w0001q2tpb4s5k4pxk","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43wd002n2tpb1ectydja"},{"post_id":"cjlok43w400242tpbtaw9tjzj","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43we002r2tpbn15r1zha"},{"post_id":"cjlok43w1001t2tpbavcx4dbs","category_id":"cjlok43w300202tpbendm8jbm","_id":"cjlok43wf002v2tpbqhepkihb"},{"post_id":"cjlok43w6002b2tpb2tzro98h","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43wh002z2tpbz7pmasjc"},{"post_id":"cjlok43w2001x2tpbpko8bhpd","category_id":"cjlok43w500292tpb0wq4lf0d","_id":"cjlok43wj00332tpb2fdgkipe"},{"post_id":"cjlok43w8002e2tpb1muuuud6","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43wk00372tpbv4nug9cz"},{"post_id":"cjlok43w300212tpbem79webm","category_id":"cjlok43w500292tpb0wq4lf0d","_id":"cjlok43wm003b2tpbia83p598"},{"post_id":"cjlok43wd002q2tpbwy68odk1","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43wn003f2tpbsanr9l7w"},{"post_id":"cjlok43w500282tpbpymizqp1","category_id":"cjlok43wd002p2tpb0gnvyov2","_id":"cjlok43wp003j2tpb3b8eh7cw"},{"post_id":"cjlok43w9002i2tpbnqf7wiej","category_id":"cjlok43wg002x2tpbovicivly","_id":"cjlok43wq003m2tpbv9t6r8ig"},{"post_id":"cjlok43wl003a2tpbcpxo6pm5","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43ws003r2tpbqz7fcdpy"},{"post_id":"cjlok43wc002m2tpbvhwo66ny","category_id":"cjlok43wk00352tpb06rvninj","_id":"cjlok43wt003u2tpblnw0iyb6"},{"post_id":"cjlok43wm003d2tpb69p3x7d8","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43wv003z2tpb9mr30xwa"},{"post_id":"cjlok43wo003i2tpb7kj9sjz5","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43ww00422tpbdcox5nua"},{"post_id":"cjlok43wf002u2tpbk2wc6334","category_id":"cjlok43wn003e2tpbwk8quxi2","_id":"cjlok43wy00472tpbwsnp91p7"},{"post_id":"cjlok43wr003q2tpbr0d1n4u2","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43wz004a2tpb3ydwkz28"},{"post_id":"cjlok43wg002y2tpbzuy6icg8","category_id":"cjlok43wn003e2tpbwk8quxi2","_id":"cjlok43x1004e2tpb3gc3mnrz"},{"post_id":"cjlok43wt003t2tpbvbwe0ob7","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43x2004h2tpbqmp52t11"},{"post_id":"cjlok43wu003y2tpbc6z9lhrq","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43x3004l2tpbvdhm3bro"},{"post_id":"cjlok43wh00322tpbjviuf6mi","category_id":"cjlok43wn003e2tpbwk8quxi2","_id":"cjlok43x4004o2tpbcw3ujz09"},{"post_id":"cjlok43ww00412tpbnktf4y54","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43x6004s2tpbrkeuc8xn"},{"post_id":"cjlok43wy00462tpblnawe8ui","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43x7004w2tpbrsawxl1v"},{"post_id":"cjlok43wk00362tpb09u3s2qy","category_id":"cjlok43wn003e2tpbwk8quxi2","_id":"cjlok43x9004z2tpb3ipqcku9"},{"post_id":"cjlok43wz00492tpbj4tun92m","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43xa00532tpbk2oio69z"},{"post_id":"cjlok43x0004d2tpbfis1bujj","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43xc00572tpbx8hzfyuu"},{"post_id":"cjlok43x2004g2tpb4g7n5e9i","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43xe005b2tpb76s2kx7c"},{"post_id":"cjlok43x4004n2tpbq8i892dm","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43xf005f2tpbxx72vrta"},{"post_id":"cjlok43x5004r2tpbjrt5ctn3","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43xg005j2tpbb2bq5n4r"},{"post_id":"cjlok43x7004v2tpbycdcdukg","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43xh005m2tpb7l24t59r"},{"post_id":"cjlok43x3004k2tpbyx6tl0e2","category_id":"cjlok43x5004q2tpbhepy6efy","_id":"cjlok43xi005r2tpbvt0ftyiq"},{"post_id":"cjlok43xa00522tpbfuxqfg3a","category_id":"cjlok43vn000n2tpbns393yk3","_id":"cjlok43xj005u2tpb2fgqal8g"},{"post_id":"cjlok43xd005a2tpbk1q3b7b5","category_id":"cjlok43va00062tpbsn87crvh","_id":"cjlok43xk005y2tpbqjzkc2q8"},{"post_id":"cjlok43x8004y2tpbudzsj4bv","category_id":"cjlok43xb00552tpbtq455xuk","_id":"cjlok43xk00602tpbydyf7bok"},{"post_id":"cjlok43xb00562tpbeiwaah4f","category_id":"cjlok43xf005e2tpb9895g1ke","_id":"cjlok43xl00642tpbh5dcmyhw"},{"post_id":"cjlok43xe005d2tpbbemw7qnn","category_id":"cjlok43xh005n2tpb168skoel","_id":"cjlok43xl00662tpbmdk3v2z3"},{"post_id":"cjlok43xj005t2tpbwjj8acoe","category_id":"cjlok43xh005n2tpb168skoel","_id":"cjlok43xm006a2tpbzu7sbomg"},{"post_id":"cjlok43xf005i2tpbwd4pgv0e","category_id":"cjlok43xk005w2tpbb3x86nto","_id":"cjlok43xm006c2tpbrw74ijms"},{"post_id":"cjlok43xg005l2tpbv696xgh0","category_id":"cjlok43xl00632tpbzlm83aej","_id":"cjlok43xm006f2tpbh55fkbn6"},{"post_id":"cjlok43xi005q2tpbv9pku4x8","category_id":"cjlok43xm00692tpbzljm8o8q","_id":"cjlok43xn006i2tpboqgu5qs6"}],"PostTag":[{"post_id":"cjlok43vg000e2tpbuzdp9k6y","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43vk000j2tpbemdheg2u"},{"post_id":"cjlok43v500032tpbaz8j4r72","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43vm000l2tpblsrc3obz"},{"post_id":"cjlok43v500032tpbaz8j4r72","tag_id":"cjlok43vf000d2tpbwmg6h2cq","_id":"cjlok43vn000q2tpbp69hap8b"},{"post_id":"cjlok43vk000k2tpbobtqdc41","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43vo000s2tpbuuqwzjth"},{"post_id":"cjlok43v800052tpbj3jeh386","tag_id":"cjlok43vk000i2tpb48rq2l4t","_id":"cjlok43vp000x2tpbh5zk413w"},{"post_id":"cjlok43vc00092tpbqbncqomu","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43vt00152tpb1evwnvzg"},{"post_id":"cjlok43vc00092tpbqbncqomu","tag_id":"cjlok43vp000w2tpbdkj71dgh","_id":"cjlok43vu00192tpb5su7c2wp"},{"post_id":"cjlok43vd000a2tpbh5flt34n","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43vv001d2tpbd7frm1t7"},{"post_id":"cjlok43ve000b2tpbibqbm1nn","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43w1001s2tpbl22znt8s"},{"post_id":"cjlok43ve000b2tpbibqbm1nn","tag_id":"cjlok43vv001c2tpbuhfv7evq","_id":"cjlok43w2001v2tpblqz2rufl"},{"post_id":"cjlok43ve000b2tpbibqbm1nn","tag_id":"cjlok43vx001j2tpb4r6nzhrz","_id":"cjlok43w3001z2tpbqjqjfe9q"},{"post_id":"cjlok43vh000f2tpbbwnzdnh3","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43w400232tpb21j2i7nd"},{"post_id":"cjlok43vh000f2tpbbwnzdnh3","tag_id":"cjlok43w0001p2tpbsz1v2e8e","_id":"cjlok43w500272tpbio6pd3yg"},{"post_id":"cjlok43vm000m2tpbjfpcdq2w","tag_id":"cjlok43w2001w2tpbwsyg55uo","_id":"cjlok43w8002g2tpbgu4ilre0"},{"post_id":"cjlok43vm000m2tpbjfpcdq2w","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43wa002k2tpb08q1zcci"},{"post_id":"cjlok43w8002e2tpb1muuuud6","tag_id":"cjlok43w0001p2tpbsz1v2e8e","_id":"cjlok43wd002o2tpbxvacminy"},{"post_id":"cjlok43w8002e2tpb1muuuud6","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43we002s2tpbfyzybh0x"},{"post_id":"cjlok43vn000r2tpb0lbd5tnc","tag_id":"cjlok43w2001w2tpbwsyg55uo","_id":"cjlok43wg002w2tpbggyno6wy"},{"post_id":"cjlok43vn000r2tpb0lbd5tnc","tag_id":"cjlok43wb002l2tpbky59vdad","_id":"cjlok43wh00302tpb3hizrvko"},{"post_id":"cjlok43wf002u2tpbk2wc6334","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43wj00342tpb22v0nynr"},{"post_id":"cjlok43wf002u2tpbk2wc6334","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43wl00382tpbeajuenbm"},{"post_id":"cjlok43wh00322tpbjviuf6mi","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43wm003c2tpbragr7yiw"},{"post_id":"cjlok43wh00322tpbjviuf6mi","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43wo003g2tpbg695m2in"},{"post_id":"cjlok43wk00362tpb09u3s2qy","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43wp003k2tpbdlooahgj"},{"post_id":"cjlok43wk00362tpb09u3s2qy","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43wr003o2tpb0cp883x7"},{"post_id":"cjlok43vo000t2tpbjbd6bdyw","tag_id":"cjlok43we002t2tpbzw0vecpd","_id":"cjlok43ws003s2tpbizzff37d"},{"post_id":"cjlok43vo000t2tpbjbd6bdyw","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43wu003w2tpb52g9kzs4"},{"post_id":"cjlok43vo000t2tpbjbd6bdyw","tag_id":"cjlok43w0001p2tpbsz1v2e8e","_id":"cjlok43wv00402tpbadf7ofad"},{"post_id":"cjlok43wr003q2tpbr0d1n4u2","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43wx00442tpbu5xvzp4d"},{"post_id":"cjlok43wr003q2tpbr0d1n4u2","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43wz00482tpbds5e8r5t"},{"post_id":"cjlok43wt003t2tpbvbwe0ob7","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43x0004b2tpbkucvp025"},{"post_id":"cjlok43wu003y2tpbc6z9lhrq","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43x1004f2tpbosldgwtm"},{"post_id":"cjlok43vp000y2tpb9113ca31","tag_id":"cjlok43we002t2tpbzw0vecpd","_id":"cjlok43x2004i2tpba3m5t4hw"},{"post_id":"cjlok43vp000y2tpb9113ca31","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43x4004m2tpbwxd2q595"},{"post_id":"cjlok43vp000y2tpb9113ca31","tag_id":"cjlok43w0001p2tpbsz1v2e8e","_id":"cjlok43x5004p2tpb5ifdh2pz"},{"post_id":"cjlok43wy00462tpblnawe8ui","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43x6004u2tpbboqf8ixm"},{"post_id":"cjlok43wy00462tpblnawe8ui","tag_id":"cjlok43w2001w2tpbwsyg55uo","_id":"cjlok43x8004x2tpbaammhnx0"},{"post_id":"cjlok43vq000z2tpbfew2692s","tag_id":"cjlok43w0001p2tpbsz1v2e8e","_id":"cjlok43x900512tpb76jtduyf"},{"post_id":"cjlok43x4004n2tpbq8i892dm","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43xa00542tpbmvf6e44z"},{"post_id":"cjlok43x4004n2tpbq8i892dm","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43xd00592tpbkec6jlvc"},{"post_id":"cjlok43x4004n2tpbq8i892dm","tag_id":"cjlok43w0001p2tpbsz1v2e8e","_id":"cjlok43xe005c2tpbq0az5kw0"},{"post_id":"cjlok43vs00132tpbsgpkqvzt","tag_id":"cjlok43w2001w2tpbwsyg55uo","_id":"cjlok43xf005h2tpb7ycxuodw"},{"post_id":"cjlok43vs00132tpbsgpkqvzt","tag_id":"cjlok43x3004j2tpb9xve3fvr","_id":"cjlok43xg005k2tpb1nmdldls"},{"post_id":"cjlok43x5004r2tpbjrt5ctn3","tag_id":"cjlok43w2001w2tpbwsyg55uo","_id":"cjlok43xh005p2tpbcimvvclz"},{"post_id":"cjlok43x5004r2tpbjrt5ctn3","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43xj005s2tpbo812l88o"},{"post_id":"cjlok43vt00162tpb76sn7zdy","tag_id":"cjlok43we002t2tpbzw0vecpd","_id":"cjlok43xj005v2tpb7yhdzkz0"},{"post_id":"cjlok43vt00162tpb76sn7zdy","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43xk005z2tpb30i8pfcw"},{"post_id":"cjlok43vt00162tpb76sn7zdy","tag_id":"cjlok43w0001p2tpbsz1v2e8e","_id":"cjlok43xk00612tpbej46hkfs"},{"post_id":"cjlok43vu001a2tpb8mbb5zaz","tag_id":"cjlok43xf005g2tpbrjaupg7m","_id":"cjlok43xl00652tpb4ol9adaf"},{"post_id":"cjlok43vu001a2tpb8mbb5zaz","tag_id":"cjlok43xh005o2tpbvekvx06q","_id":"cjlok43xl00672tpbjffl8p7n"},{"post_id":"cjlok43vv001e2tpb79933r5e","tag_id":"cjlok43xf005g2tpbrjaupg7m","_id":"cjlok43xm006b2tpbguseejrg"},{"post_id":"cjlok43vw001h2tpbnnqybm72","tag_id":"cjlok43xf005g2tpbrjaupg7m","_id":"cjlok43xm006d2tpby65kqbie"},{"post_id":"cjlok43vw001h2tpbnnqybm72","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43xn006g2tpbtuu5uuml"},{"post_id":"cjlok43vx001k2tpb20goysu0","tag_id":"cjlok43xf005g2tpbrjaupg7m","_id":"cjlok43xn006j2tpb621h0d1r"},{"post_id":"cjlok43vx001k2tpb20goysu0","tag_id":"cjlok43xm006e2tpbl8cvgnzs","_id":"cjlok43xn006k2tpbpldiz50e"},{"post_id":"cjlok43vz001n2tpbewytjvjc","tag_id":"cjlok43xf005g2tpbrjaupg7m","_id":"cjlok43xn006m2tpb41e0mijy"},{"post_id":"cjlok43w0001q2tpb4s5k4pxk","tag_id":"cjlok43xn006l2tpb7rvtgvqp","_id":"cjlok43xo006p2tpbpepjnzib"},{"post_id":"cjlok43w0001q2tpb4s5k4pxk","tag_id":"cjlok43xo006n2tpbt1n41qu5","_id":"cjlok43xo006q2tpblgv8cbl9"},{"post_id":"cjlok43w1001t2tpbavcx4dbs","tag_id":"cjlok43xo006o2tpbodbwwj67","_id":"cjlok43xo006s2tpbpyxx1j62"},{"post_id":"cjlok43w2001x2tpbpko8bhpd","tag_id":"cjlok43xo006o2tpbodbwwj67","_id":"cjlok43xp006v2tpb6v2jrl3c"},{"post_id":"cjlok43w2001x2tpbpko8bhpd","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43xp006w2tpb5h0060ts"},{"post_id":"cjlok43w300212tpbem79webm","tag_id":"cjlok43xo006o2tpbodbwwj67","_id":"cjlok43xq006z2tpbl2qr17wa"},{"post_id":"cjlok43w300212tpbem79webm","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43xq00702tpbr20hf92g"},{"post_id":"cjlok43w400242tpbtaw9tjzj","tag_id":"cjlok43xo006o2tpbodbwwj67","_id":"cjlok43xq00732tpbiu78sala"},{"post_id":"cjlok43w400242tpbtaw9tjzj","tag_id":"cjlok43xq00712tpb50nx9dow","_id":"cjlok43xq00742tpbzr8xvwkv"},{"post_id":"cjlok43w500282tpbpymizqp1","tag_id":"cjlok43xq00722tpbxiz71ciz","_id":"cjlok43xr00772tpbsoevtrbu"},{"post_id":"cjlok43w500282tpbpymizqp1","tag_id":"cjlok43xr00752tpbi4tog3dr","_id":"cjlok43xr00782tpbv3vdko71"},{"post_id":"cjlok43w6002b2tpb2tzro98h","tag_id":"cjlok43xr00762tpbtmgwmwvo","_id":"cjlok43xs007b2tpbpft67sst"},{"post_id":"cjlok43w6002b2tpb2tzro98h","tag_id":"cjlok43xr00792tpb0rjeu5bp","_id":"cjlok43xs007c2tpb3ub65k3e"},{"post_id":"cjlok43w9002i2tpbnqf7wiej","tag_id":"cjlok43xr007a2tpb44lw32zt","_id":"cjlok43xs007f2tpbn3qydqny"},{"post_id":"cjlok43w9002i2tpbnqf7wiej","tag_id":"cjlok43xs007d2tpbdt0x3mi3","_id":"cjlok43xs007g2tpb0kf5nnfm"},{"post_id":"cjlok43wc002m2tpbvhwo66ny","tag_id":"cjlok43xo006n2tpbt1n41qu5","_id":"cjlok43xt007i2tpbqnkebiey"},{"post_id":"cjlok43wd002q2tpbwy68odk1","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43xt007k2tpbqtfnkcch"},{"post_id":"cjlok43wd002q2tpbwy68odk1","tag_id":"cjlok43xh005o2tpbvekvx06q","_id":"cjlok43xt007l2tpbrmab681w"},{"post_id":"cjlok43wg002y2tpbzuy6icg8","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43xu007n2tpbg7fdgfwc"},{"post_id":"cjlok43wg002y2tpbzuy6icg8","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43xu007o2tpbnyshvb0g"},{"post_id":"cjlok43wg002y2tpbzuy6icg8","tag_id":"cjlok43xt007j2tpbpp1a1lj2","_id":"cjlok43xu007q2tpb815qsitq"},{"post_id":"cjlok43wl003a2tpbcpxo6pm5","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43xu007r2tpbadxdei9g"},{"post_id":"cjlok43wl003a2tpbcpxo6pm5","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43xu007t2tpbuwpw6c88"},{"post_id":"cjlok43wl003a2tpbcpxo6pm5","tag_id":"cjlok43xt007m2tpb8iu86c8m","_id":"cjlok43xu007u2tpbu85bjr5q"},{"post_id":"cjlok43wm003d2tpb69p3x7d8","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43xv007w2tpbs4htyt00"},{"post_id":"cjlok43wm003d2tpb69p3x7d8","tag_id":"cjlok43xu007p2tpbgf211ugq","_id":"cjlok43xv007x2tpb88h4ac9m"},{"post_id":"cjlok43wo003i2tpb7kj9sjz5","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43xv007z2tpb16bsoaob"},{"post_id":"cjlok43wo003i2tpb7kj9sjz5","tag_id":"cjlok43xu007s2tpbmzqfml3f","_id":"cjlok43xv00802tpbqtve6k6u"},{"post_id":"cjlok43wo003i2tpb7kj9sjz5","tag_id":"cjlok43w500262tpb2dtj0j1u","_id":"cjlok43xw00822tpbbkid406r"},{"post_id":"cjlok43wp003l2tpbrd765l15","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43xw00832tpbelurvxdx"},{"post_id":"cjlok43wp003l2tpbrd765l15","tag_id":"cjlok43xu007s2tpbmzqfml3f","_id":"cjlok43xw00842tpbjy6wr8lm"},{"post_id":"cjlok43ww00412tpbnktf4y54","tag_id":"cjlok43vr00122tpb3f2594tk","_id":"cjlok43xw00862tpbl7oeq9yr"},{"post_id":"cjlok43ww00412tpbnktf4y54","tag_id":"cjlok43xu007p2tpbgf211ugq","_id":"cjlok43xw00872tpbdrff5lik"},{"post_id":"cjlok43wz00492tpbj4tun92m","tag_id":"cjlok43xw00812tpbc5xcv9b5","_id":"cjlok43xx00892tpbz4t3ruvc"},{"post_id":"cjlok43wz00492tpbj4tun92m","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43xx008a2tpbxe1ptpcy"},{"post_id":"cjlok43x0004d2tpbfis1bujj","tag_id":"cjlok43xw00812tpbc5xcv9b5","_id":"cjlok43xx008c2tpb3wu39chs"},{"post_id":"cjlok43x0004d2tpbfis1bujj","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43xx008d2tpblf7s0inb"},{"post_id":"cjlok43x0004d2tpbfis1bujj","tag_id":"cjlok43xx00882tpb0izkmtki","_id":"cjlok43xy008f2tpbcvc9cp2c"},{"post_id":"cjlok43x2004g2tpb4g7n5e9i","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43xy008g2tpbkx1fca6d"},{"post_id":"cjlok43x2004g2tpb4g7n5e9i","tag_id":"cjlok43xx008b2tpbodfy5i35","_id":"cjlok43xz008i2tpbz7b44nt2"},{"post_id":"cjlok43x3004k2tpbyx6tl0e2","tag_id":"cjlok43xx008e2tpb7mk4kxrp","_id":"cjlok43xz008j2tpbj2i3iibq"},{"post_id":"cjlok43x7004v2tpbycdcdukg","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43xz008l2tpbkj8ahxra"},{"post_id":"cjlok43x7004v2tpbycdcdukg","tag_id":"cjlok43xz008h2tpbs31yd37o","_id":"cjlok43xz008m2tpb9i2uxjad"},{"post_id":"cjlok43x8004y2tpbudzsj4bv","tag_id":"cjlok43xz008k2tpbar44bpyt","_id":"cjlok43y0008o2tpb30trpple"},{"post_id":"cjlok43xa00522tpbfuxqfg3a","tag_id":"cjlok43xo006o2tpbodbwwj67","_id":"cjlok43y0008q2tpbm766xo2r"},{"post_id":"cjlok43xb00562tpbeiwaah4f","tag_id":"cjlok43y0008p2tpbnqb3oj78","_id":"cjlok43y1008u2tpba3iacath"},{"post_id":"cjlok43xb00562tpbeiwaah4f","tag_id":"cjlok43xr00752tpbi4tog3dr","_id":"cjlok43y2008v2tpbml0ef8nz"},{"post_id":"cjlok43xb00562tpbeiwaah4f","tag_id":"cjlok43xr007a2tpb44lw32zt","_id":"cjlok43y3008x2tpbr7tdv7w4"},{"post_id":"cjlok43xd005a2tpbk1q3b7b5","tag_id":"cjlok43y1008t2tpbfgxj2web","_id":"cjlok43y3008y2tpbhjbtheu2"},{"post_id":"cjlok43xd005a2tpbk1q3b7b5","tag_id":"cjlok43vb00072tpbxucaqbzb","_id":"cjlok43y300902tpb4haybdjr"},{"post_id":"cjlok43xe005d2tpbbemw7qnn","tag_id":"cjlok43y2008w2tpb7zq7m6jx","_id":"cjlok43y400922tpb65d8rr6y"},{"post_id":"cjlok43xe005d2tpbbemw7qnn","tag_id":"cjlok43xx008e2tpb7mk4kxrp","_id":"cjlok43y400932tpb28kjtkna"},{"post_id":"cjlok43xf005i2tpbwd4pgv0e","tag_id":"cjlok43y2008w2tpb7zq7m6jx","_id":"cjlok43y400962tpbo9lsf8qx"},{"post_id":"cjlok43xf005i2tpbwd4pgv0e","tag_id":"cjlok43y400942tpb1lllrnqw","_id":"cjlok43y400972tpbwvphmo0g"},{"post_id":"cjlok43xg005l2tpbv696xgh0","tag_id":"cjlok43y400952tpbm39zj5z5","_id":"cjlok43y6009c2tpb3q7iwtue"},{"post_id":"cjlok43xg005l2tpbv696xgh0","tag_id":"cjlok43y400982tpbqtj4gxap","_id":"cjlok43y6009d2tpb078u9ufi"},{"post_id":"cjlok43xg005l2tpbv696xgh0","tag_id":"cjlok43y500992tpbzerdmzz5","_id":"cjlok43y6009f2tpbd1vt2xvh"},{"post_id":"cjlok43xg005l2tpbv696xgh0","tag_id":"cjlok43y5009a2tpb7w2ooj1o","_id":"cjlok43y7009g2tpbd318di2g"},{"post_id":"cjlok43xi005q2tpbv9pku4x8","tag_id":"cjlok43xq00712tpb50nx9dow","_id":"cjlok43y7009i2tpb3kyrtcr5"},{"post_id":"cjlok43xi005q2tpbv9pku4x8","tag_id":"cjlok43xz008k2tpbar44bpyt","_id":"cjlok43y7009j2tpb82wx1xvq"},{"post_id":"cjlok43xj005t2tpbwjj8acoe","tag_id":"cjlok43y2008w2tpb7zq7m6jx","_id":"cjlok43y7009k2tpbkks4vixm"}],"Tag":[{"name":"Docker","_id":"cjlok43vb00072tpbxucaqbzb"},{"name":"Ambari","_id":"cjlok43vf000d2tpbwmg6h2cq"},{"name":"Jenkins","_id":"cjlok43vk000i2tpb48rq2l4t"},{"name":"CDH","_id":"cjlok43vp000w2tpbdkj71dgh"},{"name":"Spark","_id":"cjlok43vr00122tpb3f2594tk"},{"name":"Docker-machine","_id":"cjlok43vv001c2tpbuhfv7evq"},{"name":"安装部署","_id":"cjlok43vx001j2tpb4r6nzhrz"},{"name":"Hadoop","_id":"cjlok43w0001p2tpbsz1v2e8e"},{"name":"HBase","_id":"cjlok43w2001w2tpbwsyg55uo"},{"name":"原理","_id":"cjlok43w500262tpb2dtj0j1u"},{"name":"操作","_id":"cjlok43wb002l2tpbky59vdad"},{"name":"HDFS","_id":"cjlok43we002t2tpbzw0vecpd"},{"name":"Shell","_id":"cjlok43x3004j2tpb9xve3fvr"},{"name":"Hive","_id":"cjlok43xf005g2tpbrjaupg7m"},{"name":"使用","_id":"cjlok43xh005o2tpbvekvx06q"},{"name":"报表","_id":"cjlok43xm006e2tpbl8cvgnzs"},{"name":"Json","_id":"cjlok43xn006l2tpb7rvtgvqp"},{"name":"Scala","_id":"cjlok43xo006n2tpbt1n41qu5"},{"name":"Kafka","_id":"cjlok43xo006o2tpbodbwwj67"},{"name":"大数据","_id":"cjlok43xq00712tpb50nx9dow"},{"name":"linux","_id":"cjlok43xq00722tpbxiz71ciz"},{"name":"开发","_id":"cjlok43xr00752tpbi4tog3dr"},{"name":"Linux","_id":"cjlok43xr00762tpbtmgwmwvo"},{"name":"Mysql","_id":"cjlok43xr00792tpb0rjeu5bp"},{"name":"Java","_id":"cjlok43xr007a2tpb44lw32zt"},{"name":"SSH","_id":"cjlok43xs007d2tpbdt0x3mi3"},{"name":"Yarn","_id":"cjlok43xt007j2tpbpp1a1lj2"},{"name":"RDD","_id":"cjlok43xt007m2tpb8iu86c8m"},{"name":"SparkSQL","_id":"cjlok43xu007p2tpbgf211ugq"},{"name":"SparkStreaming","_id":"cjlok43xu007s2tpbmzqfml3f"},{"name":"ELK","_id":"cjlok43xw00812tpbc5xcv9b5"},{"name":"es","_id":"cjlok43xx00882tpb0izkmtki"},{"name":"FLINK","_id":"cjlok43xx008b2tpbodfy5i35"},{"name":"git","_id":"cjlok43xx008e2tpb7mk4kxrp"},{"name":"Hue","_id":"cjlok43xz008h2tpbs31yd37o"},{"name":"命令","_id":"cjlok43xz008k2tpbar44bpyt"},{"name":"技术","_id":"cjlok43y0008p2tpbnqb3oj78"},{"name":"zk","_id":"cjlok43y1008t2tpbfgxj2web"},{"name":"Hexo","_id":"cjlok43y2008w2tpb7zq7m6jx"},{"name":"Other","_id":"cjlok43y400942tpb1lllrnqw"},{"name":"快捷键","_id":"cjlok43y400952tpbm39zj5z5"},{"name":"Mac","_id":"cjlok43y400982tpbqtj4gxap"},{"name":"Idea","_id":"cjlok43y500992tpbzerdmzz5"},{"name":"Finder","_id":"cjlok43y5009a2tpb7w2ooj1o"}]}}